# (APPENDIX) Appendix {-} 


# References Cited 

<div class="hanging">
Dyer RJ. 2015  Is there such a thing as landscape genetics?  *Molecular Ecology*, **24**, 3518-3528.

Eckert AJ Dyer RJ.  2012.  Defining the landscape of adaptive genetic diversity.  *Molecular Ecology*, **21**, 2836-2838.

Jacquard A. 1983.  Heritability: one word, three concepts. *Biometrics*, **39**, 465-477.

Manel S, Schwartz MK, Luikart G, Taberlet P.  2003.  Landscape genetics: combining landscape ecology and population genetics.  *Trends in Ecology and Evolution*, **18**, 189-197.

</div>



# `tidyverse` 



This appendix goes over a bit of data manipulation with libraries included in `tidyverse` using data from the [Rice Rivers Center](https://ricerivers.vcu.edu), a reserach station attached to my University. 

We will be using the Rice Rivers Center data for this exercise because it is a large data set.  Open your RStudio project for the course and create a new R script (*File $\to$ New File $\to$ R Script*).  Save this script as `Get_Rice_Data.R` in the local directory.  We are going to put the following code into it (copy and paste) and we will run this script whenever we need to grab those data (it is an ugly URL but that is what it is---I had to break up the url to get it to not go off the side of the PDF).  What we are going to do here is to develop a script that will load the latest version of the data from a Google Drive account, format it, clean it up, and present it in a way that can be used easily each time.  Since the orginal data may change, or be updated, we will use this as an example of how to pre-process data and prepare it for analysis, a proceedure that we data scientists must do all too often.  

Here is some raw code to read the data set from a Google Drive account.

```{r, message=FALSE, cache=TRUE}
require(RCurl)
link <- paste("https://docs.google.com/spreadsheets/d",
              "1Mk1YGH9LqjF7drJE-td1G_JkdADOU0eMlrP01WFBT8s",
              "pub?gid=0&single=true&output=csv", sep="/")
url <- getURL( link )
con <- textConnection( url )
rice_data <- read.csv(con, stringsAsFactors = FALSE)
```

Once you the code in the script, source it (via the button on the top right of the source code editing pane, the keyboard shortcut, or by the menu *Code $\to$ Source*).  Now, in the terminal, check to see that it loaded in by taking a `summary()` of it.  It should look something like this:

```{r}
summary(rice_data)
```

So some of the data we have is in a format that may not be the most useful.  Lets add some processing steps to that script that fixes these problems.  If we localize this 'preprocessing' in the same script then whenever we need to grab the data it will both be selected from the raw data *and* it will be processed correctly.

### Preprocessing - Dates and Times {-}

The first column of data in the `data.frame` is the DateTime one and it is encoded, by default, `class(rice_data$DateTime) == "character"`.  If we look at the first value, the format of this variable is:

```{r}
rice_data$DateTime[1]
```

R has a native Date data type that we can convert this into but will have to learn a bit about the formatting of date and time values from string.  It is pretty straight forward, we just need to know how to specify to R how the string represents the elements.  Take a look at the documentation for `strptime` as

```{r}
?strptime
```

and you can get a basic idea.  First, we need to set the order of date and time elements (these are appended to the end of the script above).

```{r}
format <- '%m/%d/%Y %I:%M:%S %p'
```

Next, we create a new column of data that parses the raw character data and the format and turns it into an object that knows something about date and times.  I use a package called `lubridate`, which has a lot of very useful date/time functions in it.  If the line `library(lubridate)` gives an error, then install it using `install.packages("lubridate")`.

```{r warning=FALSE, message=FALSE}
library(lubridate)
rice_data$Date <- parse_date_time(rice_data$DateTime, 
                                  orders = format, 
                                  tz="EST")
rice_data$DateTime <- NULL
```

That last line got rid of the old column of character data now that we have a column that holds an object that is actually represents a particular time.

### Preprocessing - Removing Redundancy

There are a few more things we may want to do to this data set to make it more svelte.  First, lets look at the variables.

```{r}
names(rice_data)
```

Why is `AirTempF` and `H20_TempC`?  Let's standardize these to Celsius (add these next three chunks to the script file as well).

```{r}
rice_data$AirTemp <- 5/9 * (rice_data$AirTempF - 32)
rice_data$AirTempF <- NULL
```

Lets change the rain from inches to centimeters and put wind speed into $km^{-1}$ instead of mph.

```{r}
rice_data$Rain <- rice_data$Rain_in * 2.54
rice_data$Rain_in <- NULL
rice_data$WindSpeed <- rice_data$WindSpeed_mph * 1.60934
rice_data$WindSpeed_mph <- NULL
```

And remove some of the other redundant stuff

```{r}
rice_data$PH_mv <- NULL
rice_data$BGAPC_rfu <- NULL
rice_data$Depth_ft <- NULL
rice_data$SurfaceWaterElev_m_levelNad83m <- NULL
```


Now, lets see what we have:

```{r}
summary(rice_data)
```

If you work with formatted data such as this often, you may want to put the above code into a script (*File -> New File -> R Script*) so that you can just source that script in the beginning of each analysis session and it will automatically format things the right way.  To source it after it is saved, you would just execute the command.

```{r eval=FALSE}
source("Get_Rice_Data.R")
```

and even if the raw data changes, we will *always* get the most up-to-date version to work with.

### Manipulating Data

So we have a lot of data here.  Perhaps one of the things we want to do is to start being able to get inferences about the data.  Lets look at some ways in which we can bend and twist the data to give insights.

Lets start by looking at only a few variables in the `data.frame`.  We don't need to be manipulating the entire data set of we are only going to use a small fraction of it. So, one of the first things you can do is to just extract the subset of variables you are working with.  

We can use logical statements in the indices of the `data.frame` to grab subsets of data.  For example, the following code selects the records whose `WindSpeed` is strictly greater than 20 and returns a `data.frame` consisting of the `Date` and `Rain` columns.

```{r eval=FALSE}
rice_data[ rice_data$WindSpeed > 20, c("Date","Rain")]
```

This is all good and and logical but may be a bit cryptic.  To make this kind of operation much more transparent and more easily readable, the `tidyverse` set of packages has been put together (n.b., it is actually just a front-end package for a whole host of other packages based upon manipulation and visualization). If your computer says no such package or library, just `install.packages("tidyverse")`, and you are good.  Here are some of the ways we can use the tidyverse packages to manipulate data.

### Selecting Subsets of Data

Lets start by just grabbing `Date`, `Rain`, `AirTemp`, and `WindSpeed` using the `select()` function.

```{r message=FALSE}
library(tidyverse)
sm_data <- select(rice_data, Date, AirTemp, Rain, WindSpeed)
```

and if we look at it, we see:

```{r}
sm_data
```

Another way to use `select()` is to combine the list of things you want with the `everything()` function.  What this does is allows you to reorder the columns in a `data.frame`.  Here is the original order of variables

```{r}
names(rice_data)
```

Now, we can have the columns reordered with `Date`, `Rain`, `AirTemp`, and `WindSpeed` then all the rest.

```{r}
rice_data <- select( rice_data, Date, AirTemp, Rain, WindSpeed, everything())
names(rice_data)
```


### Filtering Rows

The filter function allows you to select rows under some specific set of conditions.  here is an example of us grabbing all the data that has a non-freezing `AirTemp`.

```{r}
data <- filter(sm_data, AirTemp > 0 )
data[1:10,]
```

You can also include functions as a part of the filter process.

```{r}
data <- filter( sm_data, weekdays(Date) == "Monday")
data[1:10,]
```

as well as include compound conditions (here I have weekends with no rain)

```{r}
data <- filter( sm_data, 
                weekdays(Date) %in% c("Saturday","Sunday"), 
                Rain == 0 )
data[1:10,]
```

Another way to grab a set of rows is to sample them randomly.  When you grab a random sample, you need to specify the number of things you are selecting *and* if the samples should be selected with or without replacement.  With replacement means that you can select the same entry more than once whereas passing the `replace=FALSE` says that you can only have a row once.

```{r}
sample_n( sm_data, size = 10, replace=FALSE)
```



### Selecting and Ordering

In addition to just grabbing the output as it is (e.g., in the order the data are found in the original `data.frame`), we can also grab it and sort it in a particular order.

The function `arrange()` grabs the data and sorts (by default in ascending order) the entire `data.frame`.

```{r}
data <- arrange(sm_data, AirTemp )
data[1:10,]
```

If you would like to sort in decreasing order

```{r}
data <- arrange(sm_data, desc( AirTemp ) )
data[1:10,]
```


### Piping Output

The functions `select()`, `filter()`, and `arrange()` provide very simple ways to grab subsets of the whole data set under some condition and there are more functions like this we will work with in the near future.  However, lets stop and talk about pipes.  

In the previous examples we did a `select()` assigned the results to a new variable then did one or more `filter()` runs, etc.  Each time we made new variables for the *explicit* purpose of passing it on to the next operator.  If we want to combine these operation, it would be something like:  

```{r}
df <- select( rice_data, Date, AirTemp, Rain) 
df <- filter( df, AirTemp > 0)
df <- arrange( df, desc(Rain) )
df[1:10,]
```




A *pipe* allows us to take a little shorthand approach to this.  The pipe operator is `%>%` (yes the greater than sign surrounded by percent symbols) and is used to connect these function together. Here is an example where I look grab a sample of entries on snowy Mondays.

```{r warning=FALSE}
rice_data %>%
  select(Date, AirTemp, Rain) %>%
  filter(AirTemp < 0, Rain > 0, weekdays(Date) == "Monday" ) %>%
  sample_n(10,replace=FALSE)
```

Literally, the `rice_data` prints out the whole `data.frame` then it is piped as the input into the `select()` function.  Since it is being piped, we do not need to assign it to a new variable and pass that variable name to the `select()` function, just the columns we want to keep.  The `select()` function does its magic and then we pipe it to the `filter()` function with the conditions I am assuming will select snowy Mondays.

This is a very convenient shorthand and we will continue to use it.

### Grouping & Summarizing

We can get some summary by looking at different groupings within the data.  This often requires us to have a something on which we can group the data.

```{r}
rice_data$Weekday <- weekdays(rice_data$Date)
days <- c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")
rice_data$Weekday <- factor( rice_data$Weekday, ordered=TRUE, levels=days)
```


```{r}
rice_data %>%
  group_by(Weekday) %>%
  summarize(Precipitation=mean(Rain))
```

In the above example, we are just dumping the output from R into the markdown.  However, it may be more appropriate to create a real table in the text (we don't show raw output in our manuscripts, right?).  To do this, we first take the output from these complicated filterings, selections, and summarizations and assigne them to a variable.  

```{r}
rain_average <- rice_data %>%
                  group_by(Weekday) %>%
                  summarize(Precipitation=mean(Rain))
```

This gives us an 'object' to pass along to a helper function (from the `knitr` library that does all the translation from markdown to *.docx or *.pdf, or whatever).  The function I use mostly is the `kable()` function (short for kniter table).  It takes a `data.frame` and formats the output appropriately for display as a text component.


```{r}
knitr::kable(rain_average, 
             caption="Average rain during 15 minute intervals for Rice Rivers pier data.")
```

If you were putting this into your manuscript, you would add `echo=FALSE` to the chunk options so that you don't see the code and only see the output.


## The `ggplot` Library

The `ggplot` library was developed by Hadley Wickham following the concept of the grammar of graphics.  The general idea is that we create a graphical display by adding several components together including:  
1. The data  
2. A set of *aesthetics* describing which variables to use for plotting and potential characteristics on the plot.
3. One or more geometries describing how the data are to be displayed on the plot
4. Potential transformation of the data
5. Statistical summaries (e.g., trend lines, error bars, etc.).
6. Text overlays for describing axes, etc.


### Aesthetics

To construct a ggplot object, we need to include at least the first three items on that list above; data, aesthetics, and a geometry.  The data we use for `ggplot()` is always a `data.frame`, which is rather convenient since we use these are the standard object to hold our data in R anyways.  

The next component, the aesthetics, determine what is shown in the plot.  This includes the name of the data column(s) in the data frame to plot as well as additional data columns that may designate color(s), grouping(s), or shapes of the symbols and lines. To designate an aesthetic, we use the function `aes()` to map what is in the `data.frame` onto the plot we are constructing.  As an example, the statement

```{r eval=FALSE}
aes( x = AirTemp, y=Rain )
```

would set those named columns as the coordinates for a bi-variate plot.  

You can put the `aes()` inside the arguments to the `ggplot()` if you want to have it apply to all the geometries.  However, you can also put it in 


### Density Plots

A density plot is one what sorts the data and produces an output of the probability density (or frequency) of the data.

```{r}
rice_data %>%
  ggplot( aes(AirTemp) ) + geom_density()
```

As you can see, the median air temperature in these data (measured from `r format(min(rice_data$Date), "%b %d")` through `r format(max(rice_data$Date),"%b %d")`) measurements is `r median(rice_data$AirTemp)`.  

The code could also be written in the following ways, moving the data from being piped into `ggplot()` to being in the arguments for either of the entries.  All the following plots make the exact same display.  It becomes important where they go later when you try to make more complicated graphics (e.g., overlaying different geometries, plotting from different data sets, etc.).  For now, it is important to be aware of the flexibility.

```{r eval=FALSE}
ggplot( rice_data, aes(AirTemp) ) + geom_density()
ggplot( rice_data ) + geom_density( aes(AirTemp) )
ggplot() + geom_density( aes(AirTemp), data=rice_data    )
```

There are several ways to customize plots and while we are here, I'll show you the first main customization—themes.


### GGPlot Themes

Themes are both color and graphical themes available for you to quickly change the overall look of your plots.  By default, all plots will look like the one above (this is the `theme_default()`).  You can change the theme by *adding* it to the `ggplot` construction.  Here is `theme_minimal()`

```{r}
rice_data %>%
  ggplot( aes(AirTemp) ) + geom_density( ) + theme_minimal()
```

Other themes include:  
- `theme_bw()`  
- `theme_classic()`  
- `theme_dark()`  
- `theme_gray()`  
- `theme_light()`  
- `theme_linedraw()`    
- `theme_minimal()`  

You can also define your own themes.  If you have a presentation that is using a particular color scheme, you can define a theme to match it.

You can change the default theme used in all subsequent `ggplot()` calls (during that session or in that markdown document) by:

```{r}
theme_set( theme_bw() )
```

to get back to the default, you will have to reset it (or add it to the individual plot commands).

Even if you do not want to change the overall theme, there is some benefit in changing specific features of current theme.  Here is an example where I adjust the font size.

```{r}
rice_data %>%
  ggplot( aes(AirTemp) ) + 
  geom_density() + 
  theme_bw( base_size = 16)
```

You can also adjust colors and other aesthetics that are *not determined* by columns in the data frame (e.g., these will be applied to the display of all the data) in the geom_* function arguments.  In this example, I'll make the fill of the density plot and the lines different colors.

```{r}
rice_data %>%
  ggplot( aes(AirTemp) ) + 
  geom_density( color="#2b8cbe", fill="#fec44f", size=4)
```

Adding text to the graphic is done in a similar way.  Both of the following approaches do the same thing, though with differing levels of granularity.

```{r, eval=FALSE}
rice_data %>%
  ggplot( aes(AirTemp) ) + 
  geom_density() + labs( x="Air Temperature (°C)", y="Frequency") 
```

```{r}
rice_data %>%
  ggplot( aes(AirTemp) ) + 
  geom_density() + xlab("Air Temperature (°C)") + ylab("Frequency")
```




### Histograms

Another way to represent the distribution of a variable is to use a histogram.  

```{r}
rice_data %>%
  ggplot( aes(AirTemp) ) + geom_histogram()
```

As you see in the message, by default `geom_histogram()` takes your data and shoves it into 30 bins. These bins are equally sized going from `min(rice_data$AirTemp)` to `max(rice_data$AirTemp)`.  You can change the number of bins by specifying a particular number of bins you want

```{r}
rice_data %>%
  ggplot( aes(AirTemp) ) + geom_histogram( bins=75)
```

or by the width of the bin (in the units of the variable being examined):

```{r}
rice_data %>%
  ggplot( aes(AirTemp) ) + geom_histogram( binwidth = 0.5 )
```

depending upon the data being used, one may make more sense to you as the analyst.

### Overlaying Data

It may be that your data are arranged into groups where it makes sense to use colors to overlay different categories of your data.  Here is an example that breaks down the temperature by day of the week.  I also use the `scale_fill_brewer()` function to make a divergent (the `div` part) color scheme (see http://colorbrewer2.org for some examples).

```{r}
rice_data %>%
  ggplot( aes(AirTemp, fill=Weekday) ) + 
  geom_histogram( binwidth = 0.5 ) + 
  scale_fill_brewer(type = "div" )
```

This overlay is a bit confusing.  Another way you can do is to use facets.  A `facet_grid()` allows us to make several rows and/or columns of plots of our data.  Below you see the `facet_grid()` function being called where the Weekdays is used to stratify plots across rows.  The nomenclature of `Weekday~.` literally means weekdays as rows is a function of everything else.  This is the same notation we will use for statistical models defining the `response~predictors`.

```{r fig.height=8, fig.width=7, fig.align='center'}
rice_data %>%
  ggplot( aes(AirTemp) ) + 
  geom_histogram( binwidth = 0.5 ) +
  facet_grid(Weekday~.)
```

One of the key things that we need to focus on is the clear presentation of our data so that our readers can easily interpret it.  The above two plots show one example of how we minimize the complexity of our data displays, the net effect of which is to make it much more interpretable.




# RGDAL & RGEOS 

Every time I upgrade R in any significant way, two libraries seem to raise their ugly heads and scream like a spoiled child—--*rgdal* and *rgeos*.  Why do these two have to be SOOOO much of a pain? Why can't we have a auto build of a binary with all the options in it for OSX? Who knows? I always feel like I get the fuzzy end of the lollipop with these two. Here is my latest approach for getting them going.

First you have to make sure you have the latest GDAL libraries. I used to get mine from [Kyngchaos](http://www.kyngchaos.com/software/frameworks), just download the framework, install it, and then do some kind of long R CMD INSTALL dance, which seems to no longer work for me. I also tried installing from Ripley's repository and found that (a) It was a version older than the one I already had on my machine, and (b) you can't install from that repository, there is a malformed header and the install.packages() function just barfs.

Time to try something new. I typically stay away from the various installer frameworks out there on OSX to keep everything in Frameworks. But this time, I used MacPorts. You can find the latest version here. Here is how I got it to help me out.  

- Download XCode from Apple, it is both free and has a lot of tools that make your life as a programmer easier.  It is a big package and you'll have to install the command line developer tools as well.  You will be prompted on how to do this.
- Downloaded the version of macports for your OS, I'm currently on 10.11 and installed it with little problems.  It takes a bit of time at the end of the installation because it is downloading a lot of information.  Be patient. 
- In the terminal, I updated it `sudo ports -v selfupdate` and again be patient, it is going to grab a lot of stuff from the internet.
- I then used it to install `gdal` as a unix library (rather than as a framework so it won't be located in /Library/Frameworks) by sudo ports install `gdal`. There were a lot of dependencies for this one so it took a while.
- I then had R install rgdal as `install.packages( rgdal, type="source")`

Worked like a charm.

