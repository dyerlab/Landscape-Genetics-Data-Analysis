[
["index.html", "Landscape Genetic Data Analysis Frontmatter", " Landscape Genetic Data Analysis Rodney J. Dyer Frontmatter Build Date: Mon Mar 11 19:42:28 2019 The content of this text was created to assist in the teaching of a course entitled Landscape Genetic Data Analysis, organized by PRStatistics. All content for this book is contained on Github at https://github.com/dyerlab/applied_landscape_genetics. © 2016 by R.J. Dyer. This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. Under this license, authorized individuals may copy, distribute, display and perform the work and make derivative works and remixes based on this text only if they give the original author credit (attribution). You are also free to distribute derivative works only under a license identical (“not more restrictive”) to the license that governs this original work. Dr. Rodney Dyer is an Associate Professor and Director of the Center for Environmental Studies at Virginia Commonwealth University in Richmond, Virginia, USA. His research focuses on genetic connectivity and structure and how the environment influences both. More information on his research can be found at http://dyerlab.com. "],
["spatial-data.html", "1 Spatial Data 1.1 Synopsis 1.2 Objectives 1.3 Logistics on Data 1.4 Projections 1.5 The sp Library 1.6 Rasters 1.7 Modifying Rasters 1.8 Rasters &amp; ggplot 1.9 3D Visualization 1.10 Saving &amp; Exporting Vector &amp; Raster Objects 1.11 Raster Manipulation 1.12 Questions", " 1 Spatial Data 1.1 Synopsis The goal of this activity is to introduce you to spatial data types and their manipulation and analysis. Functionally, we will be working with two kinds of spatial data: vector and raster data. Vector data can be considered as a finite set of points that may or may not be connected. In R, these points can be used directly, as a numeric data type, or as a Spatial class object. The sp library contains a lot of functions that help deal with points, lines, and polygons and this is going to be a short overview of how they can be derived and manipulated in the pursuit of landscape genetic studies. Rasters are a form of data that is georeferenced and (somewhat) more continuous. Raster data is perhaps best envisioned as a matrix of values, whose entries represent spatially referenced data points (pixels). The raster itself can be visualized as you could for normal matrix output. What makes a raster different, however, is that it is (or should be) georeferenced. This means that each element of the matrix represents some measurement on the ground having a specific location and spread—sometimes referred to as granularity. This is analogous to an image, where if you zoom in on it enough, you will be able to differentiate between individual pixels, each with its own color value. Raster representation specifies each pixel in terms of a location and size associated with that value that we can map onto the earth. 1.2 Objectives The specific objectives of this activity include: Familiarize yourself with geographic projections and how to manipulate both Spatial* and raster projections, ellipses, and datum. Learning about the sp library and how to create, manipulate, display, and otherwise use Spatial* objects to represent spatially relevant data. Learn how to make, acquire, manipulate, and display raster data to represent (semi) continuous landscape data. Modify raster extents, find boundaries, and extract useful data from raster and Spatial* data objects. 1.3 Logistics on Data Since this is the first hands-on practicum, it makes sense to standardize a few things. All activities for this workshop will be displayed in html5 format due to some of the display functions that rely upon javascript libraries. It is perhaps more common to provide these as PDF printouts, but that format cannot be dynamic. Much of what we want to do will be creating maps and looking at complicated data, we do not want limit ourselves to static presentations. If you are not in the habit of using RStudio, I would recommend that you use it for this course. Please make a project for this course and use that project throughout the week so we can minimize the problems associated with setting working directories, finding proper paths, etc. If you need some help on this, please ask, we are here to help. I am also going to assume that the data we have provided is located in a folder called data that is in the same directory as this document. When loading data in this and subsequent activities, I will assume that this is your working directory. If at any point, you have questions, feel free to ask. 1.4 Projections Before we break into the data, we must cover a bit about projections. Our ability to use and understand spatial data is dependent upon understanding geographic ellipses, coordinate systems, and datum. A spatial projection is a mathematical representation of a coordinate space used to identify geospatial objects. Because the earth is both non-flat and non-spheroid, we must use mathematical approaches to describe the shape of the earth in a coordinate space. We do this using an ellipsoid—a simplified model of the shape of the earth. Common ellipsoids include: NAD27 (North American Datum of 1927) based upon land surveys NAD83 based upon satellite data measuring the distance of the surface of the earth to the center of the plant. This is also internationally known as GRS80 (Geodetic Reference System 1980) internationally. WGS84 (World Geodetic System 1984) is a refinement of GRS80 done by the US military that was used in the development of GPS systems (and subsequently for all of us). A projection onto an ellipsoid is a way of converting the spherical coordinates, such as longitude and latitude, into 2-dimensional coordinates we can use. There are three main types of approaches that have been used to develop various projections. (see wikipedia for some example imagery of different projections). These include: Azimuthal: An approach in which each region of the earth is projected onto a plane tangential to the surface, typically at the pole or equator. Cylindrical: This approach projects the surface of the earth onto a cylinder, which is ‘unrolled’ like a large map. This approach ‘stretches’ distances in a east-west fashion, which is why Greenland looks so large… Conic: Another ‘unrolling’ approach, though this time instead of a cylinder, it is projected onto a cone. All projections produce bias in area, distance, or shape (some do so in more than one), so there is no ‘optimal’ projection. To give you an idea of the consequences of these projections, I’ll use the United States map as an example and we can visualize how it is projected onto a 2-dimensional space using different projections. 1.4.1 Equatorial Projections These are projections centered on the Prime Meridian (Longitude=0) library(maps) par(mfrow=c(2,2), mar=c(0,0,0,0)) # makes 2x2 grid of images with no margin map(&quot;state&quot;,proj=&quot;mercator&quot;,main=&quot;mercator&quot;) map(&quot;state&quot;,proj=&quot;mollweide&quot;, main=&quot;mollweide&quot;) map(&quot;state&quot;,proj=&quot;gilbert&quot;, main=&quot;gilbert&quot;) map(&quot;state&quot;,proj=&quot;cylequalarea&quot;,par=39.83) Figure 1.1: Mercator, Mollweide, and Gilbert equatorial projections along with a cylequalarea projection centered on the middle of the US. 1.4.2 Azimuth Projections These projections are centered on the North Pole with parallels making concentric circles. Meridians are equally spaced radial lines. par(mfrow=c(2,2), mar=c(0,0,0,0)) map(&quot;state&quot;,proj=&quot;orthographic&quot;) map(&quot;state&quot;,proj=&quot;orthographic&quot;) map(&quot;state&quot;,proj=&quot;perspective&quot;, param=8) map(&quot;state&quot;,proj=&quot;gnomonic&quot;) Figure 1.2: Orthographic, stereographic, perspective, and gnomonic projections. 1.4.3 Polar Conic Projections Here projections are symmetric around the Prime Meridian with parallel as segments of concentric circles with meridians being equally spaced. par(mfrow=c(1,2), mar=c(0,0,0,0)) map(&quot;state&quot;,proj=&quot;conic&quot;,par=39.83) map(&quot;state&quot;,proj=&quot;lambert&quot;,par=c(30,40)) Figure 1.3: Conic and Lambert projections. 1.4.4 Miscellaneous Projections These are some additional miscellaneous projections, provided for fun mostly, to show some more diversity in the ways we have come up with mapping points onto 2-dimensional displays. par(mfrow=c(2,2), mar=c(0,0,0,0)) map(&quot;state&quot;,proj=&quot;square&quot;) map(&quot;state&quot;,proj=&quot;hex&quot;) map(&quot;state&quot;,proj=&quot;bicentric&quot;, par=-98) map(&quot;state&quot;,proj=&quot;guyou&quot;) Figure 1.4: Miscellaneous projections (Square, hex, bicentric, and Guyou) highlighting some of the more excentric ways of displaying data. 1.4.5 Coordinate Systems Onto this ellipsoid, we must define a set of reference locations (in 3-space) called datum that help describe the precise shape of the surface. We typically are dealing with a combination of data that we’ve collected and that we’ve attained from some other provider. In most GIS applications, the coordinate systems we encounter are either: UTM (Universal Transverse Mercator) measuring the distance from the prime meridian for the x-coordinate and the distance from the equator (often called northing in the northern hemisphere) for the y-coordinate. These distances are in meters and the globe is divided into 60 zones, each of which is 6 degrees in width. Geographic coordinate systems use longitude and latitude. For historical purposes these are unfortunately reported in degrees, minutes, seconds, a temporal abstraction that is both annoying and a waste of time (IMHO). Decimal degrees, while less easy to remember, are easier to work with in R. State Planar coordinate systems are coordinate systems that each US State has defined for their own purposes. They are based upon some arbitrarily defined points of reference and another pain to use (IMHO). Given the differential in state area, some states are also divided into different zones. Maps you get from municipal agencies may be in this coordinate system. If your study straddles different zones or even state lines, you have some work ahead of you… It is best to use a system that is designed for your kind of work. Do not, for example, use a state plane system outside of that state as you have bias associated with the distance away from the origin. That said, Longitude/Latitude (decimal degrees) and UTM systems are probably the easiest to work with in R. 1.4.6 Projection Summary It is in your best interest to get your data into a single and uniform projection, in the same coordinate system, with the same datum. Until you do that, you cannot really start working with your data. In the sections below, we show how to set these and reproject them for uniformity. 1.5 The sp Library The sp Library is a large and complicated library that you will learn incrementally, like pulling layers of an onion… It is a very powerful tool to use and makes our lives rather enjoyable once you get the hang of it. Each object is build within a hierarchy of classes that looks like: Figure 1.5: A general schematic of sp object inheritance. Here the generic ‘Object’ is taking the place of Point, Line, Polygon, Pixel, and other data structures. In the following sections, I go through three basic data types, Point, Line, and Polygon, providing examples that we create de novo as well as extract from the Araptus attenuatus data set (in gstudio) and from WorldClim (also present in the data folder.) 1.5.1 Points Points are defined by SpatialPoints objects. A collection of points may have additional data associated with each location and would make a SpatialPointsDataFrame. This is a bit different than the normal data.frame objects we’ve been using with coordinates in them already—in fact it is the opposite of that. It is a set of points within which is located a data.frame rather than data.frame that has within it a set of points. Confused yet? Lets get to the point and make some coordinates. library(sp) x &lt;- c( -110.2725, -110.0960, -109.3270 ) y &lt;- c( 24.21441, 24.0195, 26.63783 ) coords &lt;- cbind( x, y ) pts &lt;- SpatialPoints( coords ) pts ## SpatialPoints: ## x y ## [1,] -110.2725 24.21441 ## [2,] -110.0960 24.01950 ## [3,] -109.3270 26.63783 ## Coordinate Reference System (CRS) arguments: NA Since we use coordinates all the time in our analyses, gstudio has included some helper functions as long as you are keeping our data in a data.frame. In the Arapatus attenuatus data set there are locale coordinates defined as Longitude and Latitude. By default it produces just a data.frame. library(gstudio) data(arapat) coords &lt;- strata_coordinates(arapat) summary(coords) ## Stratum Longitude Latitude ## 101 : 1 Min. :-114.3 Min. :23.08 ## 102 : 1 1st Qu.:-112.9 1st Qu.:24.52 ## 12 : 1 Median :-111.5 Median :26.21 ## 153 : 1 Mean :-111.7 Mean :26.14 ## 156 : 1 3rd Qu.:-110.4 3rd Qu.:27.47 ## 157 : 1 Max. :-109.1 Max. :29.33 ## (Other):33 However, we can also derive these points directly as a SpatialPoints object defined in the sp library by setting the optional flag as.SpatialPoints=TRUE. pts &lt;- strata_coordinates( arapat, as.SpatialPoints = TRUE ) pts ## class : SpatialPoints ## features : 39 ## extent : -114.2935, -109.1263, 23.0757, 29.32541 (xmin, xmax, ymin, ymax) ## coord. ref. : NA Notice that there is no coordinate reference system in the default extraction. This is because you can pass a wide array of coordinates to this function and it only takes the centroid. It is up to you to define the projection and datum for the data. If it is Long/Lat data as in the example, it can be defined as: proj &lt;- &quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; proj4string(pts) &lt;- CRS(proj) pts ## class : SpatialPoints ## features : 39 ## extent : -114.2935, -109.1263, 23.0757, 29.32541 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 plot( pts, axes=TRUE, xlab=&quot;Longitude&quot;, ylab=&quot;Latitude&quot;) Any set of x- and y- coordinates can be turned into a SpatialPoints object. If we are to associate data with those points, the data has to have the same number of observations as there are coordinates. In our case here, we have 39 populations and as an example I’ll determine the number of individuals genotyped in each population as a df &lt;- data.frame( table(arapat$Population) ) names(df) &lt;- c(&quot;Population&quot;,&quot;N&quot;) pts.df &lt;- SpatialPointsDataFrame(pts,df) pts.df ## class : SpatialPointsDataFrame ## features : 39 ## extent : -114.2935, -109.1263, 23.0757, 29.32541 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 ## variables : 2 ## names : Population, N ## min values : 101, 4 ## max values : SFr, 19 You can translate it back into a data.frame object as: as.data.frame( pts.df )[1:5,] ## Population N x y ## 88 101 9 -114.2935 29.32541 ## 9 102 8 -113.9449 29.01457 ## 84 12 10 -113.6679 28.96651 ## 175 153 10 -113.4897 28.72796 ## 177 156 6 -113.9914 28.66056 or access the data within the data.frame directly (thereby not needing to make a new object) using the attribute @ operator slotNames(pts.df) ## [1] &quot;data&quot; &quot;coords.nrs&quot; &quot;coords&quot; &quot;bbox&quot; &quot;proj4string&quot; pts.df@data[1:5,] ## Population N ## 1 101 9 ## 2 102 8 ## 3 12 10 ## 4 153 10 ## 5 156 6 Since it is a SpatialPoints object, you can get information about it such as the bounding box (e.g., the coordinates of a box that encloses all the points). bbox(pts.df) ## min max ## x -114.2935 -109.12633 ## y 23.0757 29.32541 1.5.2 Lines Lines are created by pairs of points. A single Line object c1 &lt;- cbind(coords$Longitude[1:2], coords$Latitude[1:2]) c2 &lt;- cbind(coords$Longitude[2:3], coords$Latitude[2:3]) L1 &lt;- Line(c1) L2 &lt;- Line(c2) L1 ## An object of class &quot;Line&quot; ## Slot &quot;coords&quot;: ## [,1] [,2] ## [1,] -114.2935 29.32541 ## [2,] -113.9449 29.01457 coordinates(L1) ## [,1] [,2] ## [1,] -114.2935 29.32541 ## [2,] -113.9449 29.01457 A collection of Line objects can be put into a Lines object. Ls1 &lt;- Lines( list(L1), ID=&quot;88 to 9&quot;) Ls2 &lt;- Lines( list(L2), ID=&quot;9 to 84&quot;) Ls1 ## An object of class &quot;Lines&quot; ## Slot &quot;Lines&quot;: ## [[1]] ## An object of class &quot;Line&quot; ## Slot &quot;coords&quot;: ## [,1] [,2] ## [1,] -114.2935 29.32541 ## [2,] -113.9449 29.01457 ## ## ## ## Slot &quot;ID&quot;: ## [1] &quot;88 to 9&quot; And if they are spatial in context (e.g., if we need to plot them in any way, shape, or form), we need to put them into a SpatialLines object, which is also constructed from a list of Lines objects. SLs &lt;- SpatialLines( list(Ls1,Ls2)) proj4string(SLs) &lt;- CRS(proj4string(pts)) SLs ## class : SpatialLines ## features : 2 ## extent : -114.2935, -113.6679, 28.96651, 29.32541 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 plot( pts, axes=TRUE, xlab=&quot;Longitude&quot;, ylab=&quot;Latitude&quot;, ylim=c(28.5,29.5), xlim=c(-114.5, -113)) plot( SLs, add=TRUE, col=&quot;red&quot;, lwd=2 ) If we want to add data to the set of lines, we can associate a data.frame with each of them with internal data. df &lt;- data.frame( Sequence = c(&quot;First&quot;,&quot;Second&quot;), Like_It= c(TRUE,FALSE), row.names = c(&quot;88 to 9&quot;,&quot;9 to 84&quot;)) SLDF &lt;- SpatialLinesDataFrame( SLs, df ) SLDF ## class : SpatialLinesDataFrame ## features : 2 ## extent : -114.2935, -113.6679, 28.96651, 29.32541 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 ## variables : 2 ## names : Sequence, Like_It ## min values : First, TRUE ## max values : Second, FALSE as.data.frame(SLDF) ## Sequence Like_It ## 88 to 9 First TRUE ## 9 to 84 Second FALSE We can also extract the line lengths of each line. SpatialLinesLengths(SLs, longlat = TRUE) ## [1] 48.34721 27.51264 1.5.3 Polygons A polygon is simply a collection of line segments that closes in on itself. We can use polygons to identify habitat, define boundaries, etc. In the short description to follow, we will create a set Polygon* objects, culminating in a SpatialPolygonsDataFrame object. We will start with the first 5 coordinates in the arapat data set. To make the polygon, we must close the coordinates, which means take the first one we put in and append it to the end of the list of coordinates, such that in this case c[1,] == c[6,]. c &lt;- cbind( coords$Longitude[1:5], coords$Latitude[1:5]) c &lt;- rbind( c, c[1,]) c ## [,1] [,2] ## [1,] -114.2935 29.32541 ## [2,] -113.9449 29.01457 ## [3,] -113.6679 28.96651 ## [4,] -113.4897 28.72796 ## [5,] -113.9914 28.66056 ## [6,] -114.2935 29.32541 Then you can construct an individual polygon object P &lt;- Polygon( c ) P ## An object of class &quot;Polygon&quot; ## Slot &quot;labpt&quot;: ## [1] -113.89179 28.89114 ## ## Slot &quot;area&quot;: ## [1] 0.1849395 ## ## Slot &quot;hole&quot;: ## [1] FALSE ## ## Slot &quot;ringDir&quot;: ## [1] 1 ## ## Slot &quot;coords&quot;: ## [,1] [,2] ## [1,] -114.2935 29.32541 ## [2,] -113.9449 29.01457 ## [3,] -113.6679 28.96651 ## [4,] -113.4897 28.72796 ## [5,] -113.9914 28.66056 ## [6,] -114.2935 29.32541 As you can see, there is some additional information provided in the default layout. A few points to be made: - The area parameter is not georeferenced as the polygon itself has no projection. - The labpt is the coordinate where a label would be plot. - The hole and ringDir determine if the polygon represent a hole in some other polygon (e.g., the doughnut hole and the direction it is plot). Similar to how we constructed SpatialLines, a Polygon must be in inserted into a set of Polygons Ps &lt;- Polygons( list(P), ID=&quot;Bob&quot;) From which a list of can be created to make a SpatialPolygons object SPs &lt;- SpatialPolygons( list(Ps)) proj4string(SPs) &lt;- CRS(proj4string(pts)) SPs ## class : SpatialPolygons ## features : 1 ## extent : -114.2935, -113.4897, 28.66056, 29.32541 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 plot( pts, axes=TRUE, xlab=&quot;Longitude&quot;, ylab=&quot;Latitude&quot;, ylim=c(28.5,29.5), xlim=c(-114.5, -113)) plot( SPs, col=&quot;red&quot;, border=&quot;blue&quot;, lwd=2, add=TRUE ) And data can be added to it making a SpatialPolygonsDataFrame (n.b., The row.names of the data.frame must match the ID we set for making the Polygons objects). If they do not, there will be an error thrown. df &lt;- data.frame( Populations=paste(coords$Stratum[1:5],collapse=&quot;, &quot;), row.names = &quot;Bob&quot;) SPDF &lt;- SpatialPolygonsDataFrame( SPs, df) SPDF ## class : SpatialPolygonsDataFrame ## features : 1 ## extent : -114.2935, -113.4897, 28.66056, 29.32541 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 ## variables : 1 ## names : Populations ## value : 88, 9, 84, 175, 177 1.5.4 Projecting sp objects. In R, we use rgdal to project points. Here I load in the coordinates of the populations in the Arapatus attenuatus data set and make a SpatialPoints object out of it. Setting the proj4string() here does not project the data, I am just specifying that the data are already in the lat/long WGS84 format because that is the format it was in when I recorded the values and put them into the data file. coords &lt;- strata_coordinates( arapat ) pts &lt;- SpatialPoints( coords[,2:3] ) proj4string(pts) &lt;- CRS(&quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) pts ## class : SpatialPoints ## features : 39 ## extent : -114.2935, -109.1263, 23.0757, 29.32541 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 The CRS() function holds the definition of the projection and interfaces between the PROJ.4 and RGDAL libraries. To project a set of data points into a new coordinate systems, we use spTransform() and pass it the definition of the new system to use. If we want to actually change the projection, ellipse, or datum, we need to project the data onto the new model. We do this using the spTransform function. Here is an example of taking the exact same data and projecting it from decimal Longitude and Latitude into Universal Transverse Mercator (UTM). pts.utm &lt;- spTransform(pts, CRS(&quot;+proj=utm +zone=12 +datum=WGS84&quot;)) summary( pts.utm ) ## Object of class SpatialPoints ## Coordinates: ## min max ## Longitude 180128 686925.2 ## Latitude 2552540 3248545.0 ## Is projected: TRUE ## proj4string : ## [+proj=utm +zone=12 +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0] ## Number of points: 39 You can see the transformations in the coordinate system by comparing the plots below. The relative position of each point is the same, it has just been loaded into a new space. Figure 1.6: Population locations in (A) longitude and latitude and (B) Universal Transverse Mercator coordinate systems. 1.5.5 Points, Lines, and Polygons in ggplot I am a huge fan of the ggplot2 graphics library, mostly because it is constructed in a way that makes sense to me and it produces very beautiful graphics. You will see various ggplot plotting functions throughout the week so let me give you a brief tutorial. The library is build upon the idea of the grammar of graphics. Essentially, a graphical display is created, sequentially, by adding (literally) components to it. These components may represent the raw data, layers of plotting geometries, spatial transformations (maps, projections, etc.), labels, summaries (expected lines, etc) and coordinate transformations. Unlike build-in graphic approaches, these are added together rather than put into a single function call. This allows us to sequentially build the graphic. Here I’m going plot the locations for the arapat data set onto a map as the final graphic but do so incrementally to demonstrate how it work. At a bare minimum, ggplot requires your data to be in a data.frame object. Columns of data will define the x-, y-, z-, color, shape, and other characteristics. coords &lt;- strata_coordinates(arapat) summary(coords) ## Stratum Longitude Latitude ## 101 : 1 Min. :-114.3 Min. :23.08 ## 102 : 1 1st Qu.:-112.9 1st Qu.:24.52 ## 12 : 1 Median :-111.5 Median :26.21 ## 153 : 1 Mean :-111.7 Mean :26.14 ## 156 : 1 3rd Qu.:-110.4 3rd Qu.:27.47 ## 157 : 1 Max. :-109.1 Max. :29.33 ## (Other):33 You initiate a ggplot by specifying the data and an aesthetic. Here the aesthetic maps the names of the columns of the data onto the x- and y- axes. To show the graphical output I just print the object. p &lt;- ggplot( coords, aes(x=Longitude, y=Latitude)) p There is nothing shown because we have not specified how to display the data in x- and y- space. It did however set up the range and labels for us automatically. To do that, we need to add (again literally add it to the object just like an equation) a geometric layer. p &lt;- p + geom_point() p By default it makes the points filled and black. Since this is a projected set of data, we should probably change the coordinate space so it knows that these are supposed to be mapped onto a projection in decimal degrees. If you look at the figure above, you will see that the x- and y- limits are maximized in the graphic display, not scaled to be spatially coherent. To change the coordinate system, we add a coordinate transformation to the data. p &lt;- p + coord_map(&quot;mercator&quot;) p This plot does not need to be incrementally crated, it can be created as a single line of code as: ggplot(coords, aes(x=Longitude,y=Latitude)) + geom_point() + coord_map(&quot;mercator&quot;) and it would create the same thing. Both lines and polygons may also be created using different geometries. In both cases, you need to have your initial data.frame set up in a specific way so that the geom_* can create the line segments and polygons. See ?geom_line and geom_polygon for more information on how that works. 1.6 Rasters Rasters are a form of data that is georeferenced and (somewhat) continuous. Raster data is perhaps best envisioned as a matrix of values, whose entries represent spatially referenced data points. The raster itself can be visualized as you could for normal matrix output. What makes a raster different, however, is that it is (or should be) georeferenced. This means that each element of the matrix represents some measurement on the ground having a specific location and spread. This is analogous to an image, where if you zoom in on it enough, you will be able to differentiate between individual pixels, it is just that for rasters, each pixel has a spatial location and size associated with it that we can map onto the earth. You can either create raster objects de novo or you can acquire them from some external source. To create one from scratch, you start with a matrix of values and then construct the raster object using the raster() function as: library(raster) r &lt;- matrix(runif(10000),nrow=100) rnd &lt;- raster( r ) rnd ## class : RasterLayer ## dimensions : 100, 100, 10000 (nrow, ncol, ncell) ## resolution : 0.01, 0.01 (x, y) ## extent : 0, 1, 0, 1 (xmin, xmax, ymin, ymax) ## coord. ref. : NA ## data source : in memory ## names : layer ## values : 6.546453e-05, 0.9997195 (min, max) which can be visualized using the normal plot command. The raster library has overridden several of the plotting functions and you can plot raster objects and decorate the images in the same way you do for normal plotting materials (??). plot(rnd) There are also many available repositories for raster data including Open Source, Governmental, and Municipal locations. One common source for these data is that of http://worldclim.org. This repository contains temperature and precipitation data generalized for the entire globe. These data are available free of charge and have been used in numerous biological studies. Moreover, they provide a set of ‘biologically relevant’ layers, called BioClim, that summarize both temperature and precipitation. They motivate these by saying: Bioclimatic variables are derived from the monthly temperature and rainfall values in order to generate more biologically meaningful variables. These are often used in ecological niche modeling (e.g., BIOCLIM, GARP). The bioclimatic variables represent annual trends (e.g., mean annual temperature, annual precipitation) seasonality (e.g., annual range in temperature and precipitation) and extreme or limiting environmental factors (e.g., temperature of the coldest and warmest month, and precipitation of the wet and dry quarters). These layers are encoded into 19 Bio-layers as defined in the following Table. These layers are available for download from their site directly (I recommend using the tiles approach so you do not have to download the entire world map) as well as from the R package dismo. Table 1.1: Key to the categories of bioclim variables derived from temperature and precipitation models for current (past and future) conditions. Rasters for these values (in GeoTiff and BIL formats) are available from http://worldclim.org. Layer Description BIO1 Annual Mean Temperature BIO2 Mean Diurnal Range (Mean of monthly (max temp - min temp)) BIO3 Isothermality (BIO2/BIO7 * 100) BIO4 Temperature Seasonality (standard deviation * 100) BIO5 Max Temperature of Warmest Month BIO6 Min Temperature of Coldest Month BIO7 Temperature Annual Range (BIO5-BIO6) BIO8 Mean Temperature of Wettest Quarter BIO9 Mean Temperature of Driest Quarter BIO10 Mean Temperature of Warmest Quarter BIO11 Mean Temperature of Coldest Quarter BIO12 Annual Precipitation BIO13 Precipitation of Wettest Month BIO14 Precipitation of Driest Month BIO15 Precipitation Seasonality (Coefficient of Variation) BIO16 Precipitation of Wettest Quarter BIO17 Precipitation of Driest Quarter BIO18 Precipitation of Warmest Quarter BIO19 Precipitation of Coldest Quarter For the purposes of this chapter, I’ll use bioclim and altitude layers from tile 22, which encompasses the spatial distribution of sampling locations in Baja California for the Araptus attenuatus dataset Common raster formats include GeoTiff, essentially an image file with some metadata associated with it, and BIL (Binary interleaved) file formats. Both of these types are available from WorldClim. In general, the GeoTiff format is slightly easier to work with as all the data is contained within a single file, whereas the BIL format has two files for each raster (the second file is a header file that has the spatial meta data associated with it). If you do use the BIL format, the file path you pass to raster() would be of the BIL file, not the header one. From Worldclim, I downloaded the elevation raster for Tile 22 and can load it into R using the raster() function as: alt &lt;- raster(&quot;./spatial_data/alt.tif&quot;) alt ## class : RasterLayer ## dimensions : 3600, 3600, 12960000 (nrow, ncol, ncell) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : -120, -90, 0, 30 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 ## data source : /Users/rodney/Desktop/Landscape-Genetics-Data-Analysis/spatial_data/alt.tif ## names : alt ## values : -202, 5469 (min, max) The alt object is summarized here. A couple of things should be pointed out here: - In total, this is an object with 12,960,000 entries in it! - The resolution of each ‘pixel’ in this representation is 0.008, which is about 30-arc seconds or ~1km. That means that each location in the study area is represented by the same exact value as the surrounding square kilometer. Obviously, if you are working on processes whose spatial scale is relevant less than 1000 m2, this kind of data is going to be of little value to you. - The values within the matrix range from -202 upwards to 5469. This is in meters. In addition, the raster has a spatial extent and a projection associated with it. For more information on projections see ??. bbox( alt ) ## min max ## s1 -120 -90 ## s2 0 30 proj4string( alt ) ## [1] &quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; This elevation raster looks like: plot( alt, xlab=&quot;Longitude&quot;, ylab=&quot;Latitude&quot; ) 1.7 Modifying Rasters We can modify rasters just as easily as we can modify matrices. The square bracket indexing you use for matrices are just as effective as before. In the next example, I mask the landscape based upon elevation. I create a copy of the original raster and then make everything whose elevation is less than 500m as missing data. Plotting this over the top of the original raster shows only locations where elevation exceeds this cutoff. alt &lt;- raster(&quot;data/bioclim/alt_22.tif&quot;) e &lt;- extent( c(-115,-109,22,30) ) baja_california &lt;- crop( alt, e ) baja_california ## class : RasterLayer ## dimensions : 960, 720, 691200 (nrow, ncol, ncell) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : -115, -109, 22, 30 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 ## data source : in memory ## names : alt_22 ## values : -202, 2263 (min, max) plot( baja_california, axes=TRUE, xlab=&quot;Longitude&quot;, ylab=&quot;Latitude&quot; ) plot( pts, add=TRUE ) bc &lt;- baja_california bc[ bc &lt; 500 ] &lt;- NA plot( baja_california, legend=FALSE, col=&quot;darkgrey&quot; ) plot( bc, add=TRUE, legend=FALSE) 1.7.1 Stacks of Rasters It is not uncommon to be working with many different raster layers at the same time. Instead of loading them individually, the raster library has a RasterStack object that can hold several rasters at one time and can be used in places where we would use individual rasters. Here is an example using the elevation and temperature rasters for tile 22. files &lt;- c(&quot;./spatial_data/alt.tif&quot;, &quot;./spatial_data/bio1.tif&quot;, &quot;./spatial_data/bio5.tif&quot;, &quot;./spatial_data/bio6.tif&quot;) bio_layers &lt;- stack( files ) bio_layers ## class : RasterStack ## dimensions : 3600, 3600, 12960000, 4 (nrow, ncol, ncell, nlayers) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : -120, -90, 0, 30 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 ## names : alt, bio1, bio5, bio6 ## min values : -202, -20, 54, -83 ## max values : 5469, 295, 424, 242 Performing operations on a stack is as easy as performing them on individual layers. Here, I trim them to the hull defined above. e &lt;- extent( c(-115,-109,22,30) ) bio_layers &lt;- crop( bio_layers, e ) plot(bio_layers) From which values may be extracted using normal methods as outlined above. library(sp) library(gstudio) data(arapat) coords &lt;- strata_coordinates(arapat) pts &lt;- SpatialPoints( coords[,2:3]) coords &lt;- strata_coordinates( arapat ) pts &lt;- SpatialPoints( coords[,2:3] ) df &lt;- extract( bio_layers, pts) df &lt;- df[ !is.na(df[,1]),] head(df) ## alt bio1 bio5 bio6 ## [1,] 681 178 331 48 ## [2,] 361 195 346 61 ## [3,] 368 197 348 62 ## [4,] 240 205 355 68 ## [5,] 177 203 352 71 ## [6,] 26 223 372 81 And visualized normally. library( GGally ) ggpairs(as.data.frame(df)) 1.8 Rasters &amp; ggplot As is the case with a lot of data types in R, there is a way to use the ggplot library to visualize rasters. Essentially, what you need to do is to transform your raster objects into data.frame objects for ggplot’s geom_tile() function. Here is an example. library(ggplot2) df &lt;- data.frame( rasterToPoints( baja_california )) names(df) &lt;- c(&quot;Longitude&quot;,&quot;Latitude&quot;,&quot;Elevation&quot;) p &lt;- ggplot( df ) + geom_tile( aes(x=Longitude,y=Latitude,fill=Elevation)) p &lt;- p + scale_fill_gradientn( colors=c(&#39;#a6611a&#39;,&#39;#dfc27d&#39;,&#39;#f5f5f5&#39;,&#39;#80cdc1&#39;,&#39;#018571&#39;)) p &lt;- p + coord_equal() + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) p As usual, we can add additional information to the plot and as we would for any other ggplot object. Here I’ll add the populations and indicate if they have samples in them that are of one species (Pure) or have a mix of the two (Mixed). num.clades &lt;- colSums( table(arapat$Species, arapat$Population) &gt; 0 ) Stratum=names(num.clades) Species= factor( c(&quot;Pure&quot;,&quot;Mixed&quot;)[num.clades] ) tmp.df &lt;- data.frame( Stratum, Species ) sites &lt;- merge( coords, tmp.df ) p + geom_point( aes(x=Longitude,y=Latitude, shape=Species), size=3, data=sites ) 1.9 3D Visualization It is also possible to visualize rasters in 3-space. The library rasterVis provides an interface to the rgl library to plot a surface. Once installed, these are easy to use for viewing surfaces. Here is an example using the elevation data we have been playing with. library(rasterVis) plot3D( baja_california , zfac=0.1) The zfac option in the plot is the amount to scale the z-axis (elevation) in relation to the x-axis and y-axis dimensions. It is a bit exaggerated at zfac=0.1 but you get the idea. 1.10 Saving &amp; Exporting Vector &amp; Raster Objects As all of these objects are R objects, they can be saved to disk using the save() function, which makes them a *.rda object. If you have objects that take a bit of time to create, it is in your best interests to save them after creation and on subsequent analyses, just use the saved versions. There are many situations where you need to save a raster you’ve manipulated. As these raster objects are R objects, you can save them directly to the file system using the write() as: write( baja_california, filename=&quot;baja_california.rda&quot;) This will save the raster object to file exactly like it is in your R session. To load it back in you just use load() and it is returned just like it was. The benefit to saving these as R objects is that you do not need to change it at all to pick up where you left off. You may also need to export the vector or raster data into a non-R format for external analyses. A common format for vector data is the ubiquitous ESRI shapefile. library(rgdal) writeOGR( obj=SPDF, dsn=&quot;spatial_data&quot;, layer=&quot;SpatialPolyExample&quot;, driver=&quot;ESRI Shapefile&quot;) For rasters, you use the writeRaster() function. The file extension is used to determine the file format used and R saves it automatically. writeRaster( baja_california, filename=&quot;baja_california.tif&quot;) 1.11 Raster Manipulation In this final section, we will delve into how to actually get data out of raster and vector components. 1.11.1 Cropping Rasters Just because we have a large raster does not mean that it is in your best interest to use the entire object. Much of the spatial analyses routines used in population genetics require measurements of intervening distance, either Euclidean or ecological. Many of the routines for estimation of these distances require the estimation of pairwise distance between all pixels. For our purposes, the arapat dataset does not occur throughout most of this map, so it is in our best interests to use only the portion of the raster relevant to our data rather than the entire thing. Here is one way of going this. I first define an extent, which consists of a vector representing the coordinates for xmin, xmax, ymin, and ymax (in decimal degrees longitude and latitude). You then crop() the raster to that extent. library(sp) library(raster) alt &lt;- raster(&quot;./spatial_data/alt.tif&quot;) e &lt;- extent( c(-115,-109,22,30) ) baja_california &lt;- crop( alt, e ) plot(baja_california, xlab=&quot;Longitude&quot;,ylab=&quot;Latitude&quot;) Lets make this base map a bit more pretty by taking the altitude and estimating the slope of each pixel and the direction it is facing (aspect). From this, we can ‘shade’ the hills in the map giving it more of a relief view we commonly see in maps. The optional parameters to hillShade provide the angle and direction of the light source. slope &lt;- terrain( baja_california, opt=&quot;slope&quot; ) aspect &lt;- terrain( baja_california, opt=&quot;aspect&quot;) baja &lt;- hillShade( slope, aspect, 40, 270 ) plot(baja, xlab=&quot;Longitude&quot;,ylab=&quot;Latitude&quot;, legend=FALSE) Onto this map, we can plot our populations. For this, we convert the raw coordinates into a SpatialPoints object and then overlay onto the map. I use two points() commands to make the symbol for each population. library(gstudio) data(arapat) coords &lt;- strata_coordinates(arapat) pts &lt;- SpatialPoints( coords[,2:3], proj4string = CRS(proj4string(baja))) plot(baja, xlab=&quot;Longitude&quot;,ylab=&quot;Latitude&quot;, legend=FALSE) points( pts, col=&quot;darkred&quot;, pch=3) points( pts, col=&quot;red&quot;, pch=16) 1.11.2 Cropping Rasters Via Polygons It is also possible to crop a raster with a more fine grained approach using a polygon. Here is an example using five points picked around the region of Loreto, BCS (I just grabbed these by looking at Google Earth). You define a polygon by a series of points, the last of which has to be identical to the first one so that the polygon is a closed object and not just a series of points on a crooked line… pts &lt;- rbind( c(-111.5,27.0), c(-112.4,26.7), c(-111.7,25.7), c(-111.1,25.4), c(-110.8,26.0), c(-111.5,27.0) ) pts ## [,1] [,2] ## [1,] -111.5 27.0 ## [2,] -112.4 26.7 ## [3,] -111.7 25.7 ## [4,] -111.1 25.4 ## [5,] -110.8 26.0 ## [6,] -111.5 27.0 From these points, we construct a SpatialPolygons object (see @ref{polygons} for more info on this convoluted construction) and then can overlay onto the map to make sure it in the correct vicinity (here we are eyeballing it a bit). For more on why this next line of code looks so crazy, see 1.5.3. polys &lt;- SpatialPolygons(list(Polygons(list(Polygon(pts)),&quot;Polygon&quot;))) plot(baja, legend=FALSE) plot(polys, add=TRUE) To use the polygon to crop the raster, we have to both remove the part of the raster that is not contained within the polygon (mask) and then cut down the remaining raster to change the bounding box to that representing the portion of the data that remains (trim). If you do not trim the raster, it will have the same amount of data associated with it as the previous raster (e.g., the underlying data matrix will have 960 rows and 720 columns) but the part that is masked will be represented by NA values. For many rasters, the data is held in-memory (see the entry for ’data sourcein the summary above) and as such removing as much of a raster that isNA` improves your ability to manipulate it better. loredo &lt;- trim( mask( baja, polys ) ) plot(loredo, xlab=&quot;Longitude&quot;,ylab=&quot;Latitude&quot;) Figure 1.7: Extraction of region within polygon from the full Baja California raster. 1.11.3 Cropping Rasters Via Convex Hull An analysis common to modern population genetics is that of finding ecological distances between objects on a landscape. The estimation of pairwise distance derived from spatial data is a computationally intensive thing, one that if you are not careful will bring your laptop to its knees! One way to mitigate this data problem is to use a minimal amount raster area so that the estimation of the underlying distance graph can be done on a smaller set of points. Cropping by a polygon like demonstrated in the previous example is a ‘by hand’ approach to estimating a box that roughly encompasses your data points. A more efficient one is one where you simply provide your coordinates and we can estimate a polygon that surrounds those coordinates with the minimal amount of wasted space. This is called a Convex Hull, which is kind of like a polygon that is created as if there was a rubber band fit around all your points. It is a minimal area that includes all of your points. For this example, I’m going to use the populations found along the peninsula and find the minimal area encompassing those points. baja_coords &lt;- coords[ !(coords$Stratum %in% c(&quot;101&quot;,&quot;102&quot;,&quot;32&quot;)), ] baja_pts &lt;- SpatialPoints( baja_coords[,2:3]) plot(baja, legend=FALSE) plot(baja_pts,add=T,col=&quot;darkred&quot;) plot(baja_pts,add=T,col=&quot;darkred&quot;,pch=16) The methods for finding the hull and adding a buffer around it are found in the rgeos package. These are pretty easy functions to use and are very helpful. If you are having trouble installing the rgeos package from source, see my webpage (http://dyerlab.com), there is a short tutorial. library(rgeos) # loads in gConvexHull &amp; gBuffer functions hull &lt;- gConvexHull(baja_pts) plot(baja, legend=FALSE) plot(baja_pts,add=T,col=&quot;darkred&quot;) plot(baja_pts,add=T,col=&quot;darkred&quot;,pch=16) plot(hull,add=T,border=&quot;red&quot;) The function gConvexHull() returns an object of type SpatialPolygons, just like we created before. However, we now have a polygon that has each of our most ‘outward’ populations on the very edge of the polygon. It may be beneficial for us to add a buffer around this polygon. hull_plus_buffer &lt;- gBuffer(hull,width=.2) plot(baja, legend=FALSE) plot(baja_pts,add=T,col=&quot;darkred&quot;) plot(baja_pts,add=T,col=&quot;darkred&quot;,pch=16) plot(hull_plus_buffer, add=T, border=&quot;red&quot;) Now, we can mask and trim it to include only the area of interest. pop_hull &lt;- trim( mask(baja,hull_plus_buffer) ) plot(pop_hull, legend=FALSE, xlab=&quot;Longitude&quot;, ylab=&quot;Latitude&quot;) plot(baja_pts,add=T,col=&quot;darkred&quot;,pch=16) This would be a great raster to start looking at ecological separation in since we have removed the extraneous data that would unintentionally cause problems with the distance estimators. 1.11.4 Extracting Point Data From Rasters So far, the rasters have been confined to representing a single static object. However, it is not uncommon to need to query a raster and find out the values at particular points. These points may be pre-defined or they may be dynamic (e.g., you need to point at a location on the map and determine the value there). For queries of the first kind, we can use the extract() function. For this I downloaded the average temperature and precipitation rasters from Worldclim. baja_temp &lt;- raster(&quot;./spatial_data/bio1.tif&quot;) baja_prec &lt;- raster(&quot;./spatial_data/bio12.tif&quot;) And then extract the values from each of these layers into the coords data we already have set up. coords$elevation &lt;- extract( baja_california, coords[,c(2,3)]) coords$mean_temp &lt;- extract( baja_temp, coords[,c(2,3)]) coords$mean_precip &lt;- extract( baja_prec, coords[,c(2,3)]) coords[1:10,] ## Stratum Longitude Latitude elevation mean_temp mean_precip ## 1 88 -114.2935 29.32541 681 178 143 ## 11 9 -113.9449 29.01457 361 195 148 ## 20 84 -113.6679 28.96651 368 197 124 ## 29 175 -113.4897 28.72796 240 205 106 ## 36 177 -113.9914 28.66056 177 203 120 ## 46 173 -112.8698 28.40846 26 223 102 ## 56 171 -113.1826 28.22308 522 195 145 ## 66 89 -113.3999 28.03661 290 203 117 ## 76 159 -113.3161 27.52944 87 211 123 ## 85 SFr -112.9640 27.36320 305 205 107 A note should be made on the temperature and precipitation values. Temperature is denoted in tenths of a degree Celsius. Though it does get quite hot at times, it does not average 188°C at population 88! Similarly, the units for precipitation are mm (or tenths of centimeters if you will…). library(ggrepel) coords &lt;- coords[ order(coords$Latitude),] p &lt;- ggplot( coords, aes(x=Latitude,y=elevation)) + geom_line(color=&quot;lightgrey&quot;) p &lt;- p + geom_point() + ylab(&quot;Elevation (m)&quot;) p + geom_text_repel(aes(label=Stratum), color=&quot;red&quot;) The package ggrepel provides a pseudo-smart labeling geometry for ggplot allowing you to have labels that are shifted around the points so as to maximize visibility. For inquires of the second type, we can use the function click() to retrieve one of several outputs. Here is the help file that describes the various components. click {raster} R Documentation Query by clicking on a map Description Click on a map (plot) to get values of a Raster* or Spatial* object at that location; and optionally the coordinates and cell number of the location. For SpatialLines and SpatialPoints you need to click twice (draw a box). Usage ## S4 method for signature 'Raster' click(x, n=Inf, id=FALSE, xy=FALSE, cell=FALSE, type=\"n\", show=TRUE, ...) ## S4 method for signature 'SpatialGrid' click(x, n=1, id=FALSE, xy=FALSE, cell=FALSE, type=\"n\", ...) ## S4 method for signature 'SpatialPolygons' click(x, n=1, id=FALSE, xy=FALSE, type=\"n\", ...) ## S4 method for signature 'SpatialLines' click(x, ...) ## S4 method for signature 'SpatialPoints' click(x, ...) Arguments x - Raster*, or Spatial* object (or missing) n - number of clicks on the map id - Logical. If TRUE, a numeric ID is shown on the map that corresponds to the row number of the output xy - Logical. If TRUE, xy coordinates are included in the output cell - Logical. If TRUE, cell numbers are included in the output type - One of \"n\", \"p\", \"l\" or \"o\". If \"p\" or \"o\" the points are plotted; if \"l\" or \"o\" they are joined by lines. See ?locator show - logical. Print the values after each click? ... - additional graphics parameters used if type != \"n\" for plotting the locations. See ?locator Value The value(s) of x at the point(s) clicked on (or touched by the box drawn). Note The plot only provides the coordinates for a spatial query, the values are read from the Raster* or Spatial* object that is passed as an argument. Thus you can extract values from an object that has not been plotted, as long as it spatially overlaps with with the extent of the plot. Unless the process is terminated prematurely values at at most n positions are determined. The identification process can be terminated by clicking the second mouse button and selecting 'Stop' from the menu, or from the 'Stop' menu on the graphics window. See Also select, drawExtent Examples r Here is the output from a single inquire on the baja_california raster map, asking for both the coordinates and the elevation of a particular location. plot( baja_california, legend=FALSE) click(baja_california, xy=TRUE) ## x y value ## 1 -113.3875 27.82083 116 In a similar fashion, you can interactively create polygon points. &gt; cape_pts &lt;- click(baja_california, n=5, xy=TRUE, type=&quot;p&quot;) and after you have selected the points, you get something like. Raster points around the Cape region identified manually. with the data as: cape_pts ## x y value ## 1 -109.9542 24.22917 NA ## 2 -110.7542 23.66250 NA ## 3 -110.1042 22.77917 NA ## 4 -109.2625 23.06250 NA ## 5 -109.3042 23.77917 NA This is an extremely helpful approach for cropping and manipulating your raster layers. 1.11.5 Extracting Neighborhood Data from Rasters Just as with points, we can also extract data around individual locations defined by either points with a buffer or from a polygon. The first approach requires us to put a buffer around the coordinate points and then extract the raster data. I’ll use the five most southern sites as an example. coords &lt;- coords[ order( coords$Latitude) , ] cape_coords &lt;- coords[1:5,] pts &lt;- SpatialPoints( cape_coords[,2:3] ) proj4string(pts) &lt;- CRS(proj4string(baja_california)) pts ## class : SpatialPoints ## features : 5 ## extent : -110.1091, -109.6487, 23.0757, 24.0195 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 These points are in the Cape region, which we can crop out of the baja_california raster as using the cape_pts we clicked out to form a polygon: cpts &lt;- SpatialPoints( cape_pts[,1:2] ) b &lt;- bbox( cpts ) cape &lt;- crop(baja_california, b) plot(cape, xlab=&quot;Longitude&quot;, ylab=&quot;Latitude&quot;) plot( pts, add=TRUE) We can buffer these points buff &lt;- gBuffer(pts, byid=TRUE, width=0.01) plot( cape ) plot( buff, add=TRUE) And we can extract the data from within those polygons for each of the five regions. vals &lt;- extract( cape, buff ) vals ## [[1]] ## [1] 79 75 28 37 ## ## [[2]] ## [1] 11 57 5 72 ## ## [[3]] ## [1] 115 142 161 132 ## ## [[4]] ## [1] 28 16 103 26 ## ## [[5]] ## [1] 584 651 609 510 597 You can do the same thing for larger polygons, as we established above when we drew the polygon around Loreto, BCS. plot( baja, legend=FALSE ) plot( polys, add=TRUE) The resulting data is just a lot bigger in size. data &lt;- extract( baja_california, polys ) length( data[[1]]) ## [1] 19242 1.11.6 Reprojecting Rasters When working with rasters, we can reproject these onto other projections rather easily. Here is an example from the worldclim elevation tile we used previously. alt &lt;- raster(&quot;data/bioclim/alt_22.tif&quot;) e &lt;- extent( c(-115,-109,22,30) ) baja_california &lt;- crop( alt, e ) baja_california ## class : RasterLayer ## dimensions : 960, 720, 691200 (nrow, ncol, ncell) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : -115, -109, 22, 30 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 ## data source : in memory ## names : alt_22 ## values : -202, 2263 (min, max) We can now project it to another projection, lets say Lambert Conic Conformal. library(rgdal) projection(baja_california) ## [1] &quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; baja_lcc &lt;- projectRaster( baja_california, crs=&quot;+proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100 +ellps=WGS84&quot;) baja_lcc ## class : RasterLayer ## dimensions : 1060, 878, 930680 (nrow, ncol, ncell) ## resolution : 845, 935 (x, y) ## extent : -1610637, -868727, 2790826, 3781926 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100 +ellps=WGS84 ## data source : in memory ## names : alt_22 ## values : -202, 2205.154 (min, max) These two projections influence the region as shown below. As usual, there is probably a way to plot these values in ggplot to make the output just a little bit more awesome. Projections of data in ggplot displays can be manipulated by appending a coord_* object to the plot as we showed above. Here are two examples using a Mercator and azimuth equal area projection of the state maps. library(ggplot2) states &lt;- map_data(&quot;state&quot;) map &lt;- ggplot( states, aes(x=long,y=lat,group=group)) map &lt;- map + geom_polygon( fill=&quot;white&quot;,color=&quot;black&quot;) map &lt;- map + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) map + coord_map(&quot;mercator&quot;) Conversely, we can plot it using the equal area Azimuth projection map + coord_map(&quot;azequalarea&quot;) or fish-eye map + coord_map(&quot;fisheye&quot;,par=3) or any other projection available listed in the mapproject() function. 1.12 Questions The WorldClim organization makes a set of bioclimatic data available for free. Grab the raster layers for Tile 22 (they are in the data folder) and crop them down to an extent that covers the entirety of sampling locations for the arapat data set. Using the population locations in the arapat dataset, extract the value for each layer. To what extent are these variables correlated with each other? At what point will we need to be concerned if we are using them as input into statistical models? Is there more variation in elevation in a circular neighborhood around Loreto (Lon: -111.343333, Lat: 26.012778 ) or Guerrero Negro (Lon: -114.056111, Lat:27.958889)? Use a buffer width of 0.04. Which sampling location is highest in elevation? Which has the hottest summers? "],
["genetic-data.html", "2 Genetic Data 2.1 Synopsis 2.2 Objectives 2.3 Genetic Markers, Types and Usage 2.4 Allele Frequencies 2.5 Hardy-Weinberg Equilibrium 2.6 Genetic Diversity 2.7 Rarefaction 2.8 Genetic Distance 2.9 Genetic Structure 2.10 F-Statistics and Thier Ilk 2.11 Statistical Structure 2.12 Questions", " 2 Genetic Data 2.1 Synopsis This afternoon’s activity is focused on understanding how to work with genetic marker data in R. As a base data type, genotypes are neither normal nor multinomial, but follow an algebra defined by Mendelian inheritance and dictated by mating system and ploidy levels. Here we cover some of the basic material on working with genetic data then dive right into the basis of population genetics—Hardy-Weinberg Equilibrium. 2.2 Objectives Load data in R using the gstudio package. Estimate allele frequencies for entire data sets as well as among subsets of your data (e.g., regions, populations, or locales). Determine if populations or subgroups of individuals deviate from expectations under Hardy-Weinberg Equilibrium. Plot population locales and summary statistics onto maps. Estimate frequencies and diversity along gradients. 2.3 Genetic Markers, Types and Usage There are several kinds of genetic markers that can be used in landscape genetic analyses, though some are more practical than others. The limiting factor in deciding on an appropriate marker is the amount of genetic variance in the marker system relative to the features we are testing against. For example, mtDNA markers are rather common in phylogeography and species and subspecies delimitation studies but they evolve at a rate that may be too slow if we are trying to understand contemporary movement patterns. They will not be able to spatially track variation in a way that a more variable and faster mutating marker type would. For the purposes of this work, we are going to focus on genetic markers rather than sequence data. These are most commonly nuclear markers—though there are cpDNA microsat markers in some species that make them very amenable to landscape genetic studies. In R, there is not a default data type for genetic markers, and as such we will rely upon one derived in a package created by R. Dyer called gstudio. This package provides not only a default data type locus but also the accompanying functions and routines that are commonly used in the spatial analysis of population genetic data (a subset of which we use in Landscape studies). There are other packages available for genetic analyses, though not all are inter-compatible. In the gstudio package, the locus object is treated just as any other kind of data and can be inserted contained within data.frame, list, and vector containers just like any other kind of data. In doing so, it allows the analysis workflow to proceed in a more streamline fashion. Here are some basic constructs for how to create and manipulate locus objects. A locus is constructed by passing it zero or more alleles. library(gstudio) loc0 &lt;- locus() loc0 ## [1] NA A locus without alleles is recorded as missing data.1 Alleles within a locus can be defined as a character types, loc1 &lt;- locus( c(&quot;A&quot;,&quot;B&quot;) ) loc1 ## [1] &quot;A:B&quot; numeric types, loc2 &lt;- locus( c(128,134) ) loc2 ## [1] &quot;128:134&quot; or any other construct loc3 &lt;- locus( c(&quot;Minor Allele&quot;, &quot;Major Allele&quot;)) loc3 ## [1] &quot;Major Allele:Minor Allele&quot; And collections of them are stored as vectors. loci &lt;- c( loc0, loc1, loc2, loc3 ) loci ## [1] NA &quot;A:B&quot; ## [3] &quot;128:134&quot; &quot;Major Allele:Minor Allele&quot; 2.3.1 Loading External Data The most way to get genotypes into R is to load them from an external file. In many labs, genotypic data is kept in a spreadsheet-like format (or data.frame in R), where: Each row of the data represent observations on a single individual. This may include information other than genotypes such as spatial coordinates, sampling locale, internal ID number, etc. Each column represents a single kind of data measured across all observations. May be missing observations, genotypes, or whatever. Loading the data in and configuring the genotypes as locus objects requires that we identify the columns of the data that are to be translated into locus objects as well as the kind of marker it is representing (AFLP vs. SNP vs. Microsatellite) and the format in which data are currently stored. The following table provides some of the more commonly used types: Locus Type Description aflp Amplified Fragment Length Polymorphism with bands encoded as \\(0\\) or \\(1\\). column Co-dominant markers such as microsatellite loci encoded as one allele per column of data. For diploid data, there are twice as many columns of alleles as there are marker loci. separated Co-dominant markers in a single column with alleles separated by a pre-defined symbol (default is ‘:’). For species with ploidy &gt; 2, this is the default approach for importing polyploid genotypes. snp Alleles encoded by the number of minor alleles 0, 1, or 2. genepop Import from the standard (non-fancy) genepop file format. Here are some examples data files that are included in the gstudio package and how they can be loaded in. The path variable in the following code chunks is the path to the example .csv file that is located in the gstudio* package. For your data, change out the path. In all cases, the structure that is loaded into R is a data.frame. Here is a raw AFLP file that comes with the gstudio package path &lt;- system.file(&quot;extdata&quot;, &quot;data_aflp.csv&quot;, package = &quot;gstudio&quot;) data &lt;- read_population( path, type=&quot;aflp&quot;, locus.columns = 4:7 ) summary(data) ## Population Latitude Longitude Loc1 Loc2 ## Length:5 Min. :37.74 Min. :-122.89 0 :3 0 :3 ## Class :character 1st Qu.:38.81 1st Qu.:-122.49 1 :2 1 :2 ## Mode :character Median :42.26 Median : -93.47 ## Mean :42.94 Mean :-101.20 ## 3rd Qu.:47.15 3rd Qu.: -89.98 ## Max. :48.75 Max. : -77.16 ## Loc3 Loc4 ## 1 :3 0 :1 ## NA&#39;s:2 1 :2 ## NA&#39;s:2 ## ## ## And a two-column microsatellite data file path &lt;- system.file(&quot;extdata&quot;, &quot;data_2_column.csv&quot;, package = &quot;gstudio&quot;) data &lt;- read_population( path, type=&quot;column&quot;, locus.columns = 4:7 ) summary(data) ## Population Latitude Longitude Loc1 ## Length:5 Min. :37.74 Min. :-122.89 10:10 :1 ## Class :character 1st Qu.:38.81 1st Qu.:-122.49 10:12 :1 ## Mode :character Median :42.26 Median : -93.47 10:14 :1 ## Mean :42.94 Mean :-101.20 12:12 :1 ## 3rd Qu.:47.15 3rd Qu.: -89.98 12:14 :1 ## Max. :48.75 Max. : -77.16 ## Loc2 ## 15:15 :1 ## 15:17 :1 ## 17:17 :1 ## NA&#39;s :2 ## ## 2.3.2 Built-In Data Sets There are some built-in data sets (described in ??) that we will be using and can be loaded into your workspaces using data(). The main one in gstudio is the Arapatus attenuatus data set (referenced in shorthand as arapat). You will become a specialist in desert bark beetle genetics over the course of this workshop. data(arapat) summary( arapat ) ## Species Cluster Population ID Latitude ## Cape : 75 CBP-C :150 32 : 19 101_10A: 1 Min. :23.08 ## Mainland : 36 NBP-C : 84 75 : 11 101_1A : 1 1st Qu.:24.59 ## Peninsula:252 SBP-C : 18 Const : 11 101_2A : 1 Median :26.25 ## SCBP-A: 75 12 : 10 101_3A : 1 Mean :26.25 ## SON-B : 36 153 : 10 101_4A : 1 3rd Qu.:27.53 ## 157 : 10 101_5A : 1 Max. :29.33 ## (Other):292 (Other):357 ## Longitude LTRS WNT EN EF ## Min. :-114.3 01:01 :147 03:03 :108 01:01 :225 01:01 :219 ## 1st Qu.:-113.0 01:02 : 86 01:01 : 82 01:02 : 52 01:02 : 52 ## Median :-111.5 02:02 :130 01:03 : 77 02:02 : 38 02:02 : 90 ## Mean :-111.7 02:02 : 62 03:03 : 22 NA&#39;s : 2 ## 3rd Qu.:-110.5 03:04 : 8 01:03 : 7 ## Max. :-109.1 (Other): 15 (Other): 16 ## NA&#39;s : 11 NA&#39;s : 3 ## ZMP AML ATPS MP20 ## 01:01 : 46 08:08 : 51 05:05 :155 05:07 : 64 ## 01:02 : 51 07:07 : 42 03:03 : 69 07:07 : 53 ## 02:02 :233 07:08 : 42 09:09 : 66 18:18 : 52 ## NA&#39;s : 33 04:04 : 41 02:02 : 30 05:05 : 48 ## 07:09 : 22 07:09 : 14 05:06 : 22 ## (Other):142 08:08 : 9 (Other):119 ## NA&#39;s : 23 (Other): 20 NA&#39;s : 5 This species also has a data set for AFLP markers we will use in the very last day of the workshop. These were derived from a subset of populations for reasons we will cover over the week. In working with data in R, the gstudio library has added several convenience functions that help in manipulation and analyses. Many of the functions in the package make some assumptions about how you have configured your data. These assumptions are here only because I am a very lazy typist and the less I have to type the better my life is. By default, I use the convention that the column of data designating your population labels is called Population. I also assume that spatial coordinates are in decimal degrees and are labeled as Longitude and Latitude. If you use names other than these, you will have to specify what the ‘stratum’ variable is and what the coordinate labels are for your data.frame of genotypes. However, if you use those conventions then you do not need to specify anything and any function that needs stratum or coordinates will have it already set as a default. Here are some easy convenience functions. Spatial coordinates for strata can be extracted using strata_coordinates(), which takes the centroid of all coordinates in each locale. In the arapat data set, all the individuals in the same population have the same spatial coordinate. Here is what those data look like. Also look at the use of the datatable here, if you present your data in HTML5, you get interactivity! This can be very helpful in your analyses and interactions with colleagues, advisers, and students. library(DT) coords &lt;- strata_coordinates( arapat ) datatable(coords, rownames = FALSE) Creating maps of locales are also such a common feature that gstudio has integration with online mapping providers, allowing you to grab some map tiles from Google, OSM, and other map providers and use them to overlay your data. Just as we did yesterday, the ggplot package provides the basic plotting and gstudio provided two geometry layers, geom_strata and geom_strata_label to help out. Notice here also that for both of these layers, you just pass the raw data.frame object to it and as long as you have “Population”, “Latitude”, and “Longitude” columns in it, the functions take care of the plotting for you. library( ggmap ) library( ggplot2 ) map &lt;- population_map(coords) p &lt;- ggmap( map ) p &lt;- p + geom_strata(data=arapat) + geom_strata_label( data=arapat ) p 2.4 Allele Frequencies At a base level, the frequency of alleles constitute the most basic type of data relevant to population genetic analyses. You can estimate allele frequencies at several different levels from your data. freqs &lt;- frequencies( arapat ) datatable( freqs, rownames = FALSE ) By default, function in gstudio return the data in a normal data.frame object (most cases) or as a matrix (in the case of genetic distances). This ensures that your workflow can proceed in a normal fashion. However, at times, things like allele frequencies are best handled as matrix objects (e.g., if you are going to use the allele frequencies in multivariate analyses or ordination). For that purpose in particular, we can use the function frequency_matrix freqs_mv &lt;- frequency_matrix( arapat, locus = c(&quot;LTRS&quot;,&quot;ZMP&quot;) ) datatable( freqs_mv, rownames=FALSE) creates a wide data.frame format for analyses. Also notice that I included a specification on which loci I was interested in using—by default it uses all of them for most analysis functions. If you don’t need all the data, no need to calculate all the parameters and then shuffle through all the output. Individual stratum-level visualization may help interpret landscape-level patterns of genetic structure. To estimate allele frequencies along partitions in your data, you must specify the stratum to use a populations. Here I use the frequency of one of the two alleles at the LTRS locus, estimate by “Population”. f &lt;- frequencies(arapat,loci=&quot;LTRS&quot;,stratum=&quot;Population&quot;) f &lt;- f[ f$Allele==&quot;01&quot;,] head(f) ## Stratum Locus Allele Frequency ## 1 101 LTRS 01 0.2777778 ## 3 102 LTRS 01 0.3125000 ## 5 12 LTRS 01 0.8000000 ## 7 153 LTRS 01 0.1500000 ## 11 159 LTRS 01 0.8888889 ## 13 160 LTRS 01 0.8000000 whose values can be used directly in ggplot as once we merge in the two data.frame objects, combining coordinates and frequencies (to use merge you must have a common column in your data.frame that can serve as an index). The frequency can be displayed as the size of the point coords &lt;- merge( coords, f[,c(1,4)]) ggmap( map ) + geom_point( aes(x=Longitude,y=Latitude,size=Frequency),data=coords) or the color of the point ggmap( map ) + geom_point( aes(x=Longitude,y=Latitude,color=Frequency),data=coords) depending upon what you think may be more effective at displaying the spatial patterns. Another useful display involves taking your frequency data and overlaying it onto Google maps as pie charts. This give you a wide view of all your data. It does, however, require an internet connection… pies_on_map( arapat, locus=&quot;LTRS&quot;) This should pop up a browser window and show you the output. Note the messages about the approximation. This is because the Google maps API has an integer for zoom factor and at times it is not able to get all the points into the field of view using an integer zoom. If this happens to you, you can manually specify the zoom as an optional argument to either function pies_on_map() or population_map(). You also need to be careful with the pies_on_map() function because the way it works is that the background tile is plotted and then I plot the pies on top of it. If you reshape your plot window outside equal x- and y- coordinates (e.g., make it a non-square figure), the spatial location of the pie charts will move! This is a very frustrating thing but it has to do with the way viewports are overlain onto graphical objects in R and I have absolutely no control over it. So, the best option is to make the plot square, export it as PNG or TIFF or whatever, then crop as necessary. 2.5 Hardy-Weinberg Equilibrium At the heart of population genetics is the expectation that genotypes will occur at predictable frequencies given a set of assumptions about the underlying population. This is formulated in the enigmatic Hardy-Weinberg Equation, \\(p^2 + 2pq + q^2 = 1\\). In this section, we delve in to what that means, where it comes from, and how we can gain biological inferences from data that do not conform to these expectations. The underlying idea of Hardy-Weinberg Equilibrium (HWE for brevity) is that genotypes should occur at frequencies as predicted by probability theory alone IF (and this is the big part), the population we are looking at is mating in particular ways. Lets spend a little time talking about what that means and what these assumptions actually are. In addition to analyzing your data, you can simulate data in gstudio to mimic several of the processes we will discuss in this section. We will start with the frequencies we calculated in the previous example for the ZMP locus and create a random population with those frequencies. freqs &lt;- frequencies( arapat, stratum=&quot;Species&quot;, loci=&quot;ZMP&quot;) freqs &lt;- freqs[ freqs$Stratum == &quot;Peninsula&quot;,] locus1 &lt;- make_loci(freqs, N = 100) data &lt;- data.frame( ID=1:100, Locus=locus1 ) summary( data) ## ID Locus ## Min. : 1.00 01:01 : 9 ## 1st Qu.: 25.75 01:02 :41 ## Median : 50.50 02:02 :50 ## Mean : 50.50 ## 3rd Qu.: 75.25 ## Max. :100.00 2.5.1 Genetic Assumptions Implicit in the description above is a characterization of what constitutes a genotype. For simplicity, as this is the way it was originally defined, we will assume that the species we are looking at is diploid, carrying homologous chromosomes from both parents. Diploidy means that in the adult life stage, each individual has two copies of each allele, one from each parental individual. If diploid is true, we can denote the genotype as \\(AA\\) for the diploid homozygote, \\(AB\\) for the heterozygote, and \\(BB\\) for the other diploid homozygote. If it is not true and the species (at least at that life stage) is haploid, then will will represent the genotype as \\(A\\) or \\(B\\). Higher levels of ploidy (triploid, tetraploid, hexaploid, etc.) are also possible in many taxa. While the math that follows is essentially the same for these ploidy levels, the actual algebra is a bit more messy. If you are working on taxa with higher ploidy levels, you can do many of the same operations we will focus on in gstudio, consult the documentation for more information. To check the ploidy level in your data (e.g., the number of alleles at a locus) we can use the ploidy() function ploidy(data$Locus ) ## [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [36] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [71] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 In addition to ploidy, genetic constraints on the formulation of the classic HWE assume that the species has two separate sexes, both of which contribute to the next generation. In general, this rules out uniparentally inherited markers such as mtDNA or cpDNA, which are transmitted in most cases by only one of the sexes during reproduction. Examining mating events between diploid individuals is something we should all remember from basic biology and genetics through the use of the Punnet Square. That was a tool designed to teach us about sexual mating, however, it is also a great example of the probabilities of mating we see in sexual reproduction. These probabilities play direct roles in the derivation of HWE. Two additional constraints arise directly from diploid sexual reproduction. First, we assume that the likelihood of a particular allele is independent of the sex of the individual that contains it. For example, male individuals should have as many alleles (e.g., no sex chromosomes that mix ploidy of genotypes) and alleles at that can occur with equal frequency as the other sex (no sex biased alleles probabilities as may arise from say sex-biased lethals). Finally, we will invoke both of Mendel’s Laws of Inheritance. His first law states that the alleles at a particular locus are statistically independent from each other. If there is an A allele provided by one parent, the probability of the second allele is either \\(A\\) or \\(B\\) is entirely independent of that first alleles state. The second law, though not necessarily relevant for many HWE applications, deals with the probability of genotypes at two loci. Here we assume that the alleles present at one locus are statistically independent of those at the other locus. We will return to this when we get to linkage but for now lets assume it is a valid assumption. 2.5.2 Punnet Squares A Punnet square is a simple tool used to teach transition probabilities in basic genetics and biology courses. Here is an interactive example using the 2-allele locus (though the tool can handle other ploidy levels) that you can play with to see the kinds of offspring genotypes produced by individual mating. The population we are examining need to have some specific demographic parameters for HWE to be valid. The most basal assumption that is invoked is that the population size must be very large. In a fact, most of the work of R.A. Fisher and J.B.S. Haldane in developing the modern synthesis relied upon this assumption as well. If population size, \\(N\\), is too small, then the stochastic change (what we call genetic drift) can have serious influences on allele frequencies. As a simulation tool, it is very important to understand that when doing stochastic approximations, \\(N\\) has a huge effect. Consider the following: genotype_frequencies( data$Locus ) ## Genotype Observed Expected ## 1 01:01 9 8.7025 ## 2 01:02 41 41.5950 ## 3 02:02 50 49.7025 Here the expected counts are \\(Np^2\\), \\(N2pq\\), and \\(Nq^2\\), conforming to the expectations from HWE, though they do differ a smidge (vernacular for small amount) from the true expectations. When you do simulation work, you need to accept this variability as a component of the system. However, reality also conforms to these same issues, populations are not infinite, populations are finite in size, etc. Perhaps the issue is our portrayal of the natural world using conventions like \\(N=\\infty\\), the genetic neighborhood is defined by a perfect circle with a constant radius, and dispersal follows a continuous and isotropic distance function… These are great to make the equations better for theoreticians but may not capture the kinds of dynamic processes that are most important to landscape genetic studies. Next, and perhaps this is more for our laziness than any other reason, we assume that the generations do not overlap. This means that the during a single generation, all individuals participate in the mating events, each of which is equally likely to be selected as one participating in a mating event (e.g., random mating). For simplicity, lets denote the frequencies of these genotypes as \\(f(AA) = P\\), \\(f(AB) = Q\\), and \\(f(BB) = R\\). If these individuals are really mating randomly with respect to their genotypes, then the probability of union between any pair is simply the product of their population frequencies. AA AB BB AA \\(P^2\\) \\(PQ\\) \\(PR\\) AB \\(PQ\\) \\(Q^2\\) \\(QR\\) BB \\(PR\\) \\(QR\\) \\(R^2\\) More importantly for our simplicity in terminology, at the end of the mating episode, all of the next generations offspring are produced and all adults die. If there were overlapping generations or multiple mating events, we would have to integrate these processes along a continuous timescale rather than treating a single mating generation as a discrete unit. Totally possible, and even fun, but not part of the original formulation of HWE. Notice how the frequencies changed across mating events (that is exactly what we call genetic drift). The smaller the population size, the larger the deviation across generations. 2.5.3 Evolutionary Assumptions Finally, we need to invoke evolutionary assumptions to meet the requirements of HWE. First, we must assume that there is \\(AB\\)solutely no mutation. A non-zero mutation rate, \\(\\mu &gt; 0\\), means that the state of a particular allele, say the \\(A\\) allele, has a likelihood of spontaneously becoming something other than the \\(A\\) allele (e.g., the \\(B\\) in this formulation). If HWE is to help us determine the genotype frequencies having one allele spontaneously mutate to another would require that we integrate the mutation rate into HWE directly. Perhaps not that surprisingly, that wasn’t integrated back in the day. The next assumption has a very similar consequence. Namely, we must assume that the set of individuals that are participating in the mating event in this population is static. Individuals from other populations are not immigrating into this population and individuals within the population are not emigrating out of this population. Just like mutation, and when we get to migration we will see directly how similar these processes are, we will take the easy way out and assume that they do not occur. The final assumption is based upon selection and is a very easy one to deal with. We simply assume it doesn’t happen. If selection were operating, say an extreme form such as increased lethality of the \\(AA\\) genotype prior to reproduction, then we would have to both specify the way in which selection is operating as well as the its magnitude. We will come back to this topic later but for the simplicity of HWE, we make the assumption that it has no effect. 2.5.4 The Mechanics In a sampled population, we estimate the frequencies of the genotypes as: \\(P = \\frac{N_{AA}}{N}\\), \\(Q = \\frac{N_{AB}}{N}\\), and \\(R = \\frac{N_{BB}}{N}\\) where \\(N_{XX}\\) is the number of \\(XX\\) genotypes and \\(N\\) is the total number of individuals in the sample. Using either \\(table()\\) or \\(genotype_frequencies\\) provides the same information. From these genotype frequencies, we can directly estimate the frequency of each allele (\\(A\\) and \\(B\\)) denoted as p and q as: \\(f(A) = P + \\frac{Q}{2} = p\\) and \\(f(B) = R + \\frac{Q}{2} = q\\) using the lower case versions of \\(p\\) and \\(q\\). This is exactly what the frequencies function actually does. Be careful about mixing up the upper and lower case variables. The population geneticist John Nason, makes the connection that this is what it means to “mind our p’s and q’s” though I suspect the etiology of this statement has more to do with liquid volume measurements than population genetics… Intuitively the formulation denoted above for \\(p\\) and \\(q\\) makes sense as the heterozygote genotype (\\(AB\\)) frequency, \\(Q\\), is split evenly between the homozygote genotype frequencies, \\(P\\) (for \\(AA\\)) and \\(R\\) (for \\(BB\\)). This can be programatically shown as via the transition_probability() function (takes offspring, mom and dad genotypes): AA &lt;- locus( c(&quot;A&quot;,&quot;A&quot;) ) Aa &lt;- locus( c(&quot;A&quot;,&quot;a&quot;) ) aa &lt;- locus( c(&quot;a&quot;, &quot;a&quot;) ) transition_probability( AA, Aa, Aa ) ## [1] 0.25 transition_probability( Aa, Aa, Aa ) ## [1] 0.5 transition_probability( aa, Aa, Aa ) ## [1] 0.25 If these are the only two alleles in the population then there is the additional restriction that \\(p + q = 1\\). It is easy to expand these methods to more than two alleles, but again the original approximation was specifically set for 2-allele systems. 2.5.5 Trans-Generational Transition Probabilities If the above expectations hold, then it is possible to iterate through all possible combinations of matings, noting the probability of observing each type, and estimate the frequency of offspring genotypes. This is a bit messy and is probably better displayed in tabular format. Parents \\(\\mathbf{P(Parents)}\\) \\(\\mathbf{P(AA)}\\) \\(\\mathbf{P(AB)}\\) \\(\\mathbf{P(BB)}\\) \\(AA\\;x\\;AA\\) \\(P^2\\) \\(P^2\\) \\(AA\\;x\\;AB\\) \\(PQ\\) \\(PQ/2\\) \\(PQ/2\\) \\(AA\\;x\\;BB\\) \\(PR\\) \\(PR\\) \\(AB\\;x\\;AB\\) \\(Q^2\\) \\(Q^2/4\\) \\(Q^2/2\\) \\(Q^2/4\\) \\(AB\\;x\\;BB\\) \\(QR\\) \\(QR/2\\) \\(QR/2\\) \\(BB\\;x\\;BB\\) \\(R^2\\) \\(R^2\\) During the parents generation, the frequencies of \\(AA\\), \\(AB\\), and \\(BB\\) were \\(P\\), \\(Q\\), and \\(R\\), defining allele frequencies as \\(p\\) and \\(q\\). If we invoke all those assumptions about genetic, demographic, and evolutionary processes being absent from mating and production of offspring genotypes (e.g., notice how we do not have any terms in there for mutation, being sex biased, etc.) then the frequency of genotypes at the next generation are defined in the table. For example, at the next generation, say \\(t+1\\), the frequency of the \\(AA\\) offspring in the population is the sum of probabilities of all matins that produced \\(AA\\) genotypes. \\[ \\begin{aligned} P_{t+1} &amp;= P_{t}^2 + \\frac{P_tQ_t}{2} + \\frac{Q^2_t}{4} \\\\\\\\ &amp;= \\left( P_t + \\frac{Q_t}{2} \\right)^2 \\\\\\\\ &amp;= p^2 \\end{aligned} \\] The other homozygote is the same producing an expectation of the frequency of the \\(BB\\) genotype at \\(t+1\\) of \\(q^2\\). Similarly for the heterozygote \\[ \\begin{aligned} Q_{t+1} &amp;= \\frac{P_tQ_t}{2} + 2P_tR_t + \\frac{Q^2_t}{2} + Q_tR_t\\\\ &amp;= 2\\left(P_t + \\frac{Q_t}{2} \\right)\\left(R_t + \\frac{Q_t}{2}\\right)\\\\ &amp;= 2pq \\end{aligned} \\] Putting these together, we have an expectation that the genotypes \\(AA\\), \\(AB\\), and \\(BB\\) will occur at frequencies of \\(p^2\\), \\(2pq\\), and \\(q^2\\), which is exactly what HWE is. More importantly, it takes only one generation of mating under the assumptions of HWE to return all genotype frequencies to HWE! However, the real population frequencies differ a bit due in part (mostly) the finite size of populations. Here we can simulate a generation of mating in the previous data as: next_gen &lt;- mate( data, N=1 ) table( data$Locus ) / nrow( data ) ## ## 01:01 01:02 02:02 ## 0.09 0.41 0.50 table( next_gen$Locus ) / nrow( data ) ## ## 01:01 01:02 02:02 ## 0.10 0.36 0.54 2.5.6 Consequences of HWE The consequences of this are important for our understanding of what processes influence population genetic structure. If none of these forces are operating in the population, then the expectation is that the allele frequencies should predict genotype frequencies. However, if there are some forces that are at work, genetic, demographic, or evolutionary, then the frequencies at which we see these genotypes will deviate from those expectations. Now, biologically speaking, is there any population that conforms exactly to these expectations—–definitely not! However, in many cases populations are large enough to not be influenced by small N, mutation is rare enough to not cause problems, etc. Using HWE, we can gain some insights into the which sets of forces may be influencing the observed genotype frequencies. In large part, we examine these changes as deviations from expectations with respect to loss or gain of heterozygotes. These expectations, as depicted above, define an idealized situation against which we measure our data. This metric was specifically designed to ignore almost every process and feature that population geneticists would be interested in looking at. If our data are consistent with these expectations then we have no support for the operation of any of these processes. Boring result, no? In a larger sense, the entire field of Population Genetics is devoted to understanding how violations of these assumptions influence population genetic structure. If everything was in HWE and none of these processes were operating, there would be very little for us to do besides DNA fingerprinting and we would all be forensic examiners… If some of these features are operating, they do have specific expectations on how they would influence the frequency of genotypes (specifically the homozygotes). Specific examples include: Decrease Heterozygosity Increase Heterozygosity Inbreeding Coding Errors Null Alleles Gametic Gene Flow Positive Assortative Mating Negative Assortative Mating Selection against Heterozygotes Outbreeding Wahlund Effects Selection for Heterozygotes In closing, it should be noted that HWE presented here is easily expandable to loci with more than two alleles as well as for loci with different ploidy levels. The same approaches apply, though it is not as clean to estimate the expectations. In the next section, we look at how we specifically test for HWE in our data. At a more meta level, one could see the entirety of this topic and population genetics as a discipline as understanding how deviations from HWE manifest and testing for the strength of these deviations. It is becoming less common for people to actually test for HWE as there are many ways that you can have a deficiency (or excess) of heterozygotes and it is probably much better to focus on the specific processes that may causing these deviations rather than the magnitude of the deviation alone. That said, it is a nice organizing principle to use HWE as a straw man argument on which to frame the rest of this topic. 2.6 Genetic Diversity At a base level, genetic diversity is the fundamental components upon which evolution operates. Without diversity, there is no evolution and as such species cannot respond to selective pressure. Genetic diversity is a property of sampling locales. It is created and maintained by demographic and evolutionary processes and the history of the organisms being examined. It is also used as a surrogate measure for the consequences of several microevolutionary processes. In this section, we will examine how to estimate genetic diversity within a sample of individuals. Genetic diversity is estimated in R using the function genetic_diversity() contained within the gstudio library. Here is the documentation for this function. We will walk through the various parameters and illustrate their use with this data set. genetic_diversity {gstudio} R Documentation Estimate genetic diversity Description This function is the main one used for estimating genetic diversity among strata. Given the large number of genetic diversity metrics, not all potential types are included. Usage genetic_diversity(x, stratum = NULL, mode = c(\"A\", \"Ae\", \"A95\", \"He\", \"Ho\", \"Fis\", \"Pe\")[2]) Arguments x - A data.frame object with locus columns. stratum - The strata by which the genetic distances are estimated. This can be an optional parameter when estimating distance measures calculated among individuals (default='Population'). mode - The particular genetic diversity metric that you are going to use. The gstudio package currently includes the following individual distance measures: A Number of alleles Ae Effective number of alleles (default) A95 Number of alleles with frequency at least five percent He Expected heterozygosity Ho Observed heterozygosity Fis Wright's Inbreeding coefficient (size corrected). Pe Locus polymorphic index. Value A data.frame with columns for strata, diversity (mode), and potentially P(mode=0). Author(s) Rodney J. Dyer rjdyer@vcu.edu Examples AA 2.6.1 Allelic Diversity At the base level, a collection of alleles in a data set provides an estimate of the breadth of genetic variation present. There are many measures commonly used in population genetics, though \\(A\\), \\(A_{95}\\), and \\(A_e\\) are quite commonly encountered. These terms are defined as: Frequency Independent Diversity - The parameter \\(A\\) measures the raw count of the number of alleles present in the sample. This is a frequency independent estimate of diversity as it does not matter if the allele is only seen once or is seen hundreds of times in the sample. Technically, we define this parameter as \\[ A = \\ell \\] where \\(\\ell\\) is the number of observed alleles. We can estimate allelic diversity by passing the function either a vector of locus objects genetic_diversity( arapat$MP20, mode=&quot;A&quot; ) ## Locus A ## 1 Locus 19 or a data.frame that has one or more columns of data that are locus objects as determined by the column_class() function. genetic_diversity( arapat, mode=&quot;A&quot; ) ## Locus A ## 1 LTRS 2 ## 2 WNT 5 ## 3 EN 5 ## 4 EF 2 ## 5 ZMP 2 ## 6 AML 13 ## 7 ATPS 10 ## 8 MP20 19 The concern associated with using a parameter such as \\(A\\) is that it throws away any information about relative abundance. For example, a data set with alleles found at an abundance of \\(N_A = 999\\) and \\(N_B = 1\\) would have the exact same frequency independent diversity as another sample with \\(N_A = N_B = 500\\), namely \\(A = 2\\). Frequency Dependent Diversity - A slightly more stringent approach may be to estimate the diversity of alleles in a frequency dependent fashion, specifying a particular cutoff. The parameter \\(A_{95}\\) is a common parameter that provides a count of the number of alleles present in a sample whose frequencies exceed a threshold of 5%. \\[ A_{95} = \\left| A_{freq(\\ell)&gt;0.05} \\right| \\] This threshold is entirely arbitrary in the same way that we use \\(\\alpha=0.05\\) as a cutoff in the teaching of formalized hypothesis testing. It could just as easily be 3%, 1%, or 10% but has a historical trajectory in its current form. The \\(A_{95}\\) parameter does remove some of the problems associated with rare alleles in our data sets as is shown by comparing the diversity of the most allele rich locus MP20 using \\(A\\) mp20.A &lt;- genetic_diversity( arapat$MP20, mode=&quot;A&quot;) mp20.A ## Locus A ## 1 Locus 19 and \\(A_{95}\\) mp20.A95 &lt;- genetic_diversity( arapat$MP20, mode=&quot;A95&quot;) mp20.A95 ## Locus A95 ## 1 Locus 4 Here 15 of the alleles we observed in the full data set occurred at a frequency less than 5%! Effective Number of Alleles - Given the arbitrary nature of the cutoff for \\(A_{95}\\), a more parsimonious approach may by one that measures the contribution of each allele to diversity in proportion to its frequency. This is \\(A_e\\) and it is defined as: \\[ A_e = \\frac{1}{\\sum_{i=1}^\\ell p_i^2} \\] and is the default mode of diversity provided by the genetic_diversity() function (e.g., the one you get if you do not specify mode=). genetic_diversity( arapat, mode=&quot;Ae&quot;) ## Locus Ae ## 1 LTRS 1.995623 ## 2 WNT 2.880450 ## 3 EN 1.814656 ## 4 EF 1.773533 ## 5 ZMP 1.513877 ## 6 AML 5.860583 ## 7 ATPS 3.563347 ## 8 MP20 5.511837 Notice that \\(A \\ge A_e \\ge A_{95}\\), as \\(A_e\\) integrates the contributions of those low frequency alleles. None of these parameters are perfect, though all provide some measurement of diversity at the level of the allele. 2.6.2 Genotypic Diversity In addition to diversity at the level of alleles, we also can estimate diversity at the level of the genotype. There are several parameters that can be derived for genotypic diversity but the most common are observed and expected heterozygosity. Observed heterozygosity is simply the fraction of all genotypes in the sample that are not homozygotes. \\[ H_O = \\frac{N_{ij}}{N} \\] where \\(N_{ij}\\) is the count of all heterozygous genotypes. The mode=&quot;Ho&quot; option in genetic_diversity() returns these values. genetic_diversity(arapat, mode=&quot;Ho&quot;) ## Locus Ho ## 1 LTRS 0.23691460 ## 2 WNT 0.27840909 ## 3 EN 0.20555556 ## 4 EF 0.14404432 ## 5 ZMP 0.15454545 ## 6 AML 0.37058824 ## 7 ATPS 0.08539945 ## 8 MP20 0.44692737 The second parameter, expected heterozygosity (\\(H_e\\)), is slightly different in that it is a parameter that needs to be estimated from data. It represents the fraction of genotypes in the data set that are expected to be heterozygous, if the sample is in equilibrium. The value for this function depends upon the data that you are estimating and how it is configured. If it is estimated from a single locale, then the parameter can be estimated directly as the expected fraction that are not homozygous. \\[ H_E = 1 - \\sum_{i=1}^\\ell p_i^2 \\] (where \\(\\ell\\) is the number of alleles at the locus). Here is an example of expected heterozygosity for the LTRS and MP20 loci. genetic_diversity( arapat$LTRS , mode=&quot;he&quot;) ## Locus he ## 1 Locus 0.4989034 genetic_diversity( arapat$MP20, mode=&quot;he&quot; ) ## Locus he ## 1 Locus 0.8185723 In an ideal world, we would be able to have enough resources to sample for these parameters with so many individuals, they would be well behaved. Unfortunately, we do not live in the real world and in many cases our ability to sample lots of individuals is limited. This problem is not a new thing, we estimate the sums of square and divide by \\(N-1\\) instead of \\(N\\) because if we we use the later, we are producing a biased estimate. Unless you are working with all the samples, every time we make population genetic inferences we will be doing so on a sub-sample of the total data available. The arapat data set is no where near a major component of the species, it is only a sample. From this sample, we estimate allele frequencies on which we derive our estimates of parameters such as \\(H_E\\). As such, if we used the formulas above to estimate everything in every study, we would have biased estimators of… pretty much everything. However, the fix is not one that is too difficult to put in, and it has a pretty well established set of behaviors. As such, if we are estimating these parameters in a single locale (for more than one location, we have further corrections to make), we should probably apply a small sample size correction. In general, this correction is: \\[ \\hat{H}_e = \\frac{2N}{2N-1}\\left[ 1-\\sum_{i=1}^\\ell p_i^2 \\right] \\] The front part is a small sample size correction factor. Its importance in your estimation diminishes as \\(N \\to \\infty\\) as shown below. Once you get above \\(N=9\\) individuals, there is an inflation of the estimated heterozygosity at a magnitude of less than 5%. Figure 2.1: Magnitude of the correction factor for small sample size estimations as a function of N. 2.6.3 Samples from Several Locales In addition to problems associated with estimating allele frequencies incorrectly (and requiring a sample size correction), when we estimate data from several locations, we also have a problem associated with the subset of locales relative to the total population size, resulting in a further correction to account of the several biased samples you are taking. \\[ H_S = \\frac{\\tilde{N}}{\\tilde{N}-1}\\left[ 1 - \\sum_{i=1}^\\ell \\frac{p_i^2}{K} - \\frac{H_O}{2\\tilde{N}} \\right] \\] where \\(\\tilde{N}\\) is the harmonic mean number of genotypes sampled across each of the \\(K\\) strata. Notice here I use the term \\(H_S\\) instead of \\(H_E\\) so that there isn’t any doubt about the differences when we write and talk about these parameters. This formulation (after Nei 1987) corrects for the sampling across separate locations. To indicate that your estimate is being made using subdivided groups of samples, pass the stratum= parameter to genetic_diversity() and set `mode=“Hes”. We can see the magnitude of the correction by looking at a single population and comparing the estimates of \\(H_S\\) for corrected and non-corrected parameters. pops &lt;- arapat[ arapat$Population %in% c(&quot;32&quot;,&quot;101&quot;,&quot;102&quot;),] he &lt;- genetic_diversity( pops, mode=&quot;He&quot;) hes &lt;- genetic_diversity(pops, stratum=&quot;Population&quot;, mode=&quot;Hes&quot;)[1:8,] df &lt;- data.frame(Locus=he[,1], He=he[,2], Hes=hes[,2]) df ## Locus He Hes ## 1 LTRS 0.38850309 0.41230882 ## 2 WNT 0.03277778 0.01843238 ## 3 EN 0.50285714 0.44289781 ## 4 EF 0.27119377 0.31900021 ## 5 ZMP 0.09500000 NA ## 6 AML 0.53854875 0.68269231 ## 7 ATPS 0.25038580 0.27105726 ## 8 MP20 0.80670340 0.81562830 They are pretty close (Pearson’s \\(\\rho =\\) 0.970), even when there are only 36 individuals in the sample.2 But they are off and this is a vitally important distinction because if you do not account for these differences you will percolate these errors up through your subsequent analyses (and this is a bad thing). 2.6.4 Multilocus Diversity There are several measures of individual locus diversity but few for multilocus diversity. One potential measure for diversity across loci is to based upon the fraction of population that has unique multilocus genotypes. This is defined as: \\[ D_m = \\frac{N_{unique}}{N} \\] and can be estimated using the function mulitlocus_diversity(). Looking across the putative species indicated in the data set, we can see that in general, the Cape populations are much less diverse than those individuals samples throughout Baja California. multilocus_diversity( arapat[ arapat$Species==&quot;Cape&quot;,]) ## [1] 0.56 multilocus_diversity( arapat[ arapat$Species==&quot;Mainland&quot;,] ) ## [1] 0.9722222 multilocus_diversity( arapat[ arapat$Species==&quot;Peninsula&quot;,] ) ## [1] 0.9007937 This is a pretty crude measurement but later when we examine models based upon conditional multilocus genetic distances, we need to make sure that the samples are both allelic rich and multilocus diverse and this approach is a nice way to do that. 2.6.5 Inbreeding Inbreeding is one of the most common deviations from random mating (and hence Hardy-Weinberg Equilibrium) that we encounter in natural populations. Inbreeding is defined, sensu stricto, as mating between two related individuals. This ranges from complete selfing, where one parent alone produces offspring, to consanguineous mating, where individuals with some degree of relatedness produce offspring. The genetic consequences of inbreeding are entirely in how alleles are arranged into genotypes, not in changing allele frequencies. The primary consequence of inbreeding is a reduction in the frequency of the heterozygous genotype. Consider the following Punnet square where a heterozygote is producing a selfed offspring. \\(A\\) \\(B\\) \\(A\\) \\(AA\\) \\(AB\\) \\(B\\) \\(AB\\) \\(BB\\) The offspring in the next generation are only 50% heterozygotes. Each generation of selfing, homozygotes produce homozygotes but only half of the offspring from heterozygotes stay as such. This process increases the relative frequency of homozygous genotypes in the population, though if you look at the offspring, the frequency of alleles do not change—there are as many A alleles as B alleles in the next generation of selfing. Conceptually, we can define an inbreeding parameter, \\(F\\), depicting the extent to which genotype frequencies have deviated from HWE due to inbreeding. But to do so, we need to differentiate between homozygote genotypes that have the same alleles because they are inbred (e.g., both alleles can be traced to a single allele in common ancestor) from those that are identical because they just happen to have the same allele (not of common ancestry). For this, we will define terms for these genotypes as: Autozygous - Two alleles that are identical within a genotype because they came from the same individual in the previous generation. These alleles are Identical by Descent and will be found in the population at a rate of \\(pF\\). Allozygous - Two alleles that are identical within a genotype but they came from alternate individuals in the parental generation. These alleles have Identity by State and are expected to occur at a frequency of \\(p^2(1-F)\\). Together, the expected frequency of the AA genotype, \\(E[AA] = p^2(1-F) + pF\\). At the extremes, if there is no inbreeding—all homozygotes are allozygous—the inbreeding statistic, \\(F=0\\) and the expectation reduces to \\(E[AA] = p^2\\). Conversely, if all offspring are the result of selfing, \\(F=1\\) and \\(E[AA] = p\\). Often the parameter \\(F\\) is the subject of our analyses and the item that is to be estimated from genetic data. Give the definition above, \\(F\\) is defined as the proportional loss of heterozygosity and is estimated as: \\[ \\begin{aligned} F &amp;= \\frac{H_e - H_o}{H_e} \\\\ &amp;= 1 - \\frac{H_o}{H_e} \\end{aligned} \\] Programatically, we can estiamte the inbreeding statistic, \\(F\\), using genetic_diversity(). Technically, this statistic is referred to as \\(F_{IS}\\), the subscripts indicating that the level of inbreeding is in the Individual relative to the expectations of the Subpopulation. There are other levels of inbreeding we will estimate below, which is why we need to make this distinction. genetic_diversity( data, mode=&quot;Fis&quot;) ## Locus Fis ## 1 Locus 0.0143046 genetic_diversity( arapat, mode=&quot;Fis&quot;) ## Locus Fis ## 1 LTRS 0.5251293 ## 2 WNT 0.5735364 ## 3 EN 0.5421225 ## 4 EF 0.6697396 ## 5 ZMP 0.5447106 ## 6 AML 0.5531682 ## 7 ATPS 0.8812849 ## 8 MP20 0.4540160 2.7 Rarefaction The primary reason for looking at diversity is to perform some comparison, which provides some insights into the biological and/or demographic processes influencing your data. Without a basis for comparison, diversity estimates are just numbers. However, deriving an estimate of diversity is a statistical sampling process and as such we must be aware of the consequences our sampling regime has on the interpretation of the data. This is where rarefaction comes in, a technique commonly used in ecology when comparing species richness among groups. Here is an example of the problem sampling may interject into your analyses. Consider a single locus with four alleles. library(gstudio) data(arapat) f &lt;- data.frame( Allele=LETTERS[1:4], Frequency=0.25) f ## Allele Frequency ## 1 A 0.25 ## 2 B 0.25 ## 3 C 0.25 ## 4 D 0.25 Selected as a random sample from an infinite population. The first sample has 5 individuals. pop1 &lt;- make_population( f,N=5) ae1 &lt;- genetic_diversity( pop1, mode=&quot;Ae&quot; ) ae1 ## Locus Ae ## 1 Locus 3.846154 And the second one has 100 individuals. pop2 &lt;- make_population( f, N=100 ) ae2 &lt;- genetic_diversity( pop2 ) ae2 ## Locus Ae ## 1 Locus 3.9992 The difference in estimated diversity among these groups are abs( ae1 - ae2 ) = 0.15. Is this statistically different or are they the same? Is it just because we sampled more individuals in the second set that we get higher values of \\(A_e\\)? Consider the MP20 locus in the beetle data set, it has a total of 19 alleles present. If we subsample this data and estimate the number of observed alleles, we see that there is an asymptotic relationship between sampling effort and estimates of allelic diversity. Here is the code for estimating frequency independent diversity, \\(A\\), using these data. loci &lt;- arapat$MP20 sz &lt;- c(2,5,10,15,20,50,100) sample_sizes &lt;- rep( sz, each=20 ) Ae &lt;- rep(NA,length(sample_sizes)) for( i in 1:length(Ae)){ loci &lt;- sample( loci, size=length(loci), replace=FALSE) Ae[i] &lt;- genetic_diversity( loci[1:sample_sizes[i]], mode=&quot;Ae&quot; )$Ae } The ‘curvy’ nature of this relationship shows a few things. It takes a moderate sample size to capture the main set of alleles in the data set. If we are looking at allocating sampling using only 5 individuals per locale, then we are not going to get the majority of the alleles present. For the rare alleles, you really need to grab large at-site samples if estimates of diversity are the main component of what you are doing. Do rare alleles aid in uncovering the biological processes you are interested in studying? They may or may not. For most purposes, we will use all the samples we have collected. In many cases though, some locales may not have as many samples as other ones. So, even with these data, if I have one locale with 10 samples and another with 50, how can I determine if the differences observed are due to true differences in the underlying diversity and which are from my sampling? Just as in testing for HWE, we can use our new friend permutation to address the differences. Rarefaction is the process of subsampling a larger dataset in smaller chunks such that we can estimate diversity among groups using the same number of individuals. Here is an example in the beetle dataset where I am going to look at differences in diversity among samples (\\(N = 75\\)) collected in the cape regions of Baja California ae.cape &lt;- genetic_diversity( arapat[ arapat$Species==&quot;Cape&quot;, &quot;WNT&quot;] ) ae.cape ## Locus Ae ## 1 Locus 1.305052 and compare those to the genetic diversity observed from a smaller collection of individuals sampled from mainland Mexico (\\(N=36\\)). ae.mainland &lt;- genetic_diversity( arapat[ arapat$Species==&quot;Mainland&quot;, &quot;WNT&quot;] )$Ae ae.mainland ## [1] 1.033889 The observed difference in effective allelic diversity, \\(A_{e,mainland} == A_{e,cape}\\), could be because the Cape region of Baja California is more diverse or it could be because there are twice as many individuals in that sample. To perform a rarefaction on these data, we do the following: Use the size of the smallest population (\\(N\\)) as the sample size for all estimates. Randomly sample individuals, without replacement, from the larger dataset in allocations of size \\(N\\). Estimate diversity parameters on these subsamples and repeat to create a ‘null distribution’ of estimated diversity values. Compare your observed value in the smallest population to that distribution created by subsampling the larger population. From the data set, this is done by cape.pop &lt;- arapat[ arapat$Species==&quot;Cape&quot;,&quot;WNT&quot;] null.ae &lt;- rarefaction( cape.pop, mode=&quot;Ae&quot;,size=36) mean(null.ae) ## [1] 1.311998 So even if the samples sizes are the same, the mean level of diversity remains relatively constant. The range in diversity range(null.ae) ## [1] 1.028163 1.769283 is quite large. Since this estimate is frequency based, random samples of alleles change the underlying estimate of Ae during each permutation. The observed estimate of diversity in the Mainland populations does fall within this range. However, the null hypothesis states that \\(A_{e,mainland} = A_{e,cape}\\) and if this is true, once we standardize sample size, we can take the distribution of permuted Ae values as a statement about what we should see if the null hypothesis were true. As such, we can treat it probabilistically and estimate the probability that \\(A_e\\), Mainland is drawn from this distribution. all.ae &lt;- c( null.ae, ae.mainland) P &lt;- sum(ae.mainland &lt;= null.ae) / ( length(all.ae) ) P ## [1] 0.995 Or graphically, it can be depicted as below. 2.7.1 Mapping Diversity Estimating diversity is great and being able to compare two or more groups for their levels of diversity is even better. But often we are looking for spatial patterns in our data. Both R and gstudio provide easy interfaces for plotting data and later in the text we will see how to integrate raster and vector data into our workflows for more sublime approaches to characterizing population genetic processes. In the mean time, it is amazingly easy to use basic plotting commands to get pretty informative output. In this example, I extract the mean coordinate of each stratum in the arapat dataset and then estimate diversity at the level of these partitions and merge the diversity estimates for the AML locus into the coordinate data.frame. library(gstudio) data(arapat) diversity &lt;- genetic_diversity(arapat, stratum=&quot;Population&quot;, mode=&quot;Ae&quot;) coords &lt;- strata_coordinates(arapat) coords &lt;- merge( coords, diversity[ diversity$Locus == &quot;AML&quot;, ] ) Then, I grab a map from the Google server and map my populations with diversity depicted as differences in the size of the points. Note that the ggmap() function provides the base map that is retrieved but when we use geom_point() we need to specify the aes() and the data= part as these data are from the data.frame we made, not from the map we grabbed from Google. library(ggmap) map &lt;- population_map(coords) ggmap(map) + geom_point( aes(x=Longitude,y=Latitude,size=Ae), data=coords) 2.8 Genetic Distance One can measure the relative distance between items based upon some basic assumptions. In Euclidean geometry, the underlying distance measures are based upon the triangle inequality. The same kinds of approaches are available in characterizing genetic separation, either among individuals or among locales. This chapter introduces some of the methodologies used in estimating genetic distances, how we analyze them, and how we portray our results in a graphical fashion amenable for interpretation. Underlying most estimates of genetic distance are sets of geometric and/or evolutionary assumptions. There are many different metrics that could be estimated from our genetic data, many of which will tell us the same general story. However, there are specific features of our data that may make one kind of metric more appropriate than another type. There are two main characteristics that control our ability to map collections of genotypes onto a real number line; Self identity and Symmetry. Self identity implies that the distance, \\(\\delta\\), between an object (a single genotype or a collection of genotypes in a population) and itself must be zero: \\(\\delta_{ii}=0\\). You are absolutely identical to yourself. Symmetry implies that the distance between objects is independent of the order in which we measure it; \\(\\delta_{ij} = \\delta_{ji}\\). This can be a more problematic assumption in some cases and when we examine estimation of ecological distances later, we will see this may not be the case. If these two conditions are satisfied, the distance may be considered metric if (and only if) it satisfies the triangle inequality, \\(\\delta_{ij} \\le \\delta_{ik} + \\delta_{kj}\\). We’ve seen this relationship in Euclidean Geometry, the squared length of the hypotenuse is equal to the sum of the squared length of both legs in a triangle; \\(z^2 = x^2 + y^2\\). Genetic distance itself is not the final product that allows us to gain inferences about underlying population genetic processes. Rather, it is the input into subsequent analyses. This section focuses first on how we estimate distances, depending upon the level of interest, and then highlights some approaches for visualizing and gaining inferences from the distance matrix itself. In later chapters, we return to the use of distance matrices when we examine network models and analyses based upon the idea of isolation models commonly used in modern population genetic analyses. 2.8.1 Individual Genetic Distance Genetic distance can be estimated based upon differences at many levels, the most basal of which is among individuals. In the discussion of the individual genetic distances, the following genotypes will be used for illustrative purposes. library(gstudio) AA &lt;- locus(c(&quot;A&quot;, &quot;A&quot;)) AB &lt;- locus(c(&quot;A&quot;, &quot;B&quot;)) BB &lt;- locus(c(&quot;B&quot;, &quot;B&quot;)) BC &lt;- locus(c(&quot;B&quot;, &quot;C&quot;)) AC &lt;- locus(c(&quot;A&quot;, &quot;C&quot;)) CC &lt;- locus(c(&quot;C&quot;, &quot;C&quot;)) Locus &lt;- c(AA, AB, AC, BB, BC, CC) Locus ## [1] &quot;A:A&quot; &quot;A:B&quot; &quot;A:C&quot; &quot;B:B&quot; &quot;B:C&quot; &quot;C:C&quot; 2.8.1.1 Euclidean Distance The most basic distance metric we can use would be Euclidean. Intuitively, we have a good idea about how this metric works as we use it every day. Euclidean distance is estimated as: \\[ \\delta_{eucl} = \\sqrt{\\sum_{i=1}^\\ell (p_{xi}-p_{yi})^2} \\] where \\(p_{x•}\\) is a vector of allele frequencies measured on the \\(x^{th}\\) individual. We can create these by translating the raw genotypes into multivariate data as we in the previous section on individual and population assignment. mv.genos &lt;- to_mv(Locus) mv.genos ## A B C ## [1,] 1.0 0.0 0.0 ## [2,] 0.5 0.5 0.0 ## [3,] 0.5 0.0 0.5 ## [4,] 0.0 1.0 0.0 ## [5,] 0.0 0.5 0.5 ## [6,] 0.0 0.0 1.0 and then using the dist() function that R has built-in to estimate \\(\\delta_{eucl}\\) D.euc &lt;- dist( mv.genos ) D.euc ## 1 2 3 4 5 ## 2 0.7071068 ## 3 0.7071068 0.7071068 ## 4 1.4142136 0.7071068 1.2247449 ## 5 1.2247449 0.7071068 0.7071068 0.7071068 ## 6 1.4142136 1.2247449 0.7071068 1.4142136 0.7071068 The output variable, D.euc, is of type class(D.euc) ## [1] &quot;dist&quot; which is definitely not a matrix object. It is a specific data type that is used for dealing with distance matrices. There are some analyses (variants of AMOVA and the Mantel test come to mind) that require the input matrix to be of type dist rather than of type matrix and don’t do the conversion for you directly. Because most distance matrices have a zero diagonal (\\(1^{st}\\) requirement outlined previously) and symmetry (the upper diagonal is equal to the lower diagonal, the \\(2^{nd}\\) requirement), it is possible to save computer memory by only dealing with the data below the diagonal. Some analyses, such as an implementation of the Mantel test, require the use of dist objects instead of matrices. It is easy to convert between the two object types as necessary. D.euc &lt;- as.matrix( D.euc ) D.euc ## 1 2 3 4 5 6 ## 1 0.0000000 0.7071068 0.7071068 1.4142136 1.2247449 1.4142136 ## 2 0.7071068 0.0000000 0.7071068 0.7071068 0.7071068 1.2247449 ## 3 0.7071068 0.7071068 0.0000000 1.2247449 0.7071068 0.7071068 ## 4 1.4142136 0.7071068 1.2247449 0.0000000 0.7071068 1.4142136 ## 5 1.2247449 0.7071068 0.7071068 0.7071068 0.0000000 0.7071068 ## 6 1.4142136 1.2247449 0.7071068 1.4142136 0.7071068 0.0000000 if you need to perform operations that are not available to dist objects (like matrix algebra). 2.8.1.2 AMOVA Distance Another geometric interpretation of inter-individual distance is that of Excoffier et al. (2004), dubbed AMOVA distance. Smouse &amp; Peakall (1999) provided the geometry of this relationship for diploid loci when discussing spatial autocorrelation. AMOVA distance can be visualized as the distance between vertices and mid-points on an equilateral triangle. Each segment is defined as having unit length and the distance is taken as the square of the distance between genotypes. In R, we calculate it as: D.amova &lt;- genetic_distance( Locus, mode=&quot;AMOVA&quot; ) rownames(D.amova) &lt;- colnames(D.amova) &lt;- as.character( Locus ) D.amova ## A:A A:B A:C B:B B:C C:C ## A:A 0 1 1 4 3 4 ## A:B 1 0 1 1 1 3 ## A:C 1 1 0 3 1 1 ## B:B 4 1 3 0 1 4 ## B:C 3 1 1 1 0 1 ## C:C 4 3 1 4 1 0 For completeness, the previous figure should be drawn as a tetrahedron with the possibility of four alleles at a locus (allowing for distances between two heterozygotes that do not share any alleles). However, that is not an easy thing to draw for me… The only interesting distance you’ll need to think about (and perhaps grab a piece of paper and draw some triangles) is the distance between two two heterozygotes that share no alleles. I’ll leave it up to you to figure out that one (or use genetic_distance() for it). 2.8.1.3 Bray-Curtis Individual Distance A distance metric that can be used to test differences among individuals or populations is that of Bray-Curtis. This distance is a transformation of Jaccard’s distance (see below) and is derived from an ecological ordination paper by Bray &amp; Curtis (1957). \\[ D_C = 1 - 2\\frac{\\sum_{i=1}^\\ell min(p_{X,i},p_{Y,i})}{\\sum_{i=1}^\\ell p_{X,i} + \\sum_{i=1}^\\ell p_{Y,i}} \\] This parameter is bound by 0 on the lower end, indicating that the two items being compared are identical and 1 at the upper end indicating complete dissimilarity. In R, this distance is defined in the vegan package for normal vegetation analysis and in gstudio for genetic data. There is some confusion in the literature as to how this distance metric should be calculated and it is implied by Yoshioka (2008) that at least some of the implementations are actually Czekanowski distance. At the time of this writing, there are some discrepancies between the distances that are calculated in different programs, whose names will be withdrawn to protect the innocent, so be aware. In the end, it is not fealty to a particular distance metric that is important in our analyses, it is the ability of some metric to describe the variation we see. The implementation in gstudio uses allele counts across all loci. 2.8.1.4 Ladder Distance For microsatellite loci, which have a specific step-wise mutation model, a genetic distance metric can be created based upon the number of repeat motif differences at genotypes rather than just binary same/different alleles (as in the AMOVA distance before). If the stepwise mutation process at microsatellite loci has played a significant role in shaping the structure on the landscape then we expect the kinds of spatial signal that is present in a ladder distance approach should differ from an approach that does not take into consideration the unique characteristics of this kind of locus. If they say provide very similar kinds of inferences then it is more likely that mutation, as a process, has not shaped the history to a large extent. 2.8.2 Population-Level Genetic Distances If we are collecting data in such a way as to have pre-defined (or determined ex post facto) groups (populations or locales), several additional distance metrics can be derived. Below are a few of the more common ones that you will run into. It should be said that there is no dearth of genetic distance metrics available. 2.8.2.1 Czekanowski (Manhattan) Distance Perhaps the simplest distance metric is that of Czekanowski—more commonly referred to as Manhattan distance. It is called Manhattan distance because it is a distance metric that is analagous to how you measure city-block distances. It is relatively difficult in a city to walk from one place to another in a straight line, rather we are forced to walk down this street, turn walk down that one, etc. Population allele frequencies can be plot on the linear axis \\(\\sum_{i=1}^\\ell p_i = 1\\) and the distance between populations is the sum of the lengths separating each population on each of the \\(\\ell\\) axis. This is calculated as: \\[ D_{Cz} = \\frac{1}{2}\\left| p_{i,x} - p_{i,y} \\right| \\] where the pipes, \\(|x|\\), indicate taking the absolute value. 2.8.2.2 Rogers (Euclidean) Distance Stepping up, slightly, in complexity, we get to Roger’s distance. This distance metric is roughly equivalent to Euclidean distance (and indeed in gstudio it is called euclidean). Here we consider the allele frequencies as before but instead of taking the absolute values of the distances along each axis, we take the straight-line distance. This is estimated as: \\[ D_R = \\sqrt{ \\frac{1}{2} \\sum_{i=1}^\\ell\\left( p_{i,x} - p_{i,y}\\right)^2 } \\] This is a more common distance metric than that of Czkanowski and has been used in classification and taxonomy. There are a few drawbacks to this distance metric that stem from not being based upon population genetic theory. These may, or may not, be relevant to what you are doing with your data and where your data is from. This distance is not proportional to time of separation of populations. There is no drift component to it. This distance is not proportional to the number of base-pair substitutions, which may be important if you are looking at haplotype divergence. This distance may loose sensitivity with increased allelic diversity. If you are using this metric at short time intervals, have loci of moderately diversity, or are not concerned about homology, this is a quick metric. 2.8.2.3 Nei’s Genetic Distance One of the most common distance metrics used is that of Nei, and is estimated as: \\[ I = \\frac{\\sum_{i=1}^L\\sum_{j=1}^{\\ell_{i}} p_{ij,x}p_{ij,y}}{\\sqrt{\\sum_{i=1}^L\\left(\\sum_{j=1}^{\\ell_i} p_{ij,x}^2\\right)\\sum_{i=1}^L\\left(\\sum_{j=1}^{\\ell_i} p_{ij,y}^2\\right)}} \\] but more commonly referred to using the transform: \\[ D_{Nei} = -\\ln(I) \\] This metric is roughly linear in time, assuming a drift-mutation equilibrium. Populations with longer periods of separation will have larger values of \\(D_{Nei}\\). If you think that drift and mutation are significant features that have shaped the divergence of your populations, then this is an appropriate metric to use. 2.8.2.4 Conditional Genetic Distance There is a final genetic distance that should be discussed here, Conditional Genetic Distance, derived from a network abstraction of genetic covariance by Dyer &amp; Nason (2004). However, both the rationale and the approach that produce these measures of distance rely on some approaches that need further development and will be put off until a more complete treatment can be done in the chapter on population graphs. 2.9 Genetic Structure The term ‘structure’ is used in many different ways in population genetics. Essentially, data has genetic structure if there is a non-random association of genotypes across your sampling locations. The interaction of population genetic processes create the genetic structure we see, though it is not easy to infer historical process from the presence of structure alone. For this we need to rely upon more cleaver experimental design. In this chapter we examine parameters designed to describe the amount of structure present in a dataset and how we can derive some level of inference from their magnitudes. Structure in populations influences the distribution (spatial, ecological, and temporal) distribution of of alleles. Until this point, we have focused on how alleles coalesce into genotypes at the individual level, local allele frequencies and the Hardy Weinberg expansion provide an expectation for the probability of seeing a heterozygote, for example. As we scale up from the individual itself to the deme (or sub-population) in which it resides, we can also estimate heterozygosity. As we coalesce these demes into populations, regions, continents, etc. we can also estimate expectations for heterozygosity at these levels as well. The deviance from these expectations at all levels can be captured in a statistic—the \\(F\\)-statistic we’ve seen already is on of them—useful for comparisons among studies, sites, etc. The \\(F\\)-statistics are perhaps the most misused parameters in population genetics but they have both a long history of use and pretty robust expectations with respect to how they will respond to various evolutionary processes. In this section, we will explore genetic structure, sensu lato, providing examples of single and multilocus statistics and how we currently think they should be used. 2.10 F-Statistics and Thier Ilk The most commonly used parameter for in population genetics are derived from Wright’s \\(F\\)-Statistics. We have already encountered one of them, the inbreeding parameter F, which we calculated using the genetic_diversity() function using mode=&quot;Fis&quot;. This parameter compares the level of observed heterozygosity to that expected under HWE as: \\[ F_{IS} = 1 - \\frac{H_O}{H_E} \\] If the locale in question is producing heterozygotes at a rate consistent with HWE then there is no ‘inbreeding’ (as we defined it previously). This being said, the expected frequency of heterozygotes can be estimated at several different demographic levels if you have data that is subdivided. Assuming you have individuals, sampled from K different locales, you can estimate heterozygosity at the following levels: Individual heterozygosity (\\(H_I\\)): This is what we’ve been calling HO, observed heterozygosity and is estimated as the fraction of heterozygotes in your sample for a particular locale, \\(H_I = \\frac{N_{ij}}{N}\\). This value can be interpreted as the “average heterozygosity of all genes in an individual” or as “the probability of observing a heterozygote at a particular locus.” Subpopulation heterozygosity (\\(H_S\\)): This is the rate of heterozygosity expected at a particular sampling sub-population (or what I refer to in this text as a locale). This is estimated as the average in expected heterozygosity (\\(H_E\\)) across all \\(K\\) sampling locales; \\(H_S = \\frac{1}{K}\\sum_{i=1}^K 2p_iq_i\\). It assumes that each of your sampling locales is in HWE, each conforming to the host of assumptions necessary to satisfy that condition. Total heterozygosity (\\(H_T\\)): This is the total heterozygosity across all the data. This is the expected heterozygosity if all the data were merged and mating as a single panmictic population. This parameter is estimated as \\(H_T = 2\\bar{p}\\bar{q}\\), where the frequencies for each allele are determined as the average across all sampling locales. If the underlying mating patterns among subdivided populations restrict gene flow, then these estimates of heterozygosity will change. At one extreme, if all populations are mixing freely and it is essentially a single panmictic population then heterozygosity at the sub-population and total population levels will be equal, \\(H_T=H_S\\), independent of the amount of inbreeding (or consanguineous mating that is actually occurring). At the other extreme, if all locales are genetically isolated from each other, then each will be diverging its own evolutionary trajectory both sub-population and total estimates of heterozygosity will diverge. In fact, it is exactly these relationships between estimates of heterozygosity that Sewell Wright used to derive the oft-misused \\(F\\)-Statistics. To date, we’ve already used one of these parameters, when examining inbreeding. Our \\(F\\)-statistic is the first demographic level. Using this new terminology for heterozygosity, the inbreeding statistic is defined as: \\[\\begin{aligned} F_{IS} &amp;= \\frac{H_S - H_I}{H_S} \\\\ &amp; = 1 - \\frac{H_I}{H_S} \\end{aligned}\\] where the subscripts on the parameters reveal the sampling level being used. In this case, the subscripts on \\(F_{IS}\\) stand for “inbreeding of _I_ndividuals relative to the _S_ubpopulation they are sampled.&quot; The values of this parameter are positive when we see less heterozygotes than expected, and can be negative when we see more heterozygotes than expected under HWE. This parameter makes particular sense in light of unknown subdivision in populations, the Wahlund Effect discussed earlier. At the next level, we can examine inbreeding of the Subpopulation relative to the Total data set, \\(F_{ST}\\). This is defined as: \\[\\begin{aligned} F_{ST} &amp;= \\frac{H_T - H_S}{H_T} \\\\ &amp;= 1 - \\frac{H_S}{H_T} \\end{aligned}\\] This parameter has been so misused in the literature, it is almost a caricature of itself. As we see from the formula, it is the reduction in heterozygosity of the subpopulation relative to the entire data set. This parameter can be either positive or negative. Wright’s original formulation of the parameter \\(F_{ST}\\) was as: \\[\\begin{aligned} F_{ST} &amp; = \\frac{\\sum_{i=1}^\\ell \\sigma_{q_{S(i)}}^2}{\\sum_{i=1}^\\ell \\left[ q_{T(i)}\\left( 1 - q_{T(i)}\\right) \\right]} \\\\ &amp; = 1 - \\frac{y_{ST}}{y_T} \\end{aligned}\\] Where \\(\\sum_{i=1}^\\ell \\sigma_{q_{S(i)}}^2\\) is the variance in allele frequencies across subpopulations (the \\(S(i)\\) subscript) using all \\(\\ell\\) alleles at the locus, and \\(q_{T(i)}\\) is the average allele frequency across all subpopulations. The parameters \\(y_{ST}\\) and \\(y_T\\) are equivalent to \\(H_S\\) (average expected heterozygosity across subpopulations) and \\(H_T\\) (expected heterozygosity of average allele frequencies) respectively. Lets take a simple example and work though this for clarity as the estimators \\(H_S\\) and \\(H_T\\) can be a bit confusing at times. Consider the case where we have three populations assayed for a single bi-allelic locus. Allele frequencies at each of the populations are: p &lt;- c(0.2, 0.3, 0.4) q &lt;- 1-p which give an expected heterozygosity of: hs &lt;- 2*p*q hs ## [1] 0.32 0.42 0.48 Whose average is: mean(hs) ## [1] 0.4066667 For \\(H_T\\), we estimate the average of the allele frequencies (\\(\\bar{p}\\) and \\(\\bar{q}\\)) and then the expectation for heterozygosity (\\(2\\bar{p}\\bar{q}\\)) as: pbar &lt;- mean(p) qbar &lt;- mean(q) ht &lt;- 2*pbar*qbar ht ## [1] 0.42 From these values, we can estimate \\(F_{ST}\\) as: \\[\\begin{aligned} F_{ST} &amp; = 1 - \\frac{H_S}{H_T} \\\\ &amp;= 1 - \\frac{0.4067}{0.42} \\\\ &amp; \\approx 0.317 \\end{aligned}\\] Values of \\(F_{ST}\\) that are close to zero indicate low levels of differentiation, whereas those close to unity represent complete differentiation (with very important caveats that follow). 2.10.1 Issues with Fixation Indices One could interpret the original configuration of \\(F_{ST}\\) as being a ratio of variances. The numerator is the variance in allele frequencies across sampled locations and the denominator is essentially the variance of binomial (or multinomial if \\(\\ell &gt; 2\\)). The largest issues with this parameter is that it is NOT a measure of genetic differentiation in the sense that it tells us how different populations are (e.g., how we would use this term in the common vernacular). In fact, Sewell Wright (1984) specifically states that \\(F_{ST}\\) can be interpreted as a measure of the amount of differentiation among subpopulations, relative to the limiting amount under complete fixation… This can be seen in the following examples and the values we get for the parameter \\(F_{ST}\\) in each. Scenario 1: Two populations, A and B, each fixed for a different allele. In this case, the heterozygosity at population A would be \\(2p_Aq_A = 2*0*1 = 0\\), likewise for population B at \\(2p_Bq_B = 2*1*0 = 0\\), and the estimate of subpopulation heterozygosity is \\(H_S = \\frac{0+0}{2} = 0\\). Total heterozygosity, \\(H_T\\), is defined using allele frequencies averaged across populations and would be \\(H_T = 2\\bar{p}\\bar{q} = 2*0.5*0.5 = 0.5\\) making \\(F_{ST} = 1 - \\frac{0}{0.5} = 1.0\\). These two populations are diverged completely from each other and this makes sense. Subpopulation heterozygosity, \\(H_S\\), is always zero when any number of populations are fixed for a single allele. Scenario 2: Two populations with two alleles each, though not the same alleles. In the most simple case lest assume population A is in HWE with alleles A and B occurring at equal frequencies and the other population has alleles C and D also at equal frequencies. In this case, heterozygosity at each population would be \\(H_{S,A} = H_{S,B} = 0.5\\) and \\(H_S = 0.5\\) as it is the average of the expected population-level heterozygosity. The total expected heterozygosity is the heterozygosity of allele frequencies averaged across populations, which in this simple example we have \\(\\bar{p}_A = \\bar{p}_B = \\bar{p}_C = \\bar{p}_D\\) and \\(H_T = 1 - \\sum_{i=1}^\\ell p_i^2 = 0.75\\)3 This makes \\(F_{ST} = \\frac{H_T - H_S}{H_T} = \\frac{0.75 - 0.25}{0.75} = 0.33\\). Intuitively, this does not make much sense, why would it be a third if both populations are in HWE, just for different alleles? Scenario 3: Three populations, the first of which is fixed for allele A and the rest that are fixed for allele B. In this scenario, \\(H_S = \\frac{1}{3}\\left[ 2p_1q_1 + 2p_2q_2 + 2p_3q_3 \\right] = 0\\) but \\(H_T\\), being defined as the expected heterozygosity of averaged allele frequencies, \\(\\bar{p} = \\frac{1+0+0}{3} = 0.33\\) and \\(\\bar{q} = \\frac{0+1+1}{3} = 0.66\\) would be \\(H_T = 2\\bar{p}\\bar{q} = 2*0.33*0.66 = 0.4356\\) and \\(F_{ST} = \\frac{0.4356 - 0}{0.4356} = 1\\). Again, when populations are fixed for different alleles, \\(F_{ST} = 1.0\\). However, in this case, two of our populations are entirely identical! How is it that the two populations with identical allele frequencies (and whose own \\(F_{ST} = 0\\) by the way) can cause \\(F_{ST}\\) to go to unity? In fact, we could have 100 populations fixed for one allele and 1 population fixed for the other and still have \\(F_{ST} = 1.0\\)! This is because, as Wright (1984, pg 82) pointed out: The fixation index is thus not a measure of degree of differentiation in the sense implied by the extreme case by absence of any common allele. It measures differentiation within the total array in the sense of the extent to which the process of fixation has gone towards completion. This is not how it is used in the literature, where it is often used to describe the differences among populations in an absolute sense. This is why these are called fixation indices (the \\(F\\) is for fixation). For completeness, we can also estimate the inbreeding of an individual relative to the total population, \\[ F_{IT} = \\frac{H_T - H_I}{H_T} \\] though it is not often used because individuals are inbred relative to the populations within which they are found, not within which the entire dataset is composed. Moreover, the three parameters have the following relationship, \\[ (1-F_{IS})(1-F_{ST}) = (1-F_{IT}) \\] which means that once \\(F_{IS}\\) and \\(F_{ST}\\) are estimated, \\(F_{IT}\\) is completely defined. Before we jump into some data, we need to address one more issue that we are confronted with. Scenario 4: Estimation of the population-level heterozygosity is not done without error. In fact, we are estimating these parameters for each of the populations and every time we need to determine estimates of allele frequencies. As such, if we are going to take this into consideration, we need to correct estimates of heterozygosity accordingly. Here is an example of the differences we will see if we do not account for these problems (as well as the issue of samples sizes and sampling locations discussed in 2.6.3). In the arapat data set, I’m going to use the two of the mainland populations and estimate \\(F_{ST}\\) from them for a single locus as an example to demonstrate this last issue and show the magnitude of the bias that may be introduced by not considering that each of the stratum have estimated heterozygosity with a bit of error. library(gstudio) data(arapat) df &lt;- arapat[ arapat$Population %in% c(&quot;101&quot;,&quot;102&quot;), c(3,13)] df &lt;- droplevels( df ) Here is what that looks like (the droplevels() bit is to remove the non-observed populations from the stratum column in the derived data.frame). df ## Population ATPS ## 328 101 02:02 ## 329 101 02:02 ## 330 101 09:09 ## 331 101 02:02 ## 332 101 04:04 ## 333 101 02:02 ## 334 101 02:02 ## 335 101 02:09 ## 336 101 02:09 ## 356 102 02:09 ## 357 102 02:02 ## 358 102 02:02 ## 359 102 02:02 ## 360 102 02:02 ## 361 102 02:02 ## 362 102 02:02 ## 363 102 02:02 If you look at the alleles present by locus, you can see quite that the allele frequencies (above) are not that close to each other—in fact, population 101 has the 04 allele that is not present in population 102. Numerically, they are: frequencies(df, stratum = &quot;Population&quot;) ## Stratum Locus Allele Frequency ## 1 101 ATPS 02 0.6666667 ## 2 101 ATPS 04 0.1111111 ## 3 101 ATPS 09 0.2222222 ## 4 102 ATPS 02 0.9375000 ## 5 102 ATPS 09 0.0625000 If we estimate heterozygosity directly for each population we see that the individual, population-level, heterozygosity, is estimated as: x &lt;- c(genetic_diversity(df[df$Population==&quot;101&quot;,],mode=&quot;He&quot;)$He, genetic_diversity(df[df$Population==&quot;102&quot;,],mode=&quot;He&quot;)$He) x ## [1] 0.4938272 0.1171875 which results in an estimate of \\(H_S\\): hs &lt;- mean(x) hs ## [1] 0.3055073 Similarly, the parameter \\(H_T\\) is: ht &lt;- genetic_diversity(df,mode=&quot;He&quot;)$He ht ## [1] 0.3442907 We can estimate \\(F_{ST}\\) as: Fst_biased &lt;- 1 - hs/ht Fst_biased ## [1] 0.1126471 If we do take into consideration the error associated with estimating these parameters, we find a much smaller value: Fst(df) ## Locus Hs Ht Fst ## 1 ATPS 0.3347824 0.3479467 0.03783444 Notice here that \\(H_S\\) is estimated from averaging (the first way) 10% lower than done when considering sampling allocations. The bias associated with \\(H_T\\) is much smaller but exists all the same. Overall, the problem here is an overestimation of \\(F_{ST}\\) by a factor of almost 3! These results are exaggerated a bit because of the small size—it would be foolish to estimate \\(F_{ST}\\) from only N=17 individuals from two populations. However, it does show the importance of considering sampling allocations. As you sample more individuals, these estimates of heterozygosity will converge. 2.10.2 Additional \\(X_{ST}\\)-like Parameters The use (and perhaps misuse) of [~] has supported the development of almost a cottage industry in other parameters, each trying to fit into a specific perceived problem in the original parameter. Here are some other extensions of this basic parameter that you may come across: \\(R_{ST}\\) for Microsatellite loci. This uses the ladder genetic distance metric discussed previously. \\(N_{ST}\\) for Nucleotides variation. \\(G_{ST}\\) for population subdivision (and in some cases mutation). \\(G_{ST}^\\prime\\) for loci with high allelic diversity (along with its nemesis \\(D_{est}\\)). Each of these parameters attempts to solve an additional problem that people have posed for \\(F_{ST}\\). The magnitude of bias associated with each may be differential, depending upon the extent to which your data are violating underlying assumptions. For example, at highly diverse loci, the expectation for [~] is limited in that it cannot reach its theoretical maximum. As such, Hedrick (ref) has derived the [~] parameter that corrects for this. In general, the degree to which [~] and [~] diverge depend upon both the diversity and the evenness of allele frequencies in your populations. This begs the question, “Is there a cutoff when I should not longer use [~] and prefer [~] instead?” I think Hedrick would suggest to always use it the secondary one, just in case, though it may be in your best interest to examine the differences in the parameters in your own data first. Here is an example of how these parameters may give alternative estimates. I’ll use the MP20 locus as it is a microsatellite locus of high diversity and compare the [~], [~], and [~] estimates. df &lt;- arapat[, c(3,14)] genetic_structure(df,stratum=&quot;Population&quot;,mode=&quot;Gst&quot;) ## Locus Gst Hs Ht P ## 1 MP20 0.308848 0.5663341 0.8194061 0.308848 genetic_structure(df,stratum=&quot;Population&quot;,mode=&quot;Gst_prime&quot;) ## Locus Gst Hs Ht P ## 1 MP20 0.7227937 0.5663341 0.8194061 0 genetic_structure(df,stratum=&quot;Population&quot;,mode=&quot;Dest&quot;) ## Locus Dest Hs Ht P ## 1 MP20 0.5686012 0.5663341 0.8194061 0.5686012 Is the diversity at this locus influencing structure estimation? Yes, you can clearly see that [~] alone produces a much smaller value than the other parameters. Which of the other ones are more accurate? That is an interpretive question you have to answer knowing your study system. 2.11 Statistical Structure Thus far, we’ve examined parameters that describe genetic structure, in relation to fixation. These paremeters are all based upon underlying population genetic assumptions. However, that need not be the case. In 1984, Wier &amp; Cockerham derived a parameter \\(\\theta\\), which was an estimator for \\(F_{ST}\\) but based upon an analysis of variance approach rather than expectations relying upon heterozygosity. This statistic is essentially a variance ratio, derived from a random-effects analysis of variance model, just like we would use in normal parametric statistics. Later, Excoffier et al. (1992) expanded upon this approach to provide a multilocus estimate of this parameter, which they called \\(\\Phi_{ST}\\) using an analysis they termed AMOVA (for Analysis of MOlecular VAriance). In 2004, Dyer et al. showed how both of these approaches are just special cases of a more general family of Q-mode linear regression models, amenable to broad ranges of sampling designs and configurations. Taken as a whole, the distance among all N individuals can be represented as a pairwise genetic distance matrix, \\(\\mathbf{D}\\). This matrix has the form: which is symmetric (${ij}^2 = {ji}^2) and has zero squared distance down the diagonal. This can be evaluated as an Analysis of Variance by decomposing the squared distance values into the Total Sums of Squared Distances \\[ SSD_{T} = \\frac{\\sum_{i=1}^N\\sum_{j=1}^N\\delta_{ij}^2}{2N-1} \\] the Sums of Squared Distances Within strata \\[ SSD_{W} = \\sum_{k=1}^K \\left[ \\frac{\\sum_{i=1}^{N_k}\\sum_{j=1}^{N_k}\\delta_{ij}^2}{2N_k-1} \\right] \\] and the Sums of Squared Distances Among populations. \\[ SSD_{A} = \\sum_{k=1}^K\\sum_{l\\ne k}^K\\frac{\\sum_{i=1}^{N_k}\\sum_{j=1}^{N_l}\\delta_{ij}^2}{N_k+N_l} \\] These sums of squares in this context enter directly into a traditional ANOVA table . From here, it is identical to a normal ANOVA analysis. We have degrees of freedom, mean squares, and variance components. The AMOVA itself, due to the way we sample the individuals and populations is a random-effects model. Our sampling typically consists of us going out and sampling a portion of the potential populations that exist rather than sampling all specified populations. As a statistical consequence, this means that the variance within (the error term) and among (the treatment variance) have to be corrected because we are taking only a sample from all potential populations. For a 1-level analysis, we can perform this in R by first setting up the data as a distance matrix and a vector of population assignments (it appears that the values need to be either numeric or as a factor for populations, though at the time of this writing, I could not get a factor representation to work properly). D &lt;- genetic_distance(arapat, stratum=&quot;Population&quot;, mode=&quot;AMOVA&quot;) D &lt;- as.dist( D ) pop &lt;- as.numeric( arapat$Population ) and then using the amova() function from the pegas library (there are many different implementations of this kind of analysis, decompose the variance into and fill out the AMOVA table as: library(pegas) fit &lt;- amova( D ~ pop, nperm=1000 ) fit ## ## Analysis of Molecular Variance ## ## Call: amova(formula = D ~ pop, nperm = 1000) ## ## SSD MSD df ## pop 2304.694 60.649831 38 ## Error 771.721 2.381855 324 ## Total 3076.415 8.498383 362 ## ## Variance components: ## sigma2 P.value ## pop 6.2701 0 ## Error 2.3819 ## ## Phi-statistics: ## pop.in.GLOBAL ## 0.7247025 ## ## Variance coefficients: ## a ## 9.293026 This gives us the standard anova table and an estimate of the probability associated with the variance component representing among-population variance, \\(\\sigma_A^2\\). The estimated variance components of this model, we can estimate a structure statistic, \\(\\Phi_{ST}\\), which for a 1-level analysis is \\[ \\Phi_{ST} = \\frac{\\sigma_A^2}{\\sigma_A^2+\\sigma_W^2} \\] and from the output of the amova() function can be estimated as (don’t know why this isn’t done automagically) PhiST &lt;- 1 - fit$varcoef/(fit$varcoef+fit$varcomp[2,1]) PhiST ## a ## 0.2040154 Which means that roughly % of the genetic variation observed in these data can be attributed to individuals being assigned to different populations. 2.11.1 Final Thoughts on Structure The estimation of genetic structure is a fundamental endeavor in population genetics that at times can be given a bit more weight than it may warrant. These parameters are simply estimates of a magnitude of structure in relation to either the degree to which populations have gone to fixation or as a statistical decomposition of raw variation. The important component here is that there are a lot of ways that a set of population histories may result in the same amount of genetic variation. The important point here is that simply looking at the magnitude of variation among strata does not allow us to differentiate among alternative demographic histories. 2.12 Questions Using the arapat data set, create a map of allele frequencies for the loci “EF” an “ZMP”. Are there patterns in allele frequencies across the landscape that suggest spatial structuring? Hardy-Weinberg Equilibrium assumes that the genotype frequencies at a locus are dictated only by the allele frequencies. As such, test the null hypothesis \\(H_O: ZMP is in HWE&quot;\\) using the \\(\\chi^2\\) approximation. Create a plot of allele frequencies for the LTRS locus allele “01” along Latitude. Does this help interpret spatial structure. Partition out the populations in the arapat data set into those with Species==&quot;Peninsula&quot; and Species==&quot;Cape&quot; and perform an analysis of molecular variance on them individually. Which group of populations has more genetic structure? No need to use negative numbers or any other kind of metric, it is just missing as if you didn’t get the observation.↩ The NA in the \\(H_{ES}\\) parameter is because there are no samples in one of the populations and a harmonic mean with a zero in it results in a divide-by-zero error.↩ When we have more than 2 alleles at a locus, it is often easier to estimate the part that is not homozygote rather than adding up for example \\(2pq + 2pr + 2ps + 2qr + 2qs + 2rs\\) to estimate heterozygotes.↩ "],
["resistance-models.html", "3 Resistance Models 3.1 Synopsis 3.2 Objectives 3.3 Resistance Costs 3.4 Estimating Distances on Resistance Rasters 3.5 Resistance Models 3.6 Questions", " 3 Resistance Models 3.1 Synopsis One of the characteristics of landscape genetics studies, maybe even a defining feature, is its use of resistance and distance approaches to understanding the spatial distribution of genetic structure and hopefully to use it in support of characterizing landscape mediated connectivity. In this activity, we will explore how to characterize and estimate separation and delve into a approaches for understanding how we can analyze similarly defined genetic distances. 3.2 Objectives In this section, we focus on the characterization and analysis of distance matrices. By the time you finish this activity, you should be able to: Create, develop, and manipulate matrices and rasters to be used in resistance modeling analyses. Understand and use alternative methods for estimating separation among locations using resistance rasters Examine different statistical models applicable to distance-based resistance modeling. 3.3 Resistance Costs The data set that we are going to be using consists of several a set of Cornus florida trees, acting as pollen recipients. These trees are situated on ~170ha site, co-distributed with Ilex species in the understory and over-topped with both Pinus and Quercus canopy dominants. Seeds were collected from several maternal individuals, germinated, and genotypes. These data were collected to examine pollen-mediated gene flow. Maternal locations are considered as spatial sampling of a ‘pollen pool’, whose spatial structure is determined by the physical and spatial separation of pollen donor and recipient trees. In addition to the genetic and spatial data, the following features were also measured using remote sensing (LiDAR &amp; Hyper-spectral Imaging): General Features Euclidean distance Elevation similarity. Forest Canopy Features Coniferous vegetation (cv) Mixed hardwoods (mh) Open field (fd) Forest Understory Features Roads (rd) Cornus canopies (cf) These layers are provided in the data/dogwood folder as *.asc raster files. Resistance is a cost associated with movement across a landscape. In the context of landscape genetics, we assume that this parameter is what is influencing connectivity. As a raster, resistance is represented by the values stored in a raster. A feature on the landscape may either promote or retard movement. The greater the value, the more effects that may have on movement across the pixel. The example below shows conifer canopy structure across the study area. It can be that movement is hindered under the conifer canopy (left) or it could be restricted moving outside the conifer canopy (right). Figure 3.1: Mirror resistance rasters based upon the presence of conifer canopy structure. The data we use for parameterizing resistance rasters may come from many different sources. The key here is that the proper set of features influencing connectivity and their individual interactions and resistance values are those that best describe the spatial distribution of genetic variation. The genotypes and where they are located are the answer, resulting from the demographic processes we are trying to analyze. Theory surrounding resistance derivations for connectivity are still being actively developed, so pay attention to the literature exciting developments are ahead. We can derive resistance rasters from many sources. They may be hand digitized, wherein features of the landscape are manually mapped onto a raster. The kinds of features most easily hand digitized are those that are large in extent and continuous such as roads, barriers, rivers, and other features of course granularity. Climatic data, commonly parameterized in terms of precipitation and temperature, can algorithmically derive raster cost surfaces. The accuracy of these rasters depend largely upon the region of the earth being modeled, the density of weather monitoring stations, and volatility of weather in that location. The most popular provider of climactic data is WorldClim, from which the rasters you worked with on Monday were provided. Increasingly, remotely sensed data sources are being used to incorporate information into resistance rasters. Platforms collecting remotely sensed data may be satellites, fixed wing aircraft, balloons, unmanned aircraft (drones), and even hand carried devices. The rasters provided were collected by fixed wing aircraft. 3.3.1 Working with Remotely Sensed Data LiDAR data generally comes in *.las or *.laz files and will have gone through some pre-processing such that each return will have an x,y, and z coordinate. Depending on the LiDAR sensor there may also be data defining the return number, intensity of the return, R,G,B data, or other classifications of ground and non-ground returns. LAS files can easily have millions, or billions, of points and thus are generally very large. We will be working with data set that has been subset to a more manageable size. The original LAS files were clipped to an area of interest using LAStools which is available at https://rapidlasso.com/lastools/ There is a GUI for Windows operating systems and can be run via command line on Mac. Depending on the data source, certain classifications may have already been assigned to returns such as ground (=2) and non-ground (=1). If data is not classified as ground/not ground you will need to run lasground which will extract ground points from you data set. Subsequently las height will determine the height above ground of each return. Next LASclassify will allow you to extract horizontal slices from the point cloud. There are two example *.las files, lowvegetation.las and mediumvegetation.las in the folder data/lidar/ that you can look at later. Given the size of these files and the time it takes to translate them into rasters, we have provided pre-existing rasters with 0.5 m spatial resolution in the same folder. There are three, High05.asc, Med05.asc and Low05.asc and they can be loaded in just like any other raster. Here is the medium vegetation layer, depicting LiDAR returns in a band that is between 3.5 - 11m above the ground, presumably the space through which the pollinators will be moving. The values in the cells of this raster are the elevation of the LiDAR return and are only salient to us if we make them all the same, indicating the presence/absence of a return rather than the elevation that it was recorded as. ## class : RasterLayer ## dimensions : 2883, 1664, 4797312 (nrow, ncol, ncell) ## resolution : 0.5, 0.5 (x, y) ## extent : 303969.6, 304801.6, 4133236, 4134677 (xmin, xmax, ymin, ymax) ## coord. ref. : NA ## data source : in memory ## names : Med05 ## values : 1, 1 (min, max) This is quite a detailed raster, much more detailed in scale than the pines one above. Given the small pixel size, you may need to zoom the raster image a bit to see the details. plot( medVeg ) The difference in the pine and medVeg raster resolutions brings up a salient point, and one that is perhaps not as well appreciated in the literature as it should be. The LiDAR returns are very fine grained, whereas the hyperspec data that created the pine raster was more course grained. However, which grain is most important for to the organism that is participating in the movement of genetic materials? It may be that for some features, organisms perceive the environment at one level of granularity and for other features they react to a different scale. We would have to tune our analyses such that we can determine which pixel size is most important. Functionally, this is accomplished using the aggregate function, supplied with a factor (e.g., the neighborhood size to aggregate) and a function (in this case we used the max function). medVeg1 &lt;- aggregate(medVeg, fact=2, FUN=max) medVeg2 &lt;- aggregate(medVeg, fact=4, FUN=max) medVeg5 &lt;- aggregate(medVeg, fact=10, FUN=max) medVeg10 &lt;- aggregate(medVeg, fact=20, FUN=max) The differences between can easily be visualized. par(mfrow=c(2,2), mai=c(0.5,0.5,0.2,0.2)) plot(medVeg1, legend=FALSE, axes=FALSE) plot(medVeg2, legend=FALSE, axes=FALSE) plot(medVeg5, legend=FALSE, axes=FALSE) plot(medVeg10, legend=FALSE, axes=FALSE) The salient point here is that the aggregate factor itself may be a parameter that we need to estimate during our analyses. The easiest way to do this is to build your rasters across a range of granularity and let the analyses sort out which is the most salient scale to use. We will see this approach again when we discuss the magnitude of raster costs. 3.4 Estimating Distances on Resistance Rasters Once we have defined one or more rasters quantifying resistance costs, we can go and estimate the distance among points on those rasters. There are several ways available for us to estimate distance. The differences in these methods are based upon the algorithm used to estimate separation and the extent of the raster used to apply these algorithms. These methods are buttressed on each end by: Least Cost Path: An optimum solution, attempting to find the shortest cost path between two points. This algorithm may produce asymmetric paths (e.g., the total cost to a location may not be the same as the distance from a location) depending on how the underlying raster is configured. All Paths (CircuitScape): This algorithm attempts to capture the way in which the entire landscape influences connectivity by integrating all the potential paths between locations. These algorithms may be given the entire raster to work with or may be constrained. Some examples of constraints are items on the landscape that are impassible (e.g., a rock), either by having missing data in the raster itself or a value of resistance that makes it impossible to be considered part of a potential path. Other approaches may include the addition of corridors, within which paths may be constrained to move. These configurations are implemented by crafting additional spatial objects in the same raster space as your cost rasters. As a last word of concern, you should also consider reducing the extent of your raster as much as feasible. The actual algorithm that estimates path distances (for least cost at least) is one that increases in complexity, time, and memory with the cube of the number of pixels! Similar scaling issues are involved with solving the linear equations necessary for the all paths problem. \\(N^3\\) increases rapidly, so everything you can do to constrain its magnitude helps. 3.4.1 Least Cost Distance A least cost path is the route between two points, say \\(A \\to B\\), whose length is minimal. This is the optimal route across the landscape. Organisms or vectors of gene exchange may, or may not, move across a landscape following an optimal path. Figure 3.2: Estimation algorithm for least-cost path distance. The path chosen is the one with the shortest overall distance, or at least one of the paths with equally short overall distances. One way to estimate distances in R, is through the use of the gdistance library. In this approach, we define a transition object based upon: 1. The cost distance raster. By default, the transition() function works on conductance, which is the inverse of resistance. In these examples, we have used a single raster, though use of RasterBrick objects is just as appropriate. 2. The function by which we estimate the pixel-to-pixel distances. 3. The neighborhood around each pixel that we look at during each step. Options for this include: - A von Neumann neighborhood (directions=4) consists of the pixels immediately above, below, and on both sides of the target pixel. - A Moore’s neighborhood (directions=8) consisting of all pixels surrounding the target pixel (e.g., even the diagonal ones) - A Knight &amp; One-Cell Queen move (directions=16), which adds the next layer of cells outside of Moore’s neighborhood. Once estimated, the transition object must be corrected for if you are either using a large extent based upon Latitude and Longitude datum (e.g., they are not unit-wise consistent in area), or you have used a direction option other than the von Neuman Neighborhood. In the example below, I use the cost distance estimated as an absolute distance with a resistance cost of 2:1. require(gdistance) pine &lt;- pines + 1 tr &lt;- transition( 1/pine, transitionFunction = mean, directions=4 ) tr &lt;- geoCorrection( tr, type=&quot;c&quot;, multpl=FALSE, scl=FALSE) tr ## class : TransitionLayer ## dimensions : 941, 527, 495907 (nrow, ncol, ncell) ## resolution : 1.000212, 1.000212 (x, y) ## extent : 304063.4, 304590.5, 4133379, 4134320 (xmin, xmax, ymin, ymax) ## coord. ref. : NA ## values : conductance ## matrix class: dsCMatrix In using the transition function, you must either invert the distance matrix tr &lt;- transition( 1/pine, transitionFunction = mean, directions=4 ) or you can invert the transitionFunction itself. tr &lt;- transition(pine, transitionFunction = 1/mean, directions=4 ) After correcting, the tr object is a raster of class TransistionLayer whose values denote the conductance across the landscape tr ## class : TransitionLayer ## dimensions : 941, 527, 495907 (nrow, ncol, ncell) ## resolution : 1.000212, 1.000212 (x, y) ## extent : 304063.4, 304590.5, 4133379, 4134320 (xmin, xmax, ymin, ymax) ## coord. ref. : NA ## values : conductance ## matrix class: dsCMatrix The size of the data within the transition matrix is a bit telling about why it takes a bit of time to estimate these objects. The cost raster itself has 941 rows and 527 columns. The transition matrix, tr, has the values of conductance stored for all (well, almost all) pairs of points on that surface in a matrix with 495907 rows and columns. That is rdim(tr@transitionMatrix)[1]^2 estimates! Again, this is why it is important to make sure you are working with raster extents that are sufficiently large to capture the area relevant to connectivity but not so large as to have areas that are irrelevant. Those irrelevant components of your landscape will still be used to estimate transition matrices! The shortest paths can be visualized as coords &lt;- cbind( x=c(304374,304546,304079), y=c(4134281,4133413,4133584)) pts &lt;- SpatialPoints( coords ) path.1 &lt;- shortestPath( tr, pts[1], pts[2], output=&quot;SpatialLines&quot;) path.2 &lt;- shortestPath( tr, pts[2], pts[3], output=&quot;SpatialLines&quot;) path.3 &lt;- shortestPath( tr, pts[3], pts[1], output=&quot;SpatialLines&quot;) plot( pine ) lines( path.1, col=&quot;red&quot;) lines( path.2, col=&quot;blue&quot;) lines( path.3, col=&quot;darkgreen&quot;) points( pts, pch=16, col=&quot;black&quot;) The distances along those paths is estimated, in a pair-wise fashion, as d.lcp &lt;- costDistance(tr, pts) d.lcp ## 1 2 ## 2 1045.1439 ## 3 1012.7618 659.2799 which is exactly what we need for incorporating ecological separation into models of connectivity. I should raise an important point here about the form of that matrix. The order of rows and columns follow the order of the coordinates that are passed to the shortestPath function. This will be the same order of stratum that you need to use for estimating both the other ecological/spatial distances as well the genetic distances. I find it helpful to add rowname and colname attributes to these matrices to make sure the orders are correct in subsequent analyses. 3.4.2 All Paths (CircuitScape) Distances The All Paths distance is estimated using the same approaches as for the Least Cost Paths, a transition matrix should be created and corrected and SpatialPoints will be used to estimate separation. There are two main differences. First, the geoCorrection function, uses a different type of correction, set it to type='r' from type='c'. Next, the pairwise distance between these locations are estimated by communteDistance instead of costDisance. Here is an example using the same points. tr.ct &lt;- transition( 1/pine, transitionFunction = mean, directions=4 ) tr.ct &lt;- geoCorrection( tr.ct, type=&quot;r&quot;, multpl=FALSE, scl=FALSE) d.ct &lt;- commuteDistance(tr.ct, pts) d.ct ## 1 2 ## 2 8085689 ## 3 7574426 6738172 You can also estimate all paths distance using the program CircuitScape by Brad McRae. Circuitscape, a stand alone Java application and you should have downloaded it and have it on your computer. What we need to run it is the cost raster and the points you are using, also as a raster, saved in ascii format. To demonstrate the use of CircuitScape, we will use a slightly larger data set for the point locations. This raster must be identical in both extent and granularity for the analysis to work. The function rasterize takes some points and a raster and returns a correctly scaled raster with the points indicated as the only non-NA pixels on the raster. trees &lt;- read.table(&quot;./data/dogwood/tree_coords.txt&quot;) trees &lt;- SpatialPoints( trees[,2:3] ) trees_raster &lt;- rasterize( x=trees, y=pines ) trees_raster ## class : RasterLayer ## dimensions : 941, 527, 495907 (nrow, ncol, ncell) ## resolution : 1.000212, 1.000212 (x, y) ## extent : 304063.4, 304590.5, 4133379, 4134320 (xmin, xmax, ymin, ymax) ## coord. ref. : NA ## data source : in memory ## names : layer ## values : 1, 17 (min, max) We can write both of these rasters as ascii files. writeRaster(trees_raster,&quot;trees_rast.asc&quot;,overwrite=TRUE) writeRaster(pine,&quot;pine_rast.asc&quot;,overwrite=TRUE) Then start Circuitscape and set the following parameters: 1. Choose Step 1: Raster 2. Choose Step 2: All pairs 3. Use the file section buttons to load in both the cost and site rasters. 4. Select an output location (it will create a lot of files), I called it ‘trees’ (the results are saved with that as a file prefix). 5. Selection the option to save a current map. 6. Hit Run. It will take a bit of time, it has to estimate distances across all pairs of points. When it finishes, it will produce a lot of files, two of which we are interested in working with. The first is the circuit distance data file. There are two version of this file, one with distances in column format (trees_resistances_3columns) and the other as a matrix (trees_resistance). If you named the output location something other than ‘tree’, it will be prefixed with that name instead—it will be obvious due to the huge number of files that appeared in your folder which ones to look at. The next file, which is much more interesting to look at, is a raster map of the current, as it was estimated across the landscape. Is should be saved as trees_cum_curmap.asc file. You can load that one as a normal raster. pine.current &lt;- raster(&quot;./spatial_data/trees_cum_curmap.asc&quot;) The surface of the current raster is definitely not smooth. Here is a blown up region of the raster around some focal trees. The cumulative effect of all paths across the landscape make a rugose topology. e &lt;- extent( c( 304421, 304583, 4133540, 4133869) ) pine.portion &lt;- crop( pine.current, e ) df &lt;- data.frame( rasterToPoints(pine.portion) ) names(df) &lt;- c(&quot;X&quot;, &quot;Y&quot;, &quot;Current&quot;) df$Current &lt;- log( df$Current) df$Current[ df$Current &gt; 0 ] &lt;- 0 ggplot(df, aes(X,Y,fill=Current)) + geom_tile() + scale_fill_gradientn( colors=c(&#39;#a6611a&#39;,&#39;#dfc27d&#39;,&#39;#f5f5f5&#39;,&#39;#80cdc1&#39;,&#39;#018571&#39;)) + coord_equal() 3.4.3 Logistical Issues Estimation of these values is not a rapid endeavor. For the relatively small raster we used in these examples and using only three points, the time to complete the estimation (in seconds) for both algorithms on my laptop was: Algorithm K Duration Least Cost Path 3 11.410 All Path (CircuitScape) 3 16.193 The CircuitScape algorithm takes a slightly different approach in estimating the distances than the gdistance and also takes a bit of time to run. 3.4.4 Pixel Size and Path Lengths As mentioned in the context of remote sensing above, pixel size also influences both the path across the landscape as well as its magnitude. Here we can estimate paths between the three example points measured on two different pixel sizes, 1x and 10x. tr1 &lt;- transition( 1/pine.p1, transitionFunction = mean, directions=4 ) tr1 &lt;- geoCorrection( tr1, type=&quot;c&quot;, multpl=FALSE, scl=FALSE) path.p11 &lt;- shortestPath( tr1, pts[1], pts[2], output=&quot;SpatialLines&quot;) path.p12 &lt;- shortestPath( tr1, pts[2], pts[3], output=&quot;SpatialLines&quot;) path.p13 &lt;- shortestPath( tr1, pts[3], pts[1], output=&quot;SpatialLines&quot;) tr2 &lt;- transition( 1/pine.p4, transitionFunction = mean, directions=4 ) tr2 &lt;- geoCorrection( tr2, type=&quot;c&quot;, multpl=FALSE, scl=FALSE) path.p21 &lt;- shortestPath( tr2, pts[1], pts[2], output=&quot;SpatialLines&quot;) path.p22 &lt;- shortestPath( tr2, pts[2], pts[3], output=&quot;SpatialLines&quot;) path.p23 &lt;- shortestPath( tr2, pts[3], pts[1], output=&quot;SpatialLines&quot;) par( mfrow=c(1,2), mar=rep(1,4)) plot( pine , legend=FALSE, axes=FALSE) lines( path.p11, col=&quot;red&quot;) lines( path.p12, col=&quot;blue&quot;) lines( path.p13, col=&quot;darkgreen&quot;) points( pts, pch=16, col=&quot;black&quot;) plot( pine.p4 , legend=FALSE, axes=FALSE) lines( path.p21, col=&quot;red&quot;) lines( path.p22, col=&quot;blue&quot;) lines( path.p23, col=&quot;darkgreen&quot;) points( pts, pch=16, col=&quot;black&quot;) Increasing the coarseness also changes the overall separation of points on the landscape, inserting positive bias. d1 &lt;- as.matrix( costDistance(tr1, trees) ) d2 &lt;- as.matrix( costDistance(tr2, trees) ) df &lt;- data.frame( Diff = d2[lower.tri(d2)] - d1[lower.tri(d1)] ) ggplot( df, aes( Diff ) ) + geom_histogram(bins=30) + xlab(&quot;Difference in Path Lengths&quot;) Unfortunately, there is no way, a priori what the proper pixel size should be, only the data can give suggestions. 3.4.5 Scaling Resistance In our examples thus far, we have assumed that the presence feature of interest was twice as costly to traverse than the landscape outside the feature. The pine raster has a \\(1.0\\) where there is no conifer canopy and a value of \\(2.0\\) under the pine trees. However, there is no a priori reason why this is the correct scaling of cost as it pertains to the movement of pollen—it could just as easily be 1:10 or 1:100. It could also be, as in the first figure above that the cost is accumulated for not moving under the conifer canopy, in which it would be a ratio of 2:1, 10:1, or 100:1. If we do not have a biologically motivated reason for assigning specific values to cost resistance, it is probably best to examine a range of potential values and let the genetic data provide suggestions regarding which would be the most explanatory. Scaling matrices can be done as either statically scaling the values or by allowing the values in neighboring cells to influence each other, similar to say neighborhood effects. pine.2 &lt;- pine pine.5 &lt;- ( pine - 1 ) * 4 + 1 pine.10 &lt;- ( pine - 1 ) * 9 + 1 pine.100 &lt;- ( pine - 1 ) * 99 + 1 par(mfrow=c(2,2), mai=c(0,0,0,0)) plot(pine.2, legend=FALSE, axes=FALSE) plot(pine.5, legend=FALSE, axes=FALSE) plot(pine.10, legend=FALSE, axes=FALSE) plot(pine.100, legend=FALSE, axes=FALSE) Smoothing is a way to modifying cells of a raster based upon the immediate neighbors. In this case, we can take the average of the eight surrounding cells and apply that value to the center cell. The focal function take the target raster, a 3x3 matrix of values indicating the amount each of the neighboring cells contribute to the characteristics of the target cell, and a function to use to determine the value assigned to the central cell. In this case, we use mean, though you could use max or other user defined functions. The net effect of this approach is to take the ‘rough edges’ out of the raster and smooth the transitions. It also expands the spatial extent of the feature into surrounding cell. w &lt;- matrix( 1, nrow=3, ncol=3) pine.smooth &lt;- focal( pine.2 , w , fun=mean) par(mfrow=c(1,2), mai=c(0,0,0,0)) plot( pine.2, legend=FALSE, axes=FALSE) plot( pine.smooth, legend=FALSE, axes=FALSE ) 3.4.6 Final Thoughts on Raster Manipulations The granularity, scale, cost, algorithm, and scaling of rasters that best describe the observed genetic structure are most likely the ones that we can designate as the most ‘biologically meaningful.’ How we determine which of the many potential combinations is actually the most relevant depends upon fitting the distance matrices to the genetic data. 3.5 Resistance Models Once we have a set of candidate loci, we can then start the analyses. There are several ways to evaluate these matrices, most of which are based upon looking for systematic changes in genetic distance that correspond with changes in one or more of the predictor raster distances. 3.5.1 The Mantel Test One of the most common approaches to evaluating the extent to which resistance distances are correlated with genetic distances is via the Mantel Test. It estimates a parameter, \\(Z\\), as the element-wise product of both matrices. \\[ Z = \\sum_{i=1}^N\\sum_{j=1}^N x_{ij}*y_{ij} \\] and then invokes the null hypothesis, \\(H_O:\\)The values in the matrices are independent. If this is true then large values in \\(\\mathbf{X}\\) should be randomly associated with all the values in \\(\\mathbf{Y}\\). However, if there is an association than large values in one matrix may be associated with large ones in the other (for a positive correlation) or small in one may be associated with large in another (for a negative correlation). If \\(H_O\\) is true, then any permutation of \\(\\mathbf{X}\\) and a re-estimation of \\(Z\\) should produce values as large (or small) as the observed \\(Z\\) value. Sounds simple, no? It also estimates a correlation coefficient, the default is a Pearson Product Moment though you can specify Spearman’s \\(\\rho\\) or Kendall’s \\(\\tau\\) as well. There are a few different implementations of the Mantel, the one I’m using below is the one from the vegan library. Here is the documentation for the function that provides a bit more information. mantel {vegan} R Documentation Mantel and Partial Mantel Tests for Dissimilarity Matrices Description Function mantel finds the Mantel statistic as a matrix correlation between two dissimilarity matrices, and function mantel.partial finds the partial Mantel statistic as the partial matrix correlation between three dissimilarity matrices. The significance of the statistic is evaluated by permuting rows and columns of the first dissimilarity matrix. Usage mantel(xdis, ydis, method=\"pearson\", permutations=999, strata = NULL, na.rm = FALSE, parallel = getOption(\"mc.cores\")) mantel.partial(xdis, ydis, zdis, method = \"pearson\", permutations = 999, strata = NULL, na.rm = FALSE, parallel = getOption(\"mc.cores\")) Arguments xdis, ydis, zdis Dissimilarity matrices or a dist objects. method Correlation method, as accepted by cor: \"pearson\", \"spearman\" or \"kendall\". permutations a list of control values for the permutations as returned by the function how, or the number of permutations required, or a permutation matrix where each row gives the permuted indices. strata An integer vector or factor specifying the strata for permutation. If supplied, observations are permuted only within the specified strata. na.rm Remove missing values in calculation of Mantel correlation. Use this option with care: Permutation tests can be biased, in particular if two matrices had missing values in matching positions. parallel Number of parallel processes or a predefined socket cluster. With parallel = 1 uses ordinary, non-parallel processing. The parallel processing is done with parallel package. Details Mantel statistic is simply a correlation between entries of two dissimilarity matrices (some use cross products, but these are linearly related). However, the significance cannot be directly assessed, because there are N(N-1)/2 entries for just N observations. Mantel developed asymptotic test, but here we use permutations of N rows and columns of dissimilarity matrix. See permutations for additional details on permutation tests in Vegan. Partial Mantel statistic uses partial correlation conditioned on the third matrix. Only the first matrix is permuted so that the correlation structure between second and first matrices is kept constant. Although mantel.partial silently accepts other methods than \"pearson\", partial correlations will probably be wrong with other methods. The function uses cor, which should accept alternatives pearson for product moment correlations and spearman or kendall for rank correlations. Value The function returns a list of class mantel with following components: Call Function call. method Correlation method used, as returned by cor.test. statistic The Mantel statistic. signif Empirical significance level from permutations. perm A vector of permuted values. The distribution of permuted values can be inspected with permustats function. permutations Number of permutations. control A list of control values for the permutations as returned by the function how. Note Legendre & Legendre (2012, Box 10.4) warn against using partial Mantel correlations. Author(s) Jari Oksanen References The test is due to Mantel, of course, but the current implementation is based on Legendre and Legendre. Legendre, P. and Legendre, L. (2012) Numerical Ecology. 3rd English Edition. Elsevier. See Also cor for correlation coefficients, protest (“Procrustes test”) for an alternative with ordination diagrams, anosim and mrpp for comparing dissimilarities against classification. For dissimilarity matrices, see vegdist or dist. See bioenv for selecting environmental variables. Examples ## Is vegetation related to environment? data(varespec) data(varechem) veg.dist Since it takes a bit of time to extract the distance matrices from the rasters, we have provided some pre-configured distance matrices for the trees identified above. These rasters are located in the data/dogwood folder. For these rasters we estimate cost distance across a range of values representing both avoidance (10:1, 7:1, 5:1, and 2:1) and preference (1:2, 1:5, 1:7 1:10) of the feature. Each distance matrix is in a single file, the name of which records the algorithm (lcp vs. ct), the feature (cf, cv, rd, fd, mh from above), and the costs of out.in. For example, the file ct.cf.1.5.rda is the Circuit Theory, Cornus florida feature with costs of 1 in the feature and 5 outside of it. In the following code, I loop through the files estimated using the least cost path approach (pattern='lcp*'). For each file, I break apart the file name and record the feature type, the in cost, the out cost, the algorithm used to estimate distances, and the correlation. lcp.files &lt;- list.files(&quot;./data/dogwood/&quot;, pattern=&quot;lcp*&quot;, full.names = TRUE) library(vegan) df &lt;- data.frame( File=lcp.files, Feature=NA, In=NA, Out=NA, Type=&quot;Least Cost&quot;, Correlation=NA, P=NA) load( &quot;./data/dogwood/G.rda&quot;) for( path in lcp.files) { file &lt;- strsplit( path, split=&quot;/&quot;)[[1]][5] components &lt;- strsplit(file, split=&quot;.&quot;,fixed=TRUE)[[1]] load( path ) fit &lt;- mantel( as.dist( G ), as.dist( D )) idx &lt;- which( df$File == path) df$Feature[idx] &lt;- components[2] df$In[idx] &lt;- as.numeric( components[3] ) df$Out[idx] &lt;- as.numeric( components[4] ) df$Correlation[idx] &lt;- fit$statistic df$P[idx] &lt;- fit$signif } library(DT) datatable(df[,2:7]) For any given feature, we have many different estimates of the correlation as the relative raster costs were varied across values of of 10:1, 7:1, 5:1, 2:1, 1:2, 5:1 7:1, 10:1. I plot the x-axis along a log10 scale to make the dispersion along the horizontal axis a bit more regular. df &lt;- df[ !(df$Feature %in% c(&quot;elevation&quot;,&quot;euclidian&quot;)),] df$Feature &lt;- factor( df$Feature ) df$Ratio &lt;- log10( df$In / df$Out ) df &lt;- droplevels(df) ggplot( df, aes(x=Ratio, y=Correlation, color=Feature)) + geom_line() + geom_point() + xlab(&quot;Log(In:Out)&quot;) Compared as a whole, we see that some features change very little across the landscape (cf, rd, and fd) while others clearly do a better job of explaining genetic covariance (mh and cv). These observations suggest that a feature like mh with a cost of 7x more in the feature as outside of it. We can see the best model as: load(&quot;data/dogwood/lcp.mh.7.1.rda&quot;) MH &lt;- D fit.mh &lt;- mantel( as.dist(G), as.dist(MH), permutations = 10000 ) fit.mh ## ## Mantel statistic based on Pearson&#39;s product-moment correlation ## ## Call: ## mantel(xdis = as.dist(G), ydis = as.dist(MH), permutations = 10000) ## ## Mantel statistic r: 0.5301 ## Significance: 9.999e-05 ## ## Upper quantiles of permutations (null model): ## 90% 95% 97.5% 99% ## 0.110 0.155 0.197 0.256 ## Permutation: free ## Number of permutations: 10000 3.5.2 The Partial Mantel In the previous example, we examined the correlation in only one feature at a time. The partial Mantel test extends this approach to fit one feature and then estimate the conditional correlation of the second one on the genetic distances providing an indication of two-way relationships. Using the same approach as before, we iterate through the remaining features and estimate the partial correlation coefficient after conditioning on the mh raster. target &lt;- lcp.files[ 34 ] remaining &lt;- lcp.files[ -(27:34) ] dfp &lt;- data.frame( File=remaining, Feature=NA, In=NA, Out=NA, Type=&quot;Least Cost&quot;, Correlation=NA, P=NA) for( path in remaining) { file &lt;- strsplit( path, split=&quot;/&quot;)[[1]][5] components &lt;- strsplit(file, split=&quot;.&quot;,fixed=TRUE)[[1]] load( path ) fit &lt;- mantel.partial(as.dist(G), as.dist(MH), as.dist(D)) idx &lt;- which( dfp$File == path) dfp$Feature[idx] &lt;- components[2] dfp$In[idx] &lt;- as.numeric( components[3] ) dfp$Out[idx] &lt;- as.numeric( components[4] ) dfp$Correlation[idx] &lt;- fit$statistic dfp$P[idx] &lt;- fit$signif } dfp &lt;- dfp[ !(dfp$Feature %in% c(&quot;elevation&quot;,&quot;euclidian&quot;)),] dfp$Feature &lt;- factor( dfp$Feature ) dfp$Ratio &lt;- log10( dfp$In / dfp$Out ) dfp &lt;- droplevels(dfp) ggplot( dfp, aes(x=Ratio, y=Correlation, color=Feature)) + geom_line() + geom_point() + xlab(&quot;Log(In:Out)&quot;) From these results, we would conclude that both mh and cv are the best features we examined. load(&quot;data/dogwood/lcp.cv.1.10.rda&quot;) CV &lt;- D fit &lt;- mantel.partial(as.dist(G), as.dist(MH), as.dist(CV), permutations = 10000) fit ## ## Partial Mantel statistic based on Pearson&#39;s product-moment correlation ## ## Call: ## mantel.partial(xdis = as.dist(G), ydis = as.dist(MH), zdis = as.dist(CV), permutations = 10000) ## ## Mantel statistic r: 0.2532 ## Significance: 0.0088991 ## ## Upper quantiles of permutations (null model): ## 90% 95% 97.5% 99% ## 0.127 0.164 0.203 0.245 ## Permutation: free ## Number of permutations: 10000 3.5.3 Distance Regression A direct extension of the Mantel test (in the same way a linear regression is an extension of a correlation) is are methods for multiple regression on distance matrices. The most commonly used implementation is that provided by the ecodist library. library(ecodist) The main difficulty in using multiple regression on distance (and Mantel as well) is to know the proper way to ascertain probabilities. Both rely upon permutations approaches, though knowing what level or what combinations of data to permute becomes problematic for all but the most simplistic model specifications. Here is the documentation for the MRM function, the one that actually fits the model and assesses the significance. MRM {ecodist} R Documentation Multiple Regression on distance Matrices Description Multiple regression on distance matrices (MRM) using permutation tests of significance for regression coefficients and R-squared. Usage MRM(formula = formula(data), data = sys.parent(), nperm = 1000, mrank = FALSE) Arguments formula formula in R/S-Plus format describing the test to be conducted. data an optional dataframe containing the variables in the model as columns of dissimilarities. By default the variables are taken from the current environment. nperm number of permutations to use. If set to 0, the permutation test will be omitted. mrank if this is set to FALSE (the default option), Pearson correlations will be used. If set to TRUE, the Spearman correlation (correlation ranked distances) will be used. Details Performs multiple regression on distance matrices following the methods outlined in Legendre et al. 1994. Value coef A matrix with regression coefficients and associated p-values from the permutation test (using the pseudo-t of Legendre et al. 1994). r.squared Regression R-squared and associated p-value from the permutation test. F.test F-statistic and p-value for overall F-test for lack of fit. Author(s) Sarah Goslee, Sarah.Goslee@ars.usda.gov References Lichstein, J. 2007. Multiple regression on distance matrices: A multivariate spatial analysis tool. Plant Ecology 188: 117-131. Legendre, P.; Lapointe, F. and Casgrain, P. 1994. Modeling brain evolution from behavior: A permutational regression approach. Evolution 48: 1487-1499. See Also mantel Examples data(graze) LOAR10.mrm Building upon the previous example, we can build up a MRM regression model, first fitting mh with a resistance of 7:1 to the genetic data. load(&quot;data/dogwood/lcp.mh.7.1.rda&quot;) X1 &lt;- D model.1 &lt;- MRM( dist(G) ~ dist(X1) ) model.1 ## $coef ## dist(G) pval ## Int 1.800108e+01 0.812 ## dist(X1) 5.317367e-05 0.001 ## ## $r.squared ## R2 pval ## 0.3801074 0.0010000 ## ## $F.test ## F F.pval ## 82.16647 0.00100 There are no built-in diagnostics for MRM model types, what is object that is returned from the function is simply a list object with those values in it. The MRM function uses a permutation scheme, implemented in C, to determine significance of the model, of the \\(R^2\\) term and of individual terms entered into the model. Model development is done by hand using MRM, building up the regression manually. There are no automated facilities for forward or reverse term selection as there are for models derived from the lm() family of analyses. Unfortunately, there are also no ways of checking leverage and fit either. These facilities need to be developed to bring these methodologies up to par with other regression fitting routines in R. After fitting the first term, we then examine the properties of the model after fitting the second term we identified using partial mantel. load(&quot;data/dogwood/lcp.cv.10.1.rda&quot;) X2 &lt;- D model.2 &lt;- MRM( dist(G) ~ dist(X1) + dist(X2) ) model.2 ## $coef ## dist(G) pval ## Int 1.789979e+01 0.888 ## dist(X1) 4.311454e-05 0.054 ## dist(X2) 1.360215e-05 0.628 ## ## $r.squared ## R2 pval ## 0.3813608 0.0010000 ## ## $F.test ## F F.pval ## 40.994 0.001 From the output, we see that the probability associated with \\(X2\\) does not suggest that this is adding significantly to the explanation of the genetic data. This suggests that in terms of developing a linear model with these data, could be defined spatially by making the mixed hardwoods raster 7:1 then fitting in the intercept and slope term from model.1, creating the expected connectivity cost raster. load(&quot;data/dogwood/mixedhardwoods.rda&quot;) mh &lt;- mh * 6 + 1 mh &lt;- model.1$coef[1,1] + mh * model.1$coef[2,1] df &lt;- data.frame( rasterToPoints( mh ) ) ggplot( df, aes(x=x,y=y,fill=layer)) + geom_tile() + coord_equal() + scale_fill_gradient(low=&quot;#f7fcfd&quot;,high=&quot;#00441b&quot;) 3.5.4 Model Comparisons &amp; Concerns Both the Mantel and the MRM approaches have been recently criticized in the literature due to some problems associated with assessing probabilities and ascertaining parameters like AIC, BIC, and DIC. At present, none of these methods can be applied to Mantel and partial Mantel approaches, so we are left with making decisions about alternatives based upon the magnitude of the correlation statistic and the probability associated with the null hypothesis. In the literature, you can find examples of people trying to use AIC with MRM-l models (I confess, I am one of them). The statistic AIC is defined as \\[ AIC = -2ln(L) + 2k \\] where \\(L\\) is the maximum likelihood of the model and \\(k\\) is the number of fit parameters. A model fit using MRM() does not have the ability to provide an estimate of \\(L\\). 3.6 Questions Here are some questions for you to exercise what was covered in the lab above. For the cost networks, I only calculated the costs associated with least cost paths. Go back and load in the circuit theory paths and perform the same set of Mantel tests. For each feature, which combination of resistance value and distance algorithm produce the highest correlations? To what extent does the pixel size influence hypothesized connectivity currents across the landscape? Select one of the aggregated pine rasters (pine.p1, pine.p2, or pine.p3) and run it through Circuitscape. Compare it to the output from the least cost distances. The LiDAR data is at a much finer resolution than the rasters used to develop the models above. Using the medium vegetation layer, a slice through the forest understory thought to represent the spaces through which the pollinators actually move, and explore how granularity and raster magnitude influence the ability to describe genetic covariance. "],
["space-networks.html", "4 Space &amp; Networks 4.1 Synopsis 4.2 Objectives 4.3 Admixture 4.4 Ordination 4.5 Networks 4.6 Mapping Networks 4.7 Genetic Distance Graphs 4.8 Population Graphs 4.9 Adding data to a graph 4.10 Extracting Graph-Theoretic Parameters 4.11 Testing for Topological Congruence 4.12 Questions", " 4 Space &amp; Networks 4.1 Synopsis In this activity we explore two different topics, admixture &amp; ordination approaches and network approaches, both of which are focuses on describing spatial population genetic structure. 4.2 Objectives This activity is divided between two different topics, explicitly spatial analyses of genetic structure and network approaches. The objective here are to: Understand admixture and how it can be used to identify natural partitioning in our data via methods like STRUCTURE and TESS. Explore various methods of ordination salient to the analysis of genetic structure and partitioning in our data. Learn about network manipulation and explore how genetic distance and population graph approaches may help describe spatial population structure. 4.3 Admixture Admixture is the mixing of two or more populations whose frequency spectra may differ. If they differ considerably, then the admixed population may be examined to determine relative contributions of each population. Figure 4.1: Hypothetical pedigree of an individual whose parents come from two separate populations. Stretches of chromosome are depicted in differing colors to highlight parentage as well as demonstrate admixture in the offspring. An ubiquitous feature of modern population genetic analyses is the presence of a model-based clustering analysis such as those provided by programs such as STRUCTURE and TESS. The general idea of these approaches is to examine the current distribution of genotypes (\\(X\\)) within individuals and populations and try to describe it in terms of historical mixing of populations (\\(Z\\)), at specified mixing rates (\\(Q\\)), whose allele frequency spectra (\\(P\\)) may differ. \\[ P(X|Z,P,Q) = \\prod_{i=1}^N\\prod_{j=1}^LP_{z_{ij},\\;p_j,\\;x_{ij}} \\] Estimating the particular values \\(Z\\), \\(P\\), and \\(Q\\) (realized for each of the \\(N\\) individuals across \\(L\\) loci), has to be done numerically. This process can provide some valuable insights to partitioning of your data. However, if you data are distributed along a gradient, these approaches are not quite as powerful. To use STRUCTURE, you need to reformat your data, either by hand or by exporting it through gstudio. The format for STRUCTURE requires that population designations to be numeric values, alleles have to be encoded in a particular way, files need to be formatted in a particular way, etc. Total pain. However, we can do this in R as: library(gstudio) data(arapat) write_population(arapat, file=&quot;arapat_structure.txt&quot;, mode=&quot;structure&quot;, stratum=&quot;Population&quot;) which will take your R data.frame with loci and other information in it and save it as a text file in the appropriate directory in the STRUCTURE format. You run STRUCTURE using either the command-line approach or via a GUI that comes with it. Most people will prefer the GUI. After starting it, do the following: Create a folder for the project. For some reason the Java GUI that STRUCTURE uses disallows the creation of new folders on OSX (don’t know if it does the same on windows). Put your data file (the arapat_structure.txt file from the previous code block) in that folder. Create a new project in STRUCTURE using \\(File \\to New\\;Project\\): Walk through the setup pages. The salient numerical information you will need is: - Number of Individuals: 363 - Number of loci: 8 - Missing data value: -9 You should also check the following boxes: - Individual ID for each individual - Putative population origin for each individual This should have you set up and ready to go. For an initial run through the data, you need to specify some parameters for the simulations. You do this by selecting \\(Parameter\\;Set \\to New\\). The default values on the parameters are a good place to start, though you have to put in the Length of the burn in and the number of reps; 100,000 &amp; 10,000 should get you started so that you see how the program runs. You can name this parameter set whatever you like. To make an actual run, you need to hit the run button (it has the exclamation mark on it). It will ask you how many groups are you going to run, this is the \\(K\\) parameter from the equation above, input 3 and let it go. It will chug along for a while, dumping out some output on the bottom of the interface. When finished it will give you an output of the run showing the parameter set. On the left, select the Results folder and then the \\(K=3\\) option to see the specifics of that particular run. Salient information on the output includes: The number of individuals per cluster before analyses (by population assignment), The allele frequency differences per cluster, The probability of the data give K=3, and The allele frequencies of each cluster Perhaps more interpretive are the plots of individuals as they were assigned to each group. You can visualize this by selecting the Bar Plot menu at the top of the Simulation Results pane. If you select it to Sort by \\(Q\\) it should look something like Figure below. This figure is based upon 363 columns of admixture information, one column for each individual. The colors are defined by the number of groups, here \\(K=3\\). You can see some individual columns (=individuals genotypes) who are entirely one color. These are individuals whose genotypes suggest they are the most likely from the group associated with that color. You can also see that there are some individuals who may be admixed between two or even three groups ad indicated by a column with more than one color. Figure 4.2: Output from STRUCTURE using the Araptus attenuatus when setting \\(K=3\\). This output is a hypothesis about admixture. As such, it is only one simulation and as we’ve done many times thus far, we should probably run several of these to generate a level of confidence for any value we specified as \\(K\\). Here is where it gets a little dicey. We specified \\(K=3\\) and as such we found out what the \\(P(X|Z,P,Q)\\) by specifying \\(K=3\\) implicitly. In the output of our data, we see can find the log likelihood of our data given these parameters. However, \\(K\\) may be some value other than three. Running this simulation with specified values only tells us the likelihood of the data for that value and for that simulation, it does not determine if the specified \\(K\\) is the correct one. Inferences on which \\(K\\) is actually correct can only be made by running the program for several iterations for each value of \\(K\\) (to understand the variability in the simulations for that particular value) and running several different values for \\(K\\) itself so we can compare the probability of the data we observed for different numbers of clusters. The data below depict some runs of the data for \\(K\\) assuming values ranging from 2 to 6. These are the raw values for the probability of the data for specific values of \\(K\\). If you are doing this on your data for real, you should do more than three runs, but this provides a base approach for understanding the output. Figure 4.3: Probability of the observed data for the numbers of clusters (\\(K\\)) ranging from 2 to 6 using STRUCTURE. A total of 10 replicates were run for each level of \\(K\\). The output shows that as we assume more clusters, the \\(log(P(Data))\\) tends towards an asymptote. There are a couple things to look at here for this output, and this is where the interpretive power of you the researcher needs to step in. The overall notion among many is that the way in which the probability of the data changes with an increasing number of clusters should be informative as to the ‘correct’ number of clusters found. This may or may not be true depending upon your data, your sampling, and the actual history of your organism. However, if it were true then the rationale suggests that when the probability of the data levels off, that may be a good place to look at other sources of inference to see if this may be supported as a plausible number of clusters. In the Figure, this looks like \\(K=4\\) may be a good place to start. If I go back to STRUCTURE and plot examine the barplot of individuals from the \\(K=4\\) data (setting “Group by PopID”). Figure 4.4: STRUCTURE output for Araptus attenuatus for \\(K=4\\) sorted by population. Here they are sorted into populations and colored by group (green, yellow, blue, red). From this display, we can make a few inferences: There is a pretty good indication that some of the ‘populations’ that I sampled appear to be filled with individuals of a single definite type. If I look at the groupings as it is presented, the 1st, 2nd, and 21st populations mainly consist of a single type (the green type). If I look at the original data set that we put out, these correspond to populations 101, 102, and 32. These are the three mainland populations that have consistently been found to be different. The rest are populations in Peninsular Baja. As we saw in the hierarchical clustering example, these populations may be further subdivided into different groups, Cape vs. the rest and perhaps more nested structure therein. In actuality, if you pull out the mtDNA types for Peninsula only and run STRUCTURE on it, you will find there is most likely a three separate groupings in the data (this is where the Cluster column in the arapat data set comes from). Determining the proper \\(K\\) is not a trivial thing. From the output of estimates of \\(P(X|Z,P,Q)\\) given a particular \\(K\\), we can examine how our data probability changes with different values of \\(K\\). We can also examine the allocation of individuals into groups and see if it makes sense, biologically. One way that is commonly used to get an idea of the magnitude of \\(K\\) is by looking at how \\(P(X|Z,P,Q)\\) changes with increasing values of \\(K\\). This is often referred to as the \\(\\delta K\\) approach. There is an argument to be made that the best assignment of \\(K\\) would be where there is a large change in \\(\\delta K\\) followed by a plateau. This is interpretive and not a definite heuristic. Figure 4.5: Estimates of \\(\\delta K\\) for the STRUCTURE output run on Araptus attenuatus. What we see is that the largest value of \\(\\delta K\\) followed by a steep reduction is when \\(K=4\\). This may indicate to us that as it stands, the totality of the data contains four groups and we can map them back onto our data set and evaluate the biological support for these groupings. It is important to consider the finding from these approaches as hypotheses, amenable to subsequent analyses rather than stark indications of reality. There are many situations where these clustering approaches may not be providing the most salient inferences into your data since the techniques were designed to identify only some kinds of structuring, not all. 4.4 Ordination There are several ways that we can look for natural groupings in our data. Barriers and sources of vicariance across the landscape create discontinuities in the genetic structure. As we begin to analyze the spatial structure of the data itself, one of the first things that needs to be done is to allow the data to tell the analyst how it is internally structured. This is a deviation from common approaches in statistical analyses. Typically, we start with a model and then see the extent to which the data fit into a model framework, potentially testing many different models. However, just because you get a significant model does not mean it is the best or even the correct one describing the underlying processes. In Dyer &amp; Nason (2004), we examined the spatial genetic structure of Lophocereus schottii (Cactaceae) and found that while the Sea of Cortéz has acted to separate mainland populations from those on peninsular Baja California, there is a particular population at the very northern extent of the range that is really a peninsular population even though it is spatially on the mainland. The differences, in aggregate, between peninsula and mainland populations are so large that even if we allocated an individual stratum into the wrong region, we still were able to derive a significant model. This is where model-free approaches are most powerful, they let the data tell us how it is distributed as opposed to asking if there is enough structure to fit into some pre-conceived model. A popular way to examine how individuals and populations are configured is to use an ordination approach or apply a multivariate rotation on the data. In what follows, I’ll describe a few ways to perform principal components analyses (though it is not really an analysis, it is more of a projection) on genotypes and genetic distance matrices. These approaches are common in multivariate statistics and have very little to do with population genetic processes or assumptions. There are several approaches amenable to transforming population genetic data into a form that is usable in normal multivariate statistics, the one used most often is discretizing the data. To demonstrate, consider the case where we have four individuals genotyped for a single locus as depicted below. library(gstudio) x &lt;- c( locus(c(1,2)), locus( c(1,1) ), locus(c(1,3)), locus(c(3,3))) x ## [1] &quot;1:2&quot; &quot;1:1&quot; &quot;1:3&quot; &quot;3:3&quot; We can convert these data into a matrix of data, where each row in the matrix represents an individual and each column determines the frequency of alleles in that individuals genotype. In this case, the resulting matrix should have four rows (one for each individual) and three columns (one for each of the three alleles present in the whole data). This conversion is done using the function to_mv(). X &lt;- to_mv(x) X ## 1 2 3 ## [1,] 0.5 0.5 0.0 ## [2,] 1.0 0.0 0.0 ## [3,] 0.5 0.0 0.5 ## [4,] 0.0 0.0 1.0 When using multiple loci, we can essentially bind onto this matrix additional columns of predictor variables. This matrix can be used as an input to most multivariate analyses. Depending upon the analysis being used, you may need to modify it a bit. Many statistical approaches require that you invert the covariance matrix. For example, the regression coefficients are found as: \\[ \\beta = (X&#39;X)^{-1}X&#39;Y \\] where the \\(X\\) matrix is the predictor data and the \\(Y\\) is the response variables. That \\((\\cdot)^{-1}\\) exponent represents inversion of the matrix resulting from the multiplication. The problem with the \\(\\mathbf{X}\\) matrix derived from the genotypes is that the rows are all linearly related in that they all sum to unity. rowSums(X) ## [1] 1 1 1 1 which is a problem for matrix inversion as it throws a zero into the mix and dividing by zero is still a problematic endeavor (I’m patiently waiting for the mathematicians to figure this out so my program stops crashing…). To fix this, we need to drop a column from each locus we use to estimate. This is an optional flag in the to_mv() function. X &lt;- to_mv(x,drop.allele = TRUE) X ## 1 2 ## [1,] 0.5 0.5 ## [2,] 1.0 0.0 ## [3,] 0.5 0.0 ## [4,] 0.0 0.0 4.4.1 Principal Component Rotations Once we have the proper data, we can perform a principal component (PC) rotation. A PC rotation is one that takes the original columns of data and performs a rotation on the values to align onto new ‘synthetic’ axes. Consider the example in the next figure. Here, some bivariate data is plot in 2-space, though this can be done for much higher dimensions of data as well—in fact it is more beneficial with more columns of data and this can be used as a way of reducing the dimensionality of the data while loosing very little (or no) content (more later on this). Figure 4.6: A rotation of 2-dimenational data from the original coordinate space (represented by the x- and y-axes) onto synthetic principal component (the red axes). The rotation itself maximizes the distributional width of the data (depicted as density plots in grey for the original axes and red for the rotated axes). The axes of a PC rotation are taken as linear combinations of the existing axes and define a new coordinate set onto which the points are plot. All points are rigidly constrained to keep the same relationship and there is no loss of information. The PC axes are defined by determining the most variable stretch through the data. In the figure on the next page, we see the raw data plot onto the X- and Y-axes. The axis of highest variance does not align with either of the original ones, and instead can be defined as a combination of both X- and Y- coordinates. If we take the blue axis as the first PC axis, the coordinate of the points would be taken along that new synthetic axis. The next PC axis is defined as being perpendicular to the previous one(s) and is identified as covering the largest variance in the data as before. This process continues until there are no more axes. In our case, the second axis would be at a right angle from the blue line (above). You can, at maximum, have as many PC axes as there are columns of data. However, the later axes may not explain any significant chunk of the underlying data, the process of rotating based upon axes of maximal variation may be able to capture the complete data set with fewer axes than the total set. This is where a technique like this may be helpful in reducing the dimensionality of the data. To perform this on raw genetic data, we first translate the raw genotypes into multivariate data, dropping an allele from each locus. library(gstudio) data(arapat) x &lt;- to_mv(arapat, drop.allele = TRUE) The full arapat data, with 363 individuals has 50 independent allele columns to it, after dropping one for each locus. Passing this to the princomp() function we get the results. fit.pca &lt;- princomp(x, cor = TRUE) Here are the first 8 (out of 50 potential) axes for the arapat data set. summary(fit.pca) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 ## Standard deviation 2.6637669 2.17988216 1.8194765 1.69808902 ## Proportion of Variance 0.1419131 0.09503772 0.0662099 0.05767013 ## Cumulative Proportion 0.1419131 0.23695080 0.3031607 0.36083083 ## Comp.5 Comp.6 Comp.7 Comp.8 ## Standard deviation 1.30595140 1.25287928 1.21993222 1.20493238 ## Proportion of Variance 0.03411018 0.03139413 0.02976469 0.02903724 ## Cumulative Proportion 0.39494101 0.42633514 0.45609983 0.48513707 ## Comp.9 Comp.10 Comp.11 Comp.12 ## Standard deviation 1.18746000 1.14296176 1.11208568 1.09678573 ## Proportion of Variance 0.02820123 0.02612723 0.02473469 0.02405878 ## Cumulative Proportion 0.51333830 0.53946553 0.56420022 0.58825900 ## Comp.13 Comp.14 Comp.15 Comp.16 ## Standard deviation 1.0837943 1.06056730 1.03893878 1.0244877 ## Proportion of Variance 0.0234922 0.02249606 0.02158788 0.0209915 ## Cumulative Proportion 0.6117512 0.63424726 0.65583513 0.6768266 ## Comp.17 Comp.18 Comp.19 Comp.20 ## Standard deviation 1.02114378 1.00993094 0.9979831 0.99607657 ## Proportion of Variance 0.02085469 0.02039921 0.0199194 0.01984337 ## Cumulative Proportion 0.69768133 0.71808054 0.7379999 0.75784331 ## Comp.21 Comp.22 Comp.23 Comp.24 ## Standard deviation 0.95870367 0.93685791 0.91638608 0.91262078 ## Proportion of Variance 0.01838225 0.01755405 0.01679527 0.01665753 ## Cumulative Proportion 0.77622557 0.79377962 0.81057489 0.82723242 ## Comp.25 Comp.26 Comp.27 Comp.28 ## Standard deviation 0.8906936 0.86951674 0.86619082 0.81902841 ## Proportion of Variance 0.0158667 0.01512119 0.01500573 0.01341615 ## Cumulative Proportion 0.8430991 0.85822031 0.87322604 0.88664219 ## Comp.29 Comp.30 Comp.31 Comp.32 ## Standard deviation 0.79160212 0.77846983 0.73738260 0.71024440 ## Proportion of Variance 0.01253268 0.01212031 0.01087466 0.01008894 ## Cumulative Proportion 0.89917487 0.91129518 0.92216984 0.93225878 ## Comp.33 Comp.34 Comp.35 Comp.36 ## Standard deviation 0.690065021 0.670553267 0.634366536 0.622844746 ## Proportion of Variance 0.009523795 0.008992834 0.008048418 0.007758712 ## Cumulative Proportion 0.941782576 0.950775409 0.958823827 0.966582539 ## Comp.37 Comp.38 Comp.39 Comp.40 ## Standard deviation 0.575373600 0.536515044 0.508130100 0.451602554 ## Proportion of Variance 0.006621096 0.005756968 0.005163924 0.004078897 ## Cumulative Proportion 0.973203635 0.978960602 0.984124526 0.988203424 ## Comp.41 Comp.42 Comp.43 Comp.44 ## Standard deviation 0.431084834 0.355831681 0.293272519 0.253948586 ## Proportion of Variance 0.003716683 0.002532324 0.001720175 0.001289798 ## Cumulative Proportion 0.991920106 0.994452430 0.996172605 0.997462403 ## Comp.45 Comp.46 Comp.47 Comp.48 ## Standard deviation 0.232386474 0.1725980744 0.140118702 0.1114152870 ## Proportion of Variance 0.001080069 0.0005958019 0.000392665 0.0002482673 ## Cumulative Proportion 0.998542473 0.9991382745 0.999530940 0.9997792069 ## Comp.49 Comp.50 ## Standard deviation 0.0914260473 5.177774e-02 ## Proportion of Variance 0.0001671744 5.361868e-05 ## Cumulative Proportion 0.9999463813 1.000000e+00 This output has two important components to it. First, it shows the axes, in decreasing order of importance and how much of the total variation they describe. The first Comp.1 axis explains 14.2% of the variance, the second explains 9.5%, etc. Second, it shows the cumulative proportion of the variation explained. From the 50 axes we started with, we can explain 49% of the variance by using just the first eight PC axes. Where this becomes meaningful for us is in how we can project our original data onto these new coordinate locations and look at the distribution to see if there are any obvious trends, partitions, gradients, etc. pred &lt;- predict(fit.pca) df &lt;- data.frame(PC1 = pred[, 1], PC2 = pred[, 2]) df$Species &lt;- arapat$Species df$Clade &lt;- arapat$Cluster df$Pop = arapat$Population ggplot(df) + geom_point(aes(x = PC1, y = PC2, shape = Species, color = Clade), size = 3, alpha = 0.75) We can see from the plot (I’ve added some designations to the points) that the 363 samples are clustered in an obvious way. The designation of ‘Species’ as depicted by the shape of the points, is defined by the mtDNA clade for each individual, independent of the nuclear marker data we are using here. Still, it shows a broad separation between the Cape, Mainland, and Peninsula groups. The colors of the points found within the Peninsula group, come more formal clustering approaches, as defined in the next two sections. In addition to working on raw genotypes, we can also perform coordinate rotations on population-level allele frequencies. Since we are using summaries of the groups here, I’ll make a data.frame that has the Stratum and Cluster designations that we can attach to the results when we plot it (e.g., so we can color the groups). data &lt;- droplevels( arapat[ arapat$Species == &quot;Peninsula&quot;,] ) groups &lt;- data.frame( unique( cbind(as.character(data$Population), as.character(data$Cluster)))) names(groups) &lt;- c(&quot;Stratum&quot;,&quot;Cluster&quot;) We have, by definition, thrown away some of the variance in our data by describing it using allele frequencies rather than the actual distribution of alleles at each locus. As such, we can gain, somewhat artificially due to the reduced amount of information content, higher explanatory power. In the following example, notice we use the to_mv_freq function which returns a single matrix of multilocus allele frequencies. library(ade4) freqs &lt;- to_mv_freq( data ) fit.dudi &lt;- dudi.pca( freqs, scale=FALSE, scannf=FALSE, nf=10) summary(fit.dudi) ## Class: pca dudi ## Call: dudi.pca(df = freqs, scale = FALSE, scannf = FALSE, nf = 10) ## ## Total inertia: 1.962 ## ## Eigenvalues: ## Ax1 Ax2 Ax3 Ax4 Ax5 ## 0.74409 0.61409 0.16652 0.11363 0.08908 ## ## Projected inertia (%): ## Ax1 Ax2 Ax3 Ax4 Ax5 ## 37.932 31.305 8.489 5.793 4.541 ## ## Cumulative projected inertia (%): ## Ax1 Ax1:2 Ax1:3 Ax1:4 Ax1:5 ## 37.93 69.24 77.73 83.52 88.06 ## ## (Only 5 dimensions (out of 32) are shown) Again, here we see a higher proportion of the variance (in allele frequencies) explained than from only the raw genotypes. df &lt;- data.frame( Axis=1:length(fit.dudi$eig), Lambda=fit.dudi$eig) df$Lambda &lt;- df$Lambda / sum( df$Lambda ) ggplot( df, aes(x=Axis,y=Lambda)) + geom_bar( stat=&quot;identity&quot;) + xlab(&quot;PCA Axis&quot;) + ylab(&quot;Proportion of Variation Explained&quot;) Projecting the data onto these new, synthetic, axes shows partitioning similar to what we have been seeing using other approaches. df &lt;- data.frame( PC1=fit.dudi$li$Axis1, PC2=fit.dudi$li$Axis2, Stratum=rownames(fit.dudi$li)) df &lt;- merge( df, groups) plot.pca &lt;- ggplot( df, aes(x=PC1,y=PC2, label=Stratum, color=Cluster)) + geom_text() plot.pca The last example I’m going to provide is yet another simplification of the data onto which we can perform coordinate rotations. Here we summarize the population-level differences into a single number, genetic distance, and perform the rotation on the genetic distance matrix itself. As most genetic distances are not ‘metric’ (e.g., do not conform to the triangle inequality) we need to make the matrix, quasi-euclidean to run the coordinate transformation. Performing rotations on distance matrices is called Principal Coordinate Analysis, or PCoA, for some reason, though it is exactly like what we’ve done before but we have changed out genotypes or allele frequencies for distance matrices. D &lt;- genetic_distance(data, mode = &quot;Nei&quot;) D &lt;- quasieuclid( as.dist( D ) ) fit.pcoa &lt;- dudi.pco(D,scannf = FALSE,full = TRUE, nf=10) summary( fit.pcoa ) ## Class: pco dudi ## Call: dudi.pco(d = D, scannf = FALSE, nf = 10, full = TRUE) ## ## Total inertia: 0.2354 ## ## Eigenvalues: ## Ax1 Ax2 Ax3 Ax4 Ax5 ## 0.125578 0.075894 0.015572 0.008647 0.003802 ## ## Projected inertia (%): ## Ax1 Ax2 Ax3 Ax4 Ax5 ## 53.336 32.234 6.614 3.673 1.615 ## ## Cumulative projected inertia (%): ## Ax1 Ax1:2 Ax1:3 Ax1:4 Ax1:5 ## 53.34 85.57 92.18 95.86 97.47 ## ## (Only 5 dimensions (out of 14) are shown) As we continue to simplify the description of inter-individual and inter-strata distances, we receive a concomitant increase in the amount of variation we can explain int he first few axes. df &lt;- data.frame( Axis=1:length(fit.pcoa$eig), Lambda=fit.pcoa$eig) df$Lambda &lt;- df$Lambda / sum( df$Lambda ) ggplot( df, aes(x=Axis,y=Lambda)) + geom_bar( stat=&quot;identity&quot;) + xlab(&quot;PCA Axis&quot;) + ylab(&quot;Proportion of Variation Explained&quot;) Though if there is structure in the data itself, all of these approaches should provide you with insights into what is going on, though they may not agree exactly. It is in the cases where genetic variation is cryptic that cause problems. df &lt;- data.frame( PC1=fit.pcoa$li$A1, PC2=fit.pcoa$li$A2, Stratum=rownames(fit.pcoa$li)) df &lt;- merge( df, groups) plot.pcoa &lt;- ggplot( df, aes(x=PC1,y=PC2, label=Stratum, color=Cluster)) + geom_text() plot.pcoa It is interesting to compare the output from rotations on frequencies and genetic distance matrices. library(cowplot) # this removes the legend from each plot so that the side-by-side # plots are not smunched together. plot.pca &lt;- plot.pca + guides( color=FALSE ) plot.pcoa &lt;- plot.pcoa + guides( color=FALSE ) + ylab(&quot;&quot;) plot_grid( plot.pca, plot.pcoa, labels=c(&quot;A&quot;,&quot;B&quot;), align=&quot;h&quot;) Figure 4.7: Comparison of population-level (A) principal component and (B) principal coordiante rotations on the same data set. Colors indicate the STRUCTURE clustering for each population. 4.4.2 Hierarchical Clustering In the previous section, we defined a new coordinate space for individuals and populations in the arapat data set. The rotation of the 50 allele encoding columns was able to describe over 95% of the observed variation using only the first 34 PC axes. As we further summarized the data by taking allele frequencies for each population and inter-population genetic distance, the number of axes needed to describe the data decreased. In this section, we are going to use the rotated coordinates to evaluate population-level differences under a model of hierarchical clustering. Hierarchical clustering are very helpful in understanding groupings in the data, particularly if there is a ‘nesting’ structure. While there are many ways to do it, they all generally proceed as follows: 1. Define a numeric metric that measured the distances between all \\(K\\) groups. 2. Find the two groups that have the smallest distance and coalesce them together into a pair. 3. Assume that the coalesced pair now constitutes a single entity, estimate the numeric metric among all \\(K-1\\) groups in the data set. 4. Go to #2 and repeat until you have coalesced all the groups together. Here again, it is the data that is telling us how it is structured rather than us imposing a model onto the data to see if it fits. To do this, the rotated coordinates are used to define the centroid of each population. Here I use the tapply() function as a short-cut to estimate the mean of each population. If you are not familiar with this approach, it essentially applies a particular function (in this case taking the mean), to a set of data in a matrix (the predicted coordinates from the PCA) based upon a set of factor levels (the populations). This is quite an efficient way to do what would take you a bit of looping to get through. p &lt;- ncol(pred) pops &lt;- arapat$Population pop.means &lt;- tapply( pred, list(rep(pops,p),col(pred)),mean) dim(pop.means) ## [1] 39 50 The result is a matrix where each row represents a population and each column represents the mean location of all individuals for each of the 50 PCA axes variables. These 50-dimensional coordinates can be used to define a pairwise distance metric using the dist() function. By default, this measures the euclidean distance (e.g., straight-line distance) between each populations 50-dimensional coordinate. pop_pw &lt;- dist(pop.means) h &lt;- hclust( pop_pw) plot(h) The plot of this shows the nesting structure of the populations as depicted in the PC-derived data. There are some interesting things to notice here. - The main separation between mainland populations (32, 101, 102) and peninsular populations (the rest) seems to be the largest difference. As indicated in the mtDNA (not used for this analysis), there seems to be a clear division between those two groups, consistent with the idea that the Sea of Cortéz has acted as a source of vicariance. - There seems to be a deep division within the peninsular populations separating out the group of populations including 98, Mat, 157, 73, 75, Aqu, ESan, 156, and 48. If we look at where these populations are found, we see that they are all located in the southern portion of Baja California—the Cape groupings… - Then there is the rest of the populations, some of which appear to be partitioned into at least two groups, though there may be more. This approach is pretty good at pulling apart components within the data set that represent different genetic clusters. 4.4.3 Interactive Tree Display Another network approach that could be used is defining bifurcating networks. Here we will examine the A. attenuatus data set using Nei’s genetic distance metric. \\[ D_{nei} = -\\ln( L ) \\] where \\[ L = \\frac{\\sum_l\\sum_u x_uy_u}{\\sqrt{(\\sum_l\\sum_u x_u^2)(\\sum_l\\sum_u y_u^2)}} \\] I make the among stratum distance matrix and then turn it into a dist object to do a hierarchical clustering on the populations. I then plot it and make the node colors equal to the grouping of the dendrogram if there were 4 groups in the data set. This is also zoom-able, for large trees, so if you want to figure out the relationships at the leaves, where the internal edges are smaller, zoom in with the mouse. It may help to “Zoom” the image. library(networkD3) d &lt;- genetic_distance(arapat,stratum = &quot;Population&quot;, mode=&quot;Nei&quot;) d &lt;- as.dist( d ) hc &lt;- hclust(d) grp_colors &lt;- c(&quot;red&quot;,&quot;green&quot;,&quot;orange&quot;,&quot;blue&quot;)[ cutree(hc,4)] dendroNetwork(hc, height=600, zoom=TRUE, textColour = grp_colors) There is a lot of information you can add to components of that tree by shading branches, overlaying other categorical data, etc. If you are interested in learning more about how to use interactive networks in our data display, there is a ton of information on web. 4.5 Networks In R, a network can be derived from several different kinds of data. For simplicity, lets start with an adjacency matrix, which represents the patterns of connectivity among nodes in a network. Here our nodes may represent individuals, locales, populations, regions, or any other clumping of our data. The adjacency matrix has as many rows and columns as there are distinct sampling units. Lets consider the case where we have 5 locales, from which we derived some measure of connectivity. The matrix has five rows and columns and a non-zero entry in each element where connectivity has been inferred. In R, we can set it up as: A &lt;- matrix(0, nrow=5, ncol=5) A[1,2] &lt;- A[2,3] &lt;- A[1,3] &lt;- A[3,4] &lt;- A[4,5] &lt;- 1 A &lt;- A + t(A) A ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 1 1 0 0 ## [2,] 1 0 1 0 0 ## [3,] 1 1 0 1 0 ## [4,] 0 0 1 0 1 ## [5,] 0 0 0 1 0 To convert this into a graph, we can use the igraph package—a comprehensive (and multilingual) package of network design, manipulation, and analysis. library(igraph) g &lt;- graph_from_adjacency_matrix( A , mode=&quot;undirected&quot;) There are several options available under the mode parameter, which describes the type of network we are going to be using. The following are available: undirected The connections between nodes are symmetric. This is the default for population graphs as covariance, the quantity the edge is representing is symmetrical. directed The edges are asymmetric. max or min Will take the largest (or smallest) value of the matrix (e.g., \\(max(A[i,j], A[j,i])\\) or \\(min( A[i,j], A[j,i])\\) ). upper or lower Uses either the upper or lower element of the matrix. plus Adds upper and lower values (e.g., \\(A[i,j] + A[j,i]\\)). The graph object presents several characteristics as an output including the number of nodes and edges, the mode of the graph, and a list of the edges (if there aren’t too many–in which case the list is truncated). g ## IGRAPH 8598dc8 U--- 5 5 -- ## + edges from 8598dc8: ## [1] 1--2 1--3 2--3 3--4 4--5 Here we see that this object is an igraph, is ’U’ndirected, and has 5 nodes and 5 edges. The edges are indicated by numbers and are graphically displayed. 4.5.1 Node &amp; Edge Attributes The underlying structure of an igraph object allows you to associate attributes (e.g., other data) with nodes and edges. Node attributes are accessed using the \\(V(graph)\\) operator (for vertex) and edge attributes are done via \\(E(graph)\\). Attributes can be set as well as retrieved using the same mechanisms for nodes. V(g)$name &lt;- c(&quot;Olympia&quot;,&quot;Bellingham&quot;,&quot;St. Louis&quot;,&quot;Ames&quot;,&quot;Richmond&quot;) V(g)$group &lt;- c(&quot;West&quot;,&quot;West&quot;, &quot;Central&quot;,&quot;Central&quot;,&quot;East&quot;) V(g)$color &lt;- &quot;#cca160&quot; list.vertex.attributes( g ) ## [1] &quot;name&quot; &quot;group&quot; &quot;color&quot; V(g)$name ## [1] &quot;Olympia&quot; &quot;Bellingham&quot; &quot;St. Louis&quot; &quot;Ames&quot; &quot;Richmond&quot; and for edges E(g) ## + 5/5 edges from 8598dc8 (vertex names): ## [1] Olympia --Bellingham Olympia --St. Louis Bellingham--St. Louis ## [4] St. Louis --Ames Ames --Richmond E(g)$color &lt;- c(&quot;red&quot;,&quot;red&quot;, &quot;red&quot;, &quot;blue&quot;,&quot;dark green&quot;) list.edge.attributes( g ) ## [1] &quot;color&quot; 4.5.2 Plotting a Graphs One of the main benefits to using R is that you can leverage the multitude of other packages to visualize and manipulate your data in interesting and informative ways. Since a popgraph is an instance of an igraph element, we can use the igraph routines for plotting. Here is an example. plot(g) There are several different options you can use to manipulate the graphical forms. By default, the plotting routines look for node and edge attributes such as name and color to plot the output appropriately. There are several additional plotting functions for plotting igraph objects. Here are some examples. plot(g, edge.color=&quot;black&quot;, vertex.label.color=&quot;darkred&quot;, vertex.color=&quot;#cccccc&quot;, vertex.label.dist=1) Below is the output from the igraph.plotting help file that covers the main options that you have for customizing the way that the network is displayed. Drawing graphs {igraph} R Documentation Drawing graphs Description The common bits of the three plotting functions plot.igraph, tkplot and rglplot are discussed in this manual page Details There are currently three different functions in the igraph package which can draw graph in various ways: plot.igraph does simple non-interactive 2D plotting to R devices. Actually it is an implementation of the plot generic function, so you can write plot(graph) instead of plot.igraph(graph). As it used the standard R devices it supports every output format for which R has an output device. The list is quite impressing: PostScript, PDF files, XFig files, SVG files, JPG, PNG and of course you can plot to the screen as well using the default devices, or the good-looking anti-aliased Cairo device. See plot.igraph for some more information. tkplot does interactive 2D plotting using the tcltk package. It can only handle graphs of moderate size, a thousend vertices is probably already too many. Some parameters of the plotted graph can be changed interactively after issuing the tkplot command: the position, color and size of the vertices and the color and width of the edges. See tkplot for details. rglplot is an experimental function to draw graphs in 3D using OpenGL. See rglplot for some more information. Please also check the examples below. How to specify graphical parameters There are three ways to give values to the parameters described below, in section 'Parameters'. We give these three ways here in the order of their precedence. The first method is to supply named arguments to the plotting commands: plot.igraph, tkplot or rglplot. Parameters for vertices start with prefix ‘vertex.', parameters for edges have prefix ‘edge.', and global parameters have no prefix. Eg. the color of the vertices can be given via argument vertex.color, whereas edge.color sets the color of the edges. layout gives the layout of the graphs. The second way is to assign vertex, edge and graph attributes to the graph. These attributes have no prefix, ie. the color of the vertices is taken from the color vertex attribute and the color of the edges from the color edge attribute. The layout of the graph is given by the layout graph attribute. (Always assuming that the corresponding command argument is not present.) Setting vertex and edge attributes are handy if you want to assign a given ‘look' to a graph, attributes are saved with the graph is you save it with save or in GraphML format with write_graph, so the graph will have the same look after loading it again. If a parameter is not given in the command line, and the corresponding vertex/edge/graph attribute is also missing then the general igraph parameters handled by igraph_options are also checked. Vertex parameters have prefix ‘vertex.', edge parameters are prefixed with ‘edge.', general parameters like layout are prefixed with ‘plot'. These parameters are useful if you want all or most of your graphs to have the same look, vertex size, vertex color, etc. Then you don't need to set these at every plotting, and you also don't need to assign vertex/edge attributes to every graph. If the value of a parameter is not specified by any of the three ways described here, its default valued is used, as given in the source code. Different parameters can have different type, eg. vertex colors can be given as a character vector with color names, or as an integer vector with the color numbers from the current palette. Different types are valid for different parameters, this is discussed in detail in the next section. It is however always true that the parameter can always be a function object in which it will be called with the graph as its single argument to get the “proper\" value of the parameter. (If the function returns another function object that will not be called again...) The list of parameters Vertex parameters first, note that the ‘vertex.' prefix needs to be added if they are used as an argument or when setting via igraph_options. The value of the parameter may be scalar valid for every vertex or a vector with a separate value for each vertex. (Shorter vectors are recycled.) size The size of the vertex, a numeric scalar or vector, in the latter case each vertex sizes may differ. This vertex sizes are scaled in order have about the same size of vertices for a given value for all three plotting commands. It does not need to be an integer number. The default value is 15. This is big enough to place short labels on vertices. size2 The “other\" size of the vertex, for some vertex shapes. For the various rectangle shapes this gives the height of the vertices, whereas size gives the width. It is ignored by shapes for which the size can be specified with a single number. The default is 15. color The fill color of the vertex. If it is numeric then the current palette is used, see palette. If it is a character vector then it may either contain integer values, named colors or RGB specified colors with three or four bytes. All strings starting with ‘#' are assumed to be RGB color specifications. It is possible to mix named color and RGB colors. Note that tkplot ignores the fourth byte (alpha channel) in the RGB color specification. For plot.igraph and integer values, the default igraph palette is used (see the ‘palette' parameter below. Note that this is different from the R palette. If you don't want (some) vertices to have any color, supply NA as the color name. The default value is “SkyBlue2\". frame.color The color of the frame of the vertices, the same formats are allowed as for the fill color. If you don't want vertices to have a frame, supply NA as the color name. By default it is “black\". shape The shape of the vertex, currently “circle\", “square\", “csquare\", “rectangle\", “crectangle\", “vrectangle\", “pie\" (see vertex.shape.pie), ‘sphere', and “none\" are supported, and only by the plot.igraph command. “none\" does not draw the vertices at all, although vertex label are plotted (if given). See shapes for details about vertex shapes and vertex.shape.pie for using pie charts as vertices. The “sphere\" vertex shape plots vertices as 3D ray-traced spheres, in the given color and size. This produces a raster image and it is only supported with some graphics devices. On some devices raster transparency is not supported and the spheres do not have a transparent background. See dev.capabilities and the ‘rasterImage' capability to check that your device is supported. By default vertices are drawn as circles. label The vertex labels. They will be converted to character. Specify NA to omit vertex labels. The default vertex labels are the vertex ids. label.family The font family to be used for vertex labels. As different plotting commands can used different fonts, they interpret this parameter different ways. The basic notation is, however, understood by both plot.igraph and tkplot. rglplot does not support fonts at all right now, it ignores this parameter completely. For plot.igraph this parameter is simply passed to text as argument family. For tkplot some conversion is performed. If this parameter is the name of an exixting Tk font, then that font is used and the label.font and label.cex parameters are ignored complerely. If it is one of the base families (serif, sans, mono) then Times, Helvetica or Courier fonts are used, there are guaranteed to exist on all systems. For the ‘symbol' base family we used the symbol font is available, otherwise the first font which has ‘symbol' in its name. If the parameter is not a name of the base families and it is also not a named Tk font then we pass it to tkfont.create and hope the user knows what she is doing. The label.font and label.cex parameters are also passed to tkfont.create in this case. The default value is ‘serif'. label.font The font within the font family to use for the vertex labels. It is interpreted the same way as the the font graphical parameter: 1 is plain text, 2 is bold face, 3 is italic, 4 is bold and italic and 5 specifies the symbol font. For plot.igraph this parameter is simply passed to text. For tkplot, if the label.family parameter is not the name of a Tk font then this parameter is used to set whether the newly created font should be italic and/or boldface. Otherwise it is ignored. For rglplot it is ignored. The default value is 1. label.cex The font size for vertex labels. It is interpreted as a multiplication factor of some device-dependent base font size. For plot.igraph it is simply passed to text as argument cex. For tkplot it is multiplied by 12 and then used as the size argument for tkfont.create. The base font is thus 12 for tkplot. For rglplot it is ignored. The default value is 1. label.dist The distance of the label from the center of the vertex. If it is 0 then the label is centered on the vertex. If it is 1 then the label is displayed beside the vertex. The default value is 0. label.degree It defines the position of the vertex labels, relative to the center of the vertices. It is interpreted as an angle in radian, zero means ‘to the right', and ‘pi' means to the left, up is -pi/2 and down is pi/2. The default value is -pi/4. label.color The color of the labels, see the color vertex parameter discussed earlier for the possible values. The default value is black. Edge parameters require to add the ‘edge.' prefix when used as arguments or set by igraph_options. The edge parameters: color The color of the edges, see the color vertex parameter for the possible values. By default this parameter is darkgrey. width The width of the edges. The default value is 1. arrow.size The size of the arrows. Currently this is a constant, so it is the same for every edge. If a vector is submitted then only the first element is used, ie. if this is taken from an edge attribute then only the attribute of the first edge is used for all arrows. This will likely change in the future. The default value is 1. arrow.width The width of the arrows. Currently this is a constant, so it is the same for every edge. If a vector is submitted then only the first element is used, ie. if this is taken from an edge attribute then only the attribute of the first edge is used for all arrows. This will likely change in the future. This argument is currently only used by plot.igraph. The default value is 1, which gives the same width as before this option appeared in igraph. lty The line type for the edges. Almost the same format is accepted as for the standard graphics par, 0 and “blank\" mean no edges, 1 and “solid\" are for solid lines, the other possible values are: 2 (“dashed\"), 3 (“dotted\"), 4 (“dotdash\"), 5 (“longdash\"), 6 (“twodash\"). tkplot also accepts standard Tk line type strings, it does not however support “blank\" lines, instead of type ‘0' type ‘1', ie. solid lines will be drawn. This argument is ignored for rglplot. The default value is type 1, a solid line. label The edge labels. They will be converted to character. Specify NA to omit edge labels. Edge labels are omitted by default. label.family Font family of the edge labels. See the vertex parameter with the same name for the details. label.font The font for the edge labels. See the corresponding vertex parameter discussed earlier for details. label.cex The font size for the edge labels, see the corresponding vertex parameter for details. label.color The color of the edge labels, see the color vertex parameters on how to specify colors. label.x The horizontal coordinates of the edge labels might be given here, explicitly. The NA elements will be replaced by automatically calculated coordinates. If NULL, then all edge horizontal coordinates are calculated automatically. This parameter is only supported by plot.igraph. label.y The same as label.x, but for vertical coordinates. curved Specifies whether to draw curved edges, or not. This can be a logical or a numeric vector or scalar. First the vector is replicated to have the same length as the number of edges in the graph. Then it is interpreted for each edge separately. A numeric value specifies the curvature of the edge; zero curvature means straight edges, negative values means the edge bends clockwise, positive values the opposite. TRUE means curvature 0.5, FALSE means curvature zero. By default the vector specifying the curvatire is calculated via a call to the curve_multiple function. This function makes sure that multiple edges are curved and are all visible. This parameter is ignored for loop edges. The default value is FALSE. This parameter is currently ignored by rglplot. arrow.mode This parameter can be used to specify for which edges should arrows be drawn. If this parameter is given by the user (in either of the three ways) then it specifies which edges will have forward, backward arrows, or both, or no arrows at all. As usual, this parameter can be a vector or a scalar value. It can be an integer or character type. If it is integer then 0 means no arrows, 1 means backward arrows, 2 is for forward arrows and 3 for both. If it is a character vector then “\" and “->\" forward arrows and “<>\" and “\" stands for both arrows. All other values mean no arrows, perhaps you should use “-\" or “–\" to specify no arrows. Hint: this parameter can be used as a ‘cheap' solution for drawing “mixed\" graphs: graphs in which some edges are directed some are not. If you want do this, then please create a directed graph, because as of version 0.4 the vertex pairs in the edge lists can be swapped in undirected graphs. By default, no arrows will be drawn for undirected graphs, and for directed graphs, an arrow will be drawn for each edge, according to its direction. This is not very surprising, it is the expected behavior. loop.angle Gives the angle in radian for plotting loop edges. See the label.dist vertex parameter to see how this is interpreted. The default value is 0. loop.angle2 Gives the second angle in radian for plotting loop edges. This is only used in 3D, loop.angle is enough in 2D. The default value is 0. Other parameters: layout Either a function or a numeric matrix. It specifies how the vertices will be placed on the plot. If it is a numeric matrix, then the matrix has to have one line for each vertex, specifying its coordinates. The matrix should have at least two columns, for the x and y coordinates, and it can also have third column, this will be the z coordinate for 3D plots and it is ignored for 2D plots. If a two column matrix is given for the 3D plotting function rglplot then the third column is assumed to be 1 for each vertex. If layout is a function, this function will be called with the graph as the single parameter to determine the actual coordinates. The function should return a matrix with two or three columns. For the 2D plots the third column is ignored. The default value is layout_nicely, a smart function that chooses a layouter based on the graph. margin The amount of empty space below, over, at the left and right of the plot, it is a numeric vector of length four. Usually values between 0 and 0.5 are meaningful, but negative values are also possible, that will make the plot zoom in to a part of the graph. If it is shorter than four then it is recycled. rglplot does not support this parameter, as it can zoom in and out the graph in a more flexible way. Its default value is 0. palette The color palette to use for vertex color. The default is categorical_pal, which is a color-blind friendly categorical palette. See its manual page for details and other palettes. This parameters is only supported by plot, and not by tkplot and rglplot. rescale Logical constant, whether to rescale the coordinates to the [-1,1]x[-1,1](x[-1,1]) interval. This parameter is not implemented for tkplot. Defaults to TRUE, the layout will be rescaled. asp A numeric constant, it gives the asp parameter for plot, the aspect ratio. Supply 0 here if you don't want to give an aspect ratio. It is ignored by tkplot and rglplot. Defaults to 1. frame Boolean, whether to plot a frame around the graph. It is ignored by tkplot and rglplot. Defaults to FALSE. main Overall title for the main plot. The default is empty if the annotate.plot igraph option is FALSE, and the graph's name attribute otherwise. See the same argument of the base plot function. Only supported by plot. sub Subtitle of the main plot, the default is empty. Only supported by plot. xlab Title for the x axis, the default is empty if the annotate.plot igraph option is FALSE, and the number of vertices and edges, if it is TRUE. Only supported by plot. ylab Title for the y axis, the default is empty. Only supported by plot. Author(s) Gabor Csardi csardi.gabor@gmail.com See Also plot.igraph, tkplot, rglplot, igraph_options Examples ## Not run: # plotting a simple ring graph, all default parameters, except the layout g In addition to the physical appearance of nodes, edges, and labels, networks are must also have a ‘layout’ that describes the relative position of nodes on the plot surface. There are several ways you can define a layout, here are some examples. layout &lt;- layout.circle( g ) plot( g, layout=layout) layout &lt;- layout.fruchterman.reingold( g ) plot( g, layout=layout) In addition to normal plotting, you can also integrate interactive plotting. Here is an example using the networkD3 library. It is interactive, so grab one of the nodes and move it around. library(networkD3) edgelist &lt;- as_edgelist(g) df &lt;- data.frame( src=edgelist[,1], target=edgelist[,2]) simpleNetwork(df,fontSize = 14,opacity = 0.95) 4.6 Mapping Networks For quick maps I typically use the maps library. It is pretty straightforward to use and does not take too much thought to quickly plot something or find the appropriate raster files. Below, I add some coordinates to the data set. V(g)$Latitude &lt;- c( 47.15, 48.75,38.81, 42.26, 37.74 ) V(g)$Longitude &lt;- c(-122.89,-122.49,-89.98, -93.47, -77.16 ) Then overlay this onto a map using the overlay_popgraph() function. Here is an example where I plot it over the map of the US states. library(maps) library(popgraph) pg &lt;- as.popgraph( g ) map( &quot;state&quot; ) overlay_popgraph( pg ) Figure 4.8: Map of graph stetched onto continential US map. This function requires that you already have a plot available (it uses the lines() and points() routines). If you try to just overlay this with no existing plot, it will not work (and should throw an error). 4.7 Genetic Distance Graphs Several graph-theoretic approaches have been suggested in the literature, some of which are based upon statistical models (e.g., popgraphs in the next chapter) and some of which are less structured. A common approach has been to use a measure of pair-wise genetic distance, measured between individuals or strata. In the following example, Nei’s genetic distance is used, though other types of distance may also be employed. D &lt;- genetic_distance(arapat,mode=&quot;Nei&quot;) yielding a \\(KxK\\) distance matrix. Nei’s distance produces values that are non-negative D[1:6,1:6] ## 101 102 12 153 156 157 ## 101 0.0000000 0.226820 0.8385165 0.9177730 1.19671667 1.09526743 ## 102 0.2268200 0.000000 1.2589813 1.1874872 1.29512009 1.25632140 ## 12 0.8385165 1.258981 0.0000000 0.1340892 1.22979312 0.90185134 ## 153 0.9177730 1.187487 0.1340892 0.0000000 1.14458359 0.86975864 ## 156 1.1967167 1.295120 1.2297931 1.1445836 0.00000000 0.03459425 ## 157 1.0952674 1.256321 0.9018513 0.8697586 0.03459425 0.00000000 and in the case of the arapat data, produces a bivariate distribution of distances. df &lt;- data.frame( Nei=D[ lower.tri(D)]) ggplot(df, aes(x=Nei)) + geom_histogram(bins=20) If we were to plot a graph based upon pair-wise genetic distances, such as that above, we would get a saturated network (e.g., everything is connected to everything else). g.nei &lt;- graph.adjacency(D, mode = &quot;undirected&quot;, weighted = TRUE) plot( g.nei ) Figure 4.9: A graph constructed from Nei’s genetic distance matrix for the whole arapat data set. Not too informative. However, we can ‘prune’ the genetic distance matrix, producing reduced edge sets, which presumably will provide some valuable information on the internal structure of the graph. In the next set of code, I’m going to order the edges by the weight and remove them one at a time. For each reduced edge set graph, I’m going to record how many connected groups are in the topology. If the removal of the edge causes a bifurcation of the network into more than the current number of components, I’m going to save that graph into a list I made beforehand. # copy the graph g &lt;- g.nei # list for saving graphs graphs_to_save &lt;- list() num_clusters &lt;- components( g )$no graphs_to_save[[num_clusters]] &lt;- g e &lt;- E(g)$weight for( i in 1:length(e)) { idx &lt;- which( e == max(e) ) g &lt;- delete_edges(g, idx ) tmp &lt;- components( g )$no if( tmp != num_clusters ){ num_clusters &lt;- tmp graphs_to_save[[num_clusters]] &lt;- g } e &lt;- E(g)$weight } We can then iterate through the graphs, looking at how the saturated graph decomposes into sub components. plot(graphs_to_save[[2]]) plot(graphs_to_save[[3]]) plot(graphs_to_save[[4]]) We can see which populations belong to each cluster as: components( graphs_to_save[[3]]) ## $membership ## 101 102 12 153 156 157 159 160 161 162 163 164 ## 1 1 2 2 3 3 2 2 2 2 3 2 ## 165 166 168 169 171 173 175 177 32 48 51 58 ## 2 2 2 2 2 2 2 2 1 3 2 2 ## 64 73 75 77 84 88 89 9 93 98 Aqu Const ## 2 3 3 2 2 2 2 2 2 3 3 3 ## ESan Mat SFr ## 3 3 2 ## ## $csize ## [1] 3 25 11 ## ## $no ## [1] 3 and interpret patterns by looking at how the topology fragments. 4.8 Population Graphs Population Graphs are a statistical representation of among population genetic variance, \\(\\sigma^2_A\\), as viewed through a network (Dyer &amp; Nason 2004). A population graph is a graph-theoretic interpretation of genetic covariance and serves as a tool for understanding underlying evolutionary history for a set of populations. These structures are estimated in R using the popgraphs library. library(popgraph) As other networks, a population graph is a graph-theoretic structure that can be represented. Here we will focus on the former approach as it is native to this package. If you use the latter one, it will produce a *.pgraph file and you can read it in using the read_popgraph() function. 4.9 Adding data to a graph A population graph is made more informative if you can associate some data with topology. External data may be spatial or ecological data associated with each node. Edge data may be a bit more complicated as it is traversing both spatial and ecological gradients and below we’ll see how to extract particular from rasters using edge crossings. Included in the popgraph package are some build-in data sets to illustrate some of the utility. Included is the cactus topology that was originally used to develop this approach (from Dyer &amp; Nason 2004). data(lopho) class(lopho) ## [1] &quot;popgraph&quot; &quot;igraph&quot; lopho ## IGRAPH 6af7beb UNW- 21 52 -- ## + attr: name (v/c), size (v/n), color (v/c), Region (v/c), weight ## | (e/n) ## + edges from 6af7beb (vertex names): ## [1] BaC--LaV BaC--Lig BaC--PtC BaC--PtP BaC--SnE ## [6] BaC--SnI BaC--StR Ctv--PtP Ctv--SLG Ctv--SnF ## [11] Ctv--SenBas LaV--Lig LaV--PtC LaV--SnE LaV--SnF ## [16] LaV--TsS Lig--PtC Lig--SnI Lig--StR Lig--TsS ## [21] PtC--SnE PtC--StR PtC--TsS PtC--SenBas PtP--SnF ## [26] PtP--SnI PtP--SenBas SLG--SnF SLG--SnI SnE--StR ## [31] SnE--TsS SnF--SnI SnI--StR StR--TsS StR--SenBas ## + ... omitted several edges We can associate data with the nodes using the decorate_graph() function. This takes a data.frame object and tries to match up the columns of data in the data.frame to the nodes. Here is an example with some addition built-in data. The option stratum indicates the name of the column that has the node labels in it (which are stored as V(graph)$name). data(baja) summary(baja) ## Region Population Latitude Longitude ## Baja :16 BaC : 1 Min. :22.93 Min. :-114.7 ## Sonora:13 Cabo : 1 1st Qu.:24.45 1st Qu.:-112.6 ## CP : 1 Median :27.95 Median :-111.8 ## Ctv : 1 Mean :27.33 Mean :-111.8 ## ELR : 1 3rd Qu.:29.59 3rd Qu.:-110.7 ## IC : 1 Max. :31.95 Max. :-109.5 ## (Other):23 lopho &lt;- decorate_graph( lopho, baja, stratum=&quot;Population&quot;) lopho ## IGRAPH 6af7beb UNW- 21 52 -- ## + attr: name (v/c), size (v/n), color (v/c), Region (v/c), ## | Latitude (v/n), Longitude (v/n), weight (e/n) ## + edges from 6af7beb (vertex names): ## [1] BaC--LaV BaC--Lig BaC--PtC BaC--PtP BaC--SnE ## [6] BaC--SnI BaC--StR Ctv--PtP Ctv--SLG Ctv--SnF ## [11] Ctv--SenBas LaV--Lig LaV--PtC LaV--SnE LaV--SnF ## [16] LaV--TsS Lig--PtC Lig--SnI Lig--StR Lig--TsS ## [21] PtC--SnE PtC--StR PtC--TsS PtC--SenBas PtP--SnF ## [26] PtP--SnI PtP--SenBas SLG--SnF SLG--SnI SnE--StR ## [31] SnE--TsS SnF--SnI SnI--StR StR--TsS StR--SenBas ## + ... omitted several edges Each vertex now has several different types of data associated with it now. We will use this below. 4.9.1 Plotting a graph using ggplot2 routines The ggplot2 package provides a spectacular plotting environment in an intuitive context and there are now some functions to support the Population Graphs in this context. If you haven’t used ggplot2 before, it may at first be a bit odd because it deviates from normal plotting approaches where you just shove a bunch of arguments into a single plotting function. In ggplot, you build a graphic in the same way you build a regression equation. A regression equation has an intercept and potentially a bunch of independent terms. This is exactly how ggplot builds plots, by adding together components. To specify how things look in a plot, you need to specify an aesthetic using the aes() function. Here is where you supply the variable names you use for coordinate, coloring, shape, etc. For both of the geom_*set functions, these names must be attributes of either the node or edge sets in the graph itself. Here is an example using the Lopohcereus graph. We begin by making a ggplot() object and then adding to it a geom_ object. The 5popgraph package comes with two functions, one for edges and one for nodes. library(ggplot2) p &lt;- ggplot() p &lt;- p + geom_edgeset( aes(x=Longitude,y=Latitude), lopho ) p I broke up the plotting into several lines to improve readability, it is not necessary to to this in practice though. The addition of additional geom_ objects to the plot will layer them on top (n.b., I also passed the size=4 option to the plot as the default point size is a bit too small and this is how you could change that). p &lt;- p + geom_nodeset( aes(x=Longitude, y=Latitude), lopho, size=4) p And then you can add additional options to the plot, like axis labels and a less exciting background theme (the theme_empty() provided by popgraph is actually transparent so you can save the image and throw it into a presentation as necessary). p &lt;- ggplot() + geom_edgeset( aes(x=Longitude,y=Latitude), lopho, color=&quot;darkgrey&quot; ) p &lt;- p + geom_nodeset( aes(x=Longitude, y=Latitude, color=Region, size=size), lopho) p &lt;- p + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) p + theme_minimal() You can also use the default layout routines in igraph for visualization. Here is an example using Fruchterman-Reingold algorithm. library(igraph) c &lt;- layout.fruchterman.reingold( lopho ) V(lopho)$x &lt;- c[,1] V(lopho)$y &lt;- c[,2] p &lt;- ggplot() + geom_edgeset( aes(x,y), lopho, color=&quot;darkgrey&quot; ) p &lt;- p + geom_nodeset( aes(x, y, color=Region, size=size), lopho) p + theme_minimal() 4.9.2 Reading Existing popgraph Files The online versions of Population Graphs provides a *.pgraph file for visualization. These files are visualized in several different software platforms including GeneticStudio (Dyer 2009), a OpenGL visualization application (Dyer &amp; Nason 2004), an online visualization framework at http://dyerlab.com, and of course, in R. We shall focus on this last one. Reading in files to R graph &lt;- read.popgraph( &quot;thegraph.pgraph&quot; ) 4.9.3 Saving Population Graph Objects A popgraph object is a normal R object and can be saved using the normal R mechanisms. save( lopho, file=&quot;MyLophoGraph.rda&quot;) For interoperability, popgraph objects can also be saved in other formats. These are accessed through the write.popgraph() function. write.popgraph(lopho,file=&quot;~/Desktop/Cactus.pgraph&quot;, format=&quot;pgraph&quot;) There are several other options available for outputting your graph. Currently the other formats that have been implemented are: json A format for html javascript data processing. kml The Keyhole Markup Language which is read by GoogleEarth. This requires Latitude and Longtitude vertex properties. graphml The graph markup language. html Export as an interactive html document you can manipulate on your desktop (uses javascript d3.js library so you need an internet connection). pajek Export to a format that works with the software Pajek (http://pajek.imfm.si/doku.php?id=pajek) pgraph The format used in GeneticStudio and the original popgraph 3D viewer (this is the default). adjacency Saves the adjacency matrix of the graph (binary) as a CSV file paths Saves the shortest paths matrix as a CSV file weights Saves the adjacency matrix with edge weights. 4.9.4 Interactive Networks To create a popgraph, you need to pass the popgraph() function genotypes as multivariate variables—the function to_mv() does this behind the scene—and a vector of variables that allocate each row of data to a node. Here we use the ‘Population’ vector from the arapat data.frame. library(gstudio) library(popgraph) data(arapat) graph &lt;- popgraph(to_mv(arapat),groups = arapat$Population) print(graph) ## IGRAPH 269e561 UNW- 39 71 -- ## + attr: name (v/c), size (v/n), weight (e/n) ## + edges from 269e561 (vertex names): ## [1] 101--102 101--32 102--32 12 --161 12 --165 12 --93 ## [7] 153--165 153--58 156--157 156--48 156--73 156--75 ## [13] 157--48 157--Aqu 157--ESan 159--171 159--173 159--89 ## [19] 160--168 160--169 160--93 160--SFr 161--162 161--165 ## [25] 161--93 161--SFr 162--64 162--77 162--93 163--75 ## [31] 163--Const 163--ESan 164--165 164--169 164--51 164--Const ## [37] 164--SFr 165--168 165--169 165--77 166--168 168--51 ## [43] 168--58 168--64 168--77 169--58 169--93 169--SFr ## + ... omitted several edges The forceNetwork() function is what does the plotting and it needs some data that are in a specific format. Essentially, there needs to be two data.frame objects with the following attributes: nodes - A data.frame with each row representing the name of the node to be displayed, the group the node belongs to (if there are groupings of nodes to be displayed by alternative colors), and a vector of node sizes. edges - A data.frame representing the edge connecting the nodes, labeled as ‘from’ and ‘to’ and a vector of weights. The ‘from’ and ‘to’ vectors need to be numeric values of the nodes in the other data frame and need to be 0-indexed (e.g., the first node name it is going to look up is indexed as ‘0’ in the javascript instead of ‘1’ as is common in R). nodes &lt;- to_data.frame( graph, mode=&quot;nodes&quot;, as.named=FALSE ) edges &lt;- to_data.frame( graph, mode=&quot;edges&quot;, as.named=FALSE ) edges$source &lt;- edges$source - 1 edges$target &lt;- edges$target - 1 The only last thing to do is to define a grouping of populations. This will be represented in the network as a coloring. For this one, I’m going to use the hypothesized STRUCTURE clustering. In the arapat data set, there is a designation for each individual on which cluster they belong. Some populations are ‘pure’ in their groupings but others (in spatial regions of sympatry) they are mixed. Below I determine the estimated STRUCTURE groups for each population and collapse those who have more than one into a single string. grps &lt;- by( arapat$Cluster, arapat$Population, unique ) l &lt;- lapply( grps, function(x) { g &lt;- paste(sort(x),collapse=&quot;/&quot;) }) df &lt;- data.frame( name=names(l), group=as.character(l)) nodes &lt;- merge( nodes, df ) Once defined, we can then call the function to make the data.frame objects and then do the plot. These graphics are interactive, grab a node and drag it around! library(networkD3) forceNetwork(Links = edges, Nodes = nodes, Source = &quot;source&quot;, Target = &quot;target&quot;, Value = &quot;value&quot;, NodeID = &quot;name&quot;, Group = &quot;group&quot;, opacity=1.0, legend=TRUE, fontSize = 16, zoom=TRUE ) 4.9.5 Spatial Population Graphs Mapping the nodes and edges onto real space is a key task in the understanding of how covariance is partitioned on the landscape. There are several approaches that can be used in R since it is such a flexible platform. In what follows I will use a series of techniques that I find useful ordered from the simplest to the more complicated. 4.9.6 Integrating Google and ggplot2 for Plotting R has some pretty good facilities for using spatial assets from Google and OpenStreetMaps and is a very easy way to get quick plots from Population Graphs, particularly if you can integrate it into the ggplot2 framework. Using the ggmap package, you can request map tiles and use as backgrounds onto which you can plot specific objects. To do so, you must first get: Either the centroid of the location you are interested in finding and a value of zoom (just like in Google maps), or A bounding box with left, bottom, right, top coordinates. This is a bit of an experimental thing and does not always get you what you want. Some fiddling is required with either way you go. The map you get from get_map() is essentially a huge matrix of hex colors as shown above. library(ggmap) location &lt;- c( mean(V(lopho)$Longitude), mean(V(lopho)$Latitude)) location ## [1] -112.17571 27.91048 map &lt;- get_map(location,maptype=&quot;satellite&quot;, zoom=6) dim(map) ## [1] 1280 1280 map[1:4,1:4] ## [,1] [,2] [,3] [,4] ## [1,] &quot;#3C5283&quot; &quot;#394D83&quot; &quot;#394D83&quot; &quot;#3C5283&quot; ## [2,] &quot;#3C5283&quot; &quot;#3C5283&quot; &quot;#3C5283&quot; &quot;#405285&quot; ## [3,] &quot;#3C5283&quot; &quot;#3C5283&quot; &quot;#3C5283&quot; &quot;#40558A&quot; ## [4,] &quot;#3C5283&quot; &quot;#3C5283&quot; &quot;#3C5283&quot; &quot;#40558A&quot; This map object can be passed on to ggmap(), which replaces the traditional ggplot() function and sets up the bounding box in terms of Latitude and Longtime. Onto this, you can plot the graph topology using: geom_edgeset() This takes the graph and plots out the edges. geom_nodeset() This plots out the nodes. You could probably use a regular data.frame and geom_point() as well. Here is an example: p &lt;- ggmap( map ) p &lt;- p + geom_edgeset( aes(x=Longitude,y=Latitude), lopho, color=&quot;white&quot; ) p &lt;- p + geom_nodeset( aes(x=Longitude, y=Latitude, color=Region, size=size), lopho) p + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) 4.9.7 Integrating Raster Maps At times we have raster data upon we can plot a population graph. Here is an example from Baja California. The underlying raster image is cropped from a WorldClim tile and represents elevation. library(raster) data(alt) plot(alt) Since it is a raster object, it knows how to plot itself relatively well. There are a ton of good references for showing you how to play with raster data (e.g., Bivand et al. 2008) To plot our graph onto this topology, we export the spatial components of the graph into objects that interact with rasters. The packages provides simple extraction of features into SpatialLines and SpatialPoints objects. lopho.nodes &lt;- to_SpatialPoints(lopho) lopho.nodes ## class : SpatialPoints ## features : 21 ## extent : -114.73, -109.99, 23.58, 31.95 (xmin, xmax, ymin, ymax) ## coord. ref. : NA lopho.edges &lt;- to_SpatialLines(lopho) head(lopho.edges) ## class : SpatialLines ## features : 1 ## extent : -111.79, -109.99, 24.04, 26.59 (xmin, xmax, ymin, ymax) ## coord. ref. : NA Once we have them extracted into the right format, we can add them to the raster plot. I plot the nodes twice to overlay a circular icon (pch=16) onto the default cross marker and make them 50 % larger (cex=1.5). plot( alt ) plot( lopho.edges, add=TRUE, col=&quot;#555555&quot; ) plot( lopho.nodes, add=TRUE, col=&quot;black&quot;, cex=1.5 ) plot( lopho.nodes, add=TRUE, col=V(lopho)$color, pch=16, cex=1.5 ) 4.9.8 Extracting Spatial Data Using Population Graphs Since we are dealing with the spatial stuff right now, it makes sense to look into how we can use the topological features of the graph to extract spatial data. 4.9.8.1 Node Specific Data The node data nodes is a SpatialPoints object and can be used to pull data from raster sources. I’ll start by creating a data.frame with some existing data in it. df.nodes &lt;- data.frame(Pop=V(lopho)$name, Latitude=V(lopho)$Latitude, Longitude=V(lopho)$Longitude) Then we can extract the elevation from the alt raster as: library(raster) df.nodes$Elevation &lt;- extract( alt, lopho.nodes ) summary(df.nodes) ## Pop Latitude Longitude Elevation ## BaC : 1 Min. :23.58 Min. :-114.7 Min. : 5.0 ## CP : 1 1st Qu.:25.73 1st Qu.:-112.9 1st Qu.: 14.0 ## Ctv : 1 Median :28.82 Median :-112.0 Median : 66.0 ## LaV : 1 Mean :27.91 Mean :-112.2 Mean :159.2 ## LF : 1 3rd Qu.:29.73 3rd Qu.:-111.3 3rd Qu.:259.0 ## Lig : 1 Max. :31.95 Max. :-110.0 Max. :667.0 ## (Other):15 Additional data could be extracted from other rasters. See http://worldclim.org for some example data that may prove useful. 4.9.8.2 Extracting Data Along Popgraph Edges It is also possible to extract data along vectors, or other SpatialLines objects, which the edges in a popgraph can be transformed into. This is a particularly helpful approach if you are trying to quantify the value of characteristics of the environment between your sampling locations. In the following example, I estimate the population graph from the arapat data set coords &lt;- strata_coordinates(arapat) graph &lt;- decorate_graph( graph, coords ) edges &lt;- to_SpatialLines(graph) proj4string(edges) &lt;- CRS( proj4string( alt )) plot( alt, legend=FALSE) plot(edges,add=TRUE) and determine which of the edges has the longest length. edge_lengths &lt;- SpatialLinesLengths( edges ) longest &lt;- sort( edge_lengths,decreasing = TRUE )[1] longest ## Edge 164 SFr ## 322.8491 This edge is found at: idx ## Edge 164 SFr ## 37 edge &lt;- edges[ 37 ] edge ## class : SpatialLines ## features : 1 ## extent : -112.964, -111.5441, 24.74642, 27.3632 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 plot( alt, legend=FALSE) plot(edge,add=TRUE) From this edge object (or a collection of edge objects) we can again extract values from a raster. Here I pull out the elevation profile of this edge. To plot it, I need to make a sequence of latitude values equal in length to that of the observed elevation values I extracted. elev &lt;- extract( alt, edge )[[1]] from_lat &lt;- V(graph)$Latitude[V(graph)$name == &quot;SFr&quot;] to_lat &lt;- V(graph)$Latitude[ V(graph)$name == &quot;164&quot;] lat &lt;- seq( from=from_lat, to=to_lat, length.out = length(elev) ) df &lt;- data.frame( Latitude=lat, Elevation=elev) p &lt;- ggplot( df, aes(Latitude,Elevation)) + geom_line(color=&quot;lightgrey&quot;) p + geom_point() + ylab(&quot;Elevation (m)&quot;) Population Graphs have been used to determine if there is an preference (or avoidance) of a specific raster value for the location of individual edges on the landscape. For example, Dyer et al. (2012) were interested in determining if there the edge in the pollination graph (e.g., a population graph constructed from male pollen haplotypes) preferentially traverse (or avoid) specific intervening habitat features. To do this, they permuted the edge set among nodes in the graph and recorded the prevalence (mean and variance) of specific features extracted from specific categorical rasters representing both canopy and understory features. The permutation of a network should be done such that you preserve aspects of the spatial heterogeneity and spatial arrangement of the nodes on the landscape. You probably do not want to permute all edges randomly in the graph (though this option is available in the code), but would probably be better served by permuting the network while maintaining both the node coordinates (e.g., where they exist on the landscape) as well as the degree distribution of the overall network. This second criteria holds constant higher network structure. The general idea is to: Create a saturated graph and extract ecological features for all potential connections. This gives us a data.frame within which we can pull out ecological values for each permutation. This is the most computationally intensive process and doing it once and then extracting values from the data.frame for each permutation is a more efficient approach. Once you have all the potential values of your features, you can permute the observed matrix, while holding both the connection probability (e.g., the number of edges) and the degree distribution (e.g., the amount of edges connected to nodes) constant using the randomize_graph function included in the popgraph library. For each permutation, you then compile the permuted environmental factor as a null distribution and then compare those to the observed. This may sound a bit convoluted, but this example may help. Consider the hypothetical case where we think that the edges in the population graph, are restricted in elevation because we believe that dispersing insects fly around high elevation mountains rather than over them.4 If this is true, then we should expect that the average (or max) elevation along any of the observed edges in the Population Graph would be less than what would be expected if we permuted the edges among nodes and measured elevation along edges from permuted graphs. First, we need to set up the network and extract values of elevation along all potential edges. I make a saturated graph from all potential data(baja) graph &lt;- popgraph( to_mv( arapat ), arapat$Population ) graph &lt;- decorate_graph( graph, coords ) allpops &lt;- V(graph)$name I then can make an adjacency matrix connecting all pairs of populations A &lt;- matrix(1,nrow=length(allpops),ncol=length(allpops)) diag(A) &lt;- 0 rownames(A) &lt;- colnames(A) &lt;- allpops saturated_graph &lt;- graph.adjacency(A, mode = &quot;undirected&quot;) saturated_graph &lt;- as.popgraph( saturated_graph ) From which I can pull all the edges (all 741 of them) as SpatialLines objects saturated_graph &lt;- decorate_graph( saturated_graph, coords ) all_edges &lt;- to_SpatialLines( saturated_graph ) From these 741 SpatialLines objects, we can extract data from the elevation raster. edge_values &lt;- extract( alt, all_edges, fun=max, na.rm=TRUE, df=TRUE) This will take a bit of time to complete. The options that I provided were: - fun=max - The function used is the max function. - na.rm=TRUE - Ignore all missing data (e.g., when an edge crosses water on the alt raster, the extracted values are NA) - df=TRUE - Return the answer as a data.frame object instead of just a vector. This data.frame has two columns, one for edge number and the other for value. I’m going to put an additional pair of columns with the names of the nodes the edges are connected to into this data.frame edge_names &lt;- as_edgelist( saturated_graph ) edge_values$Nodes &lt;- paste( edge_names[,1], edge_names[,2], sep=&quot;-&quot;) head(edge_values) ## ID alt_22 Nodes ## 1 1 25 101-102 ## 2 2 906 101-12 ## 3 3 31 101-153 ## 4 4 72 101-156 ## 5 5 1079 101-157 ## 6 6 1361 101-159 This constitutes all potential connections across the landscape. From this we can extract the edges that we observed in the original Population Graph e &lt;- as_edgelist( graph ) obs &lt;- edge_values$alt_22[ edge_values$Nodes %in% paste( e[,1], e[,2], sep=&quot;-&quot;) ] mean(obs) ## [1] 744.5634 We can now permute the network a moderate number of times and take the values of permuted elevation to see if our observed are smaller than all potential elevations for this specific network. perm_elev &lt;- rep(NA,999) for( i in 1:length(perm_elev) ) { perm_graph &lt;- randomize_graph( graph ) e &lt;- as_edgelist( perm_graph ) perm_val &lt;- edge_values$alt_22[ edge_values$Nodes %in% paste( e[,1], e[,2], sep=&quot;-&quot;) ] perm_elev[i] &lt;- mean(perm_val) } Now, we can see where the observed value occurs in the distribution of elevations created under the null hypothesis of no difference in elevation across edges. df &lt;- data.frame( Elevation=c(mean(obs),perm_elev), Category=c(&quot;Observed&quot;,rep(&quot;Permuted&quot;,999))) ggplot( df, aes(x=Elevation,fill=Category)) + geom_histogram(stat=&quot;bin&quot;, bins=40) + xlab(&quot;Elevation (m)&quot;) + ylab(&quot;Distribution of Permuted Elevations&quot;) In fact, we can estimate the probability as: sum( mean(obs) &gt;= perm_elev ) ## [1] 1 As it turns out, the observed edges do in fact appear to be traversing lower elevations than the potential set of edges that could be present (while controlling for spatial location of populations and graph structure). 4.10 Extracting Graph-Theoretic Parameters The underlying structure of a popgraph object is based upon the igraph package from Gabor Csardi. A population graph is essentially a specific kind of igraphobject and can be decorated with metadata that is useful for spatial population genetic analyses. As such, there is a wealth of existing analyses from both the igraph as well as the sna packages that can be used on popgraph objects. Here are some examples. 4.10.1 Matrix Representations of Population Graph Objects A graph topology is a graphical representation of a matrix and there are several reasons why you may want to use these matrices. The function to_matrix() is an easy front-end to several kinds of matrices. Matrix structure itself can be defined by adjacency matrices, either binary (the default) or weighed by the edge weight. Several graph-theoretic parameters are derived from the adjacency matrix. Here is an example from our little graph that started this document. to_matrix( lopho, mode=&quot;adjacency&quot;)[1:5,1:5] ## BaC Ctv LaV Lig PtC ## BaC 0 0 1 1 1 ## Ctv 0 0 0 0 0 ## LaV 1 0 0 1 1 ## Lig 1 0 1 0 1 ## PtC 1 0 1 1 0 to_matrix( lopho, mode=&quot;edge weight&quot;)[1:5,1:5] ## BaC Ctv LaV Lig PtC ## BaC 0.000000 0 9.052676 9.71615 12.38248 ## Ctv 0.000000 0 0.000000 0.00000 0.00000 ## LaV 9.052676 0 0.000000 12.07282 12.80017 ## Lig 9.716150 0 12.072820 0.00000 14.22483 ## PtC 12.382480 0 12.800170 14.22483 0.00000 In addition to who each node is connected to, it is often of interest to know the length of the shortest path through the matrix connecting nodes. Here is a slightly larger example, using the cactus data so we can look at isolation by graph distance. cGD &lt;- to_matrix( lopho, mode=&quot;shortest path&quot;) cGD[1:5,1:5] ## BaC Ctv LaV Lig PtC ## BaC 0.000000 9.195038 9.052676 9.71615 12.38248 ## Ctv 9.195038 0.000000 13.083311 15.23302 20.49099 ## LaV 9.052676 13.083311 0.000000 12.07282 12.80017 ## Lig 9.716150 15.233023 12.072820 0.00000 14.22483 ## PtC 12.382480 20.490990 12.800170 14.22483 0.00000 It should be noted that the shortest distance through a population graph is defined as the parameter \\(cGD\\), conditional graph distance (see Dyer et al. 2010 for more information on this parameter). Now, we need the physical distance between the nodes. If the physical size of the sampling area is small we could just use the Pythagorean equation. However, here the distance is relatively large and the curvature of the earth may be of interest to take into account. There are several functions that will calculate ‘great circle distance’ but the easiest is rdist.earth() from the fields function. library(gstudio) df &lt;- data.frame( Stratum = V(lopho)$name, Longitude =V(lopho)$Longitude, Latitude = V(lopho)$Latitude ) pDist &lt;- strata_distance( df ) Now, we can plot these values against each other to see if there is a pattern of ‘isolation by distance’ captured in the graph topology. To do this, I extract only the upper triangle (e.g., the values above the diagonal of each matrix) because they are symmetric matrices and we do not want to look at each datum twice. df &lt;- data.frame( cGD=cGD[upper.tri(cGD)], Phys=pDist[upper.tri(pDist)]) cor.test( df$Phys, df$cGD, method=&quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: df$Phys and df$cGD ## S = 729990, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.5270434 We can plot these values and make a trend line pretty easily. Here is a plot using ggplot2 (a very nice plotting library; you could use plot() to do the normal plotting but I think ggplot2 does such a nice job I encourage its use). qplot( Phys, cGD, geom=&quot;point&quot;, data=df) + stat_smooth(method=&quot;loess&quot;) + xlab(&quot;Physical Distance&quot;) + ylab(&quot;Conditional Genetic Distance&quot;) The trend line is the Loess predicted line with confidence interval. 4.10.1.1 Node Specific Parameters Features of the topology can be extracted as either properties of the nodes or the edges. Node properties may provide insights into localized processes (e.g., a ‘sink’ population). There are a lot of different parameters that can be derived and several packages in R that help out. Here are some basic ones. df.nodes$closeness &lt;- closeness(lopho) df.nodes$betweenness &lt;- betweenness(lopho) df.nodes$degree &lt;- degree( lopho ) df.nodes$eigenCent &lt;- evcent( lopho )$vector df.nodes$Region &lt;- factor(V(lopho)$Region) summary(df.nodes,color=&quot;Region&quot;) ## Pop Latitude Longitude Elevation ## BaC : 1 Min. :23.58 Min. :-114.7 Min. : 5.0 ## CP : 1 1st Qu.:25.73 1st Qu.:-112.9 1st Qu.: 14.0 ## Ctv : 1 Median :28.82 Median :-112.0 Median : 66.0 ## LaV : 1 Mean :27.91 Mean :-112.2 Mean :159.2 ## LF : 1 3rd Qu.:29.73 3rd Qu.:-111.3 3rd Qu.:259.0 ## Lig : 1 Max. :31.95 Max. :-110.0 Max. :667.0 ## (Other):15 ## closeness betweenness degree eigenCent ## Min. :0.002194 Min. : 0.00 Min. :3.000 Min. :0.0005975 ## 1st Qu.:0.002466 1st Qu.: 0.00 1st Qu.:4.000 1st Qu.:0.0040621 ## Median :0.002919 Median : 4.00 Median :5.000 Median :0.1514600 ## Mean :0.002889 Mean :17.43 Mean :4.952 Mean :0.3043499 ## 3rd Qu.:0.003227 3rd Qu.:25.00 3rd Qu.:6.000 3rd Qu.:0.6687298 ## Max. :0.003844 Max. :98.00 Max. :7.000 Max. :1.0000000 ## ## Region ## 1:12 ## 2: 9 ## ## ## ## ## The relationship between the node variables can be evaluated in a pair plot. library(GGally) ggpairs(df.nodes,columns=2:9, color=&#39;Region&#39;) 4.10.2 Edge Specific Parameters Edges may have specific properties as well. Here are some examples using betweeness centrality, community, and regionality (if the edge connects within Baja or Sonora or crosses the Sea of Cortéz). df.edge &lt;- data.frame( Weight=E(lopho)$weight ) df.edge$betweenness &lt;- edge.betweenness(lopho) df.edge$Region &lt;- rep(&quot;Baja&quot;,52) df.edge$Region[36:52] &lt;- &quot;Sonora&quot; df.edge$Region[c(11,24,27,35)] &lt;- &quot;Cortez&quot; ggpairs(df.edge, color=&quot;Region&quot;) 4.11 Testing for Topological Congruence If you have more than one topology and the node sets overlap sufficiently, you can test for the topological congruence of the two. There are several specific reasons why population graph topologies may be congruent. In this section we use the spatial genetic structure of the Senita cactus (Lophocereus schottii) and its obligate pollinator, Upiga virescens as an example. This is an obligate pollination mutualism and as such we should expect there to be some degree of spatial genetic congruence between the species due to this co-evolution. data(upiga) upiga &lt;- decorate_graph(upiga,baja,stratum=&quot;Population&quot;) upiga.nodes &lt;- to_SpatialPoints(upiga) upiga.edges &lt;- to_SpatialLines(upiga) These two data sets were ‘mostly’ collected in the same physical locations. Here is a comparison of the two topologies. par(mfrow=c(1,2)) plot(lopho) plot(upiga) You can clearly see some differences in both the node and edge sets. However, given the relationship between these organisms, there is an expectation that they should share some spatial structure. The function congruence_topology() is designed to extract the congruence graph that is the intersection of both node and edge sets. If the node sets are not completely overlapping (in this case they are not), it will give you a warning. If you want to compare topologies, you must start with identical node sets because the topology in a Population Graph is based upon the entire structure, not just pairwise differences. See the write up about the gstudio package for more information on this. cong &lt;- congruence_topology(lopho,upiga) plot(cong) We can then take the congruence graph and plot it or work with it in the normal fashion. cong &lt;- decorate_graph( cong, baja ) cong.nodes &lt;- to_SpatialPoints(cong) cong.edges &lt;- to_SpatialLines(cong) plot(alt) plot(cong.edges,add=T) plot(cong.nodes,add=T, pch=16, col=&quot;red&quot;) There are several ways to examine ‘congruence’ in graph topologies, of which I show two. The first method is based upon the correlation of pair-wise distance through the graph for each. That is to say, are proximate nodes in lopho similarly close in upiga? This is called “Distance Congruence” and is based upon a non-parametric correlation of path distances. test_congruence(lopho,upiga,method=&quot;distance&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: distances.graph1 and distances.graph2 ## t = 7.3025, df = 118, p-value = 3.6e-11 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4207347 0.6701315 ## sample estimates: ## cor ## 0.5579029 Another way is to ask about the pattern of connectivity. Based upon the number of nodes and edges in lopho and upiga, are there more in the congruence graph than could be expected if the two graph were just randomly associated? This is called “Structural Congruence” and is determined combinatorially. What is returned by this is the probability having as many edges as observed in the congruence graph given the size of the edge sets in the predictor graphs. You can think of this as the fraction of the area under the line as large or larger than the observed. test_congruence(lopho,upiga, method=&quot;combinatorial&quot;) ## CDF ## 0.03625037 Hope this provides enough of an overview of the popgraph package to get you started. If you have any questions feel free to email [mailto://rjdyer@vcu.edu]. 4.12 Questions Take the individuals in the arapat data set partition out those that are designated as Cluster=&quot;Peninsula&quot;. Save these individuals to a text file formatted for STRUCTURE. In small groups, perform a set of replicate runs in STRUCTURE from K=1 to K=5. What do you conclude about this analysis? How does this fit into the whole data arapat data set? In the decomposition of the genetic distance graph, does the pattern of fragmentation mimic the way in which we found the subcomponents using STRUCTURE and the various principal component groups? This is a truly hypothetical and contrived example to show how you would do this in the code and is not motivated by any biologically motivated reasons. Ya’ gotta make up examples sometimes…↩ "],
["quantitative-genetics.html", "5 Quantitative Genetics 5.1 Phenotypic Variance 5.2 Data Examples 5.3 Heritability 5.4 Questions", " 5 Quantitative Genetics In the vast majority of landscape genetic studies, we focus on genetic markers and how thier spatial structure is influenced by ecological, environmental, and spatial features. This focus emphasizes neutral genetic variance, whose configuration is due to demographic and microevolutionary processes other than selection. However, if we are interested in adaptive features in natural populations, we have a much more difficult task to accomplish, one that necessitates us to partition the neutral signal from the data so that we can identify the putatively adaptive components. This chapter spends a bit of time focusing on the nature of phenotypes that we can measure that may contribute to fitness. Across a landscape, the interation between gentoypes and the environment create variability in phenotypes. These phenotypes, most commonly derived from many interacting loci, are what define the fitness landscape by the way in which they respond to differing selection pressures. A schematic representation of the relationship between genotypes, environmental fetures, phentoypes, and the resulting fitness landscape (adapted from Eckert &amp; Dyer, 2012).) The next chapter examines how we can look at the genetic markers alone to get inferences about where selection may be acting—a hypotheis generating approahc. This one focuses on measuring and determing the extent to which traits can respond to selection. Fisher’s fundamental theorem of natural selection states that the ability of a population to respond to selection is based upon: The variance of the trait. Traits without variation cannot respond, selection has nothing to work on. Demographic processes such as bottlenecks may create populations characterized by genetic homogeneity, which is why in conservation genetics we tend to favor practices that help to increase genetic diversity. The way in which selection is acting. If a genetic variant has a deleterious effect, the relative cost of this to the organism will be determined by the relative consequences in terms of survival and/or reproduction. Strong effects (e.g., lethals) will obviously result in a more rapid response in the population than changes with minor effect. Existing population fitness. Populations with low average fitness may respond quicker to selection pressures than those whose average fitness is already high. These three factors determine the extent to which selection may infleunce phenotypes in natural populations. 5.1 Phenotypic Variance A phenotype is a trait that can be measured on an individual and may be morphological, biochemical, behaviorial, or any other kind of feature. At a population level, the variance in this phenotype, \\(\\sigma_P^2\\), can be partitioned into additive components as follows: \\[ \\sigma_P^2 = \\sigma_G^2 + \\sigma_E^2 + \\sigma_{GxE}^2 \\] The \\(\\sigma_G^2\\) term is the proportion of the phenotypic variance that can be attributed to genetic variation. Phenotypes are also influenced by environmental variation, \\(\\sigma_E^2\\). Finally, there may be an interaction between the genetic and environemtnal variance, \\(\\sigma_{GxE}^2\\), that also contributes to phenotypic variance. The genetic variance, \\(\\sigma_G^2\\), can be further partitioned into additive components salient to understanding the evolution of traits. \\[ \\sigma_G^2 =\\sigma_A^2 + \\sigma_D^2 + \\sigma_I^2 \\] These components are infleunced by the way in which alleles, loci, and other genetic components contribute to the production of a particular trait. Additive genetic variance, \\(\\sigma_G^2\\): Additive genetic variance is created by the ineraction among alleles at a particluar locus as well as the contributions of loci in the development of a trait. In the simplest model, consider a locus that encodes for a protein that modifies a substrate component in the biochemical pathway for a phenotype (say pigmentation development that leads to flower color). Variant alleles may be more or less efficient in their capacity to modify substrate causing differences among gentypes. Loci along this pigmentation pathway may individually have many variant types, all of which contribute to the resulting phenotype. This is additive variance. Dominance variance, \\(\\sigma_D^2\\): Dominance variance can be similarly defined based upon the interaction among alleles at a locus. In the classic Mendelian model, a dominant allele can mask the effects of other alleles. This deviates from the pure additive model in that all you need at a diploid locus is a single copy of the dominant allele to express the dominant pheotype. Interaction variance, \\(\\sigma_I^2\\): Genetic interaction variance captures the way in which alternative loci influence each other. The classic case here is that of coat color in the Labrador dog breed. One locus determines the pigmentation color, with alleles that encode for black pigmentation color dominant to brown. An entirely different locus determines the ability to incorporate the pigmentation into the hair shaft itself with a dominant allele that allows the pigentation to be incorporated (resulting in either a black or brown dog) and a recessive allele the prevents incorporation (producing the yellow labrador). At each locus, the interaction of the alleles contribute to the dominance variance but the way in which the second locus infleunces the phenotype produced by the first locus is interaction variance. Estimating the features that contribute to a phenotype is not trivial and requires specific sampling approahces which often include analyses across generations and may require precise information on the relatedness of individuals or controlled matings and methods to standardize environmental variance. 5.2 Data Examples Here we will use some example data that was collected to understand the quantitative consequences of urban cultivar gene escape on native populations in the tree Cornus florida. This species is widley distributed in natural populations throughout forests of easter North America. It has also been artifically selected to express traits that make it suitable for use in the cultivar trade by nurseries. Cultivar types can easily thrive in open sunlight (as is often found in residental yards, etc), whereas the native variants are typically typically found in more shaded areas of the forest understory. An experiment was set up to collect arrays of seedling siblings from maternal individuals in both the urban environment and proximate native populations. These seeds were germinated and grown in a greenhouse environment. Seedlings were then randomly assigned to either a full light (to mimic the urban environment) or a shaded (to represent the native understory) environment. Several quantiative measurements were collected throughout the growning season. A subset of these data are available in the seedlings.rda data file. Dogwood seedlings growing in the greenhouse. Seeds were collected from parental trees from urban (left) and natural (right) populations and grown in either full light (to mimic urban environments) and shaded (to mimic native popualtions) The data presented represent the seedlings that had survived to the end of the growing season, partitioned by seed source. A summary of the data is listed below. Source Treatment Number of Trees Offspring fan Cultivar 24 131 rice Native 9 40 Quantitative measurements were made on each seedling throughout the growing season to determine if cultivar types were producing different phenotypes than native stock. Some of the measured traits include: Cold - The length of time (in days) that the seedling remained dormant after cold stratification. Branches - The number of branching stems on each seedling. Leaves - The total number of leaves on the seedling. Height - Total seedling height. Leaf Area - The size of the largest leaf on the seedling. summary(seedlings) ## Source Treatment Mom Leaves Height ## fan :131 SHADE: 52 U09 : 14 Min. : 2.00 Min. : 2.70 ## rice: 40 SUN :119 U17 : 14 1st Qu.: 5.00 1st Qu.: 8.75 ## U18 : 13 Median :10.00 Median :17.70 ## 598 : 10 Mean :13.75 Mean :16.14 ## U19 : 10 3rd Qu.:21.00 3rd Qu.:21.70 ## U24 : 10 Max. :54.00 Max. :34.70 ## (Other):100 ## Branches Cold LeafArea ## Min. :0.000 Min. : 71.00 Min. : 1.14 ## 1st Qu.:0.000 1st Qu.: 80.00 1st Qu.: 14.73 ## Median :1.000 Median : 84.00 Median : 50.60 ## Mean :2.216 Mean : 89.79 Mean : 47.47 ## 3rd Qu.:4.000 3rd Qu.: 94.00 3rd Qu.: 69.21 ## Max. :8.000 Max. :138.00 Max. :160.20 ## The relationship among these traits are presented below. library(GGally) library(ggplot2) ggpairs( seedlings, columns=4:8) Like most data, these are imperfect. In particular, there are some families whose seedling mortality resulted in few seeds. t &lt;- table( seedlings$Mom ) t ## ## 354 471 590 598 600 645 646 712 819 U09 U11 U12 U13 U14 U15 U17 U18 U19 ## 3 1 4 10 2 4 5 8 3 14 5 5 5 7 1 14 13 10 ## U20 U23 U24 U25 U26 U27 U29 U31 U32 U36 U39 U40 U44 U46 U47 ## 1 5 10 5 3 1 5 7 2 2 2 8 1 3 2 As a result, before we go forward, lets limit the data to those mothers whose families have at least 5 surviving offspring. keepers &lt;- names(t)[ t &gt;= 4 ] seedlings &lt;- seedlings[ seedlings$Mom %in% keepers, ] seedlings &lt;- droplevels( seedlings ) 5.2.1 Measured Traits Visual summaries of the measured traits are shown below. p &lt;- ggplot(seedlings) + theme_bw() p + geom_density( aes(fill=Source,x=Cold), alpha=0.75) Figure 5.1: Length of time (in days) of seedling dormancy following cold stratification for seedlings collected from maternal trees in urban (fan) and native (rice) populations. In the remaining plot, I partion the display of phenotypic variability further by separating out seedlings that were grown in either the SUN (full light) or SHADE (reduced light) treatments. p &lt;- p + facet_grid(Treatment~.) p + geom_density( aes(fill=Source,x=Height), alpha=0.75) Figure 5.2: The height of the seedling measured from the soil to the top of the apical meristem. p + geom_density( aes(fill=Source,x=Branches), alpha=0.75) Figure 5.3: The number of branches from the main seedling stem. p + geom_density( aes(fill=Source,x=LeafArea), alpha=0.75) Figure 5.4: The photosynthetic area measured on the largest leaf from each seedling. geom_density( aes(fill=Source,x=Leaves), alpha=0.75) ## mapping: x = ~Leaves, fill = ~Source ## geom_density: na.rm = FALSE ## stat_density: na.rm = FALSE ## position_identity Overall, there are noticable differences in phenotypes due to both Source and Treatment as well as some pronounced bi-modality in the data. 5.3 Heritability For a phenotype to be influenced by natural selection, it must be heritable. Heritability is defined as the extent to which quantitative variation in inherited across generations. A distinction should be made between inheritance and heritability. Inheritance is the ability to pass along a phenotype to the next generation–any phenotype. Heritability is the passing along of phenotypic variance. As such, all traits that have heritability are also inherited but not all inherited traits have heritability! Heritability can be estimated at two different levels of specificity. Broad sense heritability (\\(H^2\\)) is the proportion of the phenotypic variance that can be attribted to genetic variance. \\[ H^2 = \\frac{\\sigma_G^2}{\\sigma_P^2} \\] This estimate includes the additive (\\(\\sigma_A^2\\)), dominance (\\(\\sigma_D^2\\)), and interaction (\\(\\sigma_I^2\\)) variance and is the easiest parameter to estimate. A common approach to estimating \\(H^2\\) is to report it as the proportion of variance explained (\\(R^2\\)) from an ANOVA based upon family-level data. Often we treat the family unit as a random effect in an analysis of variance. In R we fit random effects models using the lmer function. library(lme4) h2.cold &lt;- lmer(Cold ~ (1|Mom), data=seedlings) summary(h2.cold) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Cold ~ (1 | Mom) ## Data: seedlings ## ## REML criterion at convergence: 1130.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.24598 -0.40934 -0.05679 0.18957 3.11295 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Mom (Intercept) 136.5 11.69 ## Residual 116.4 10.79 ## Number of obs: 144, groups: Mom, 19 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 91.96 2.85 32.26 If you are not familiar with fitting a random effect to a model in R, this may look a bit odd. The formula Cold ~ (1|Mom) is examining differences among the length of dormancy as explained by seedlings grouped by mother. The maternal stratum is treated as a random effect because we are assuming that the mothers we used are a random selection from a much broader population of mothers. The differences between a random and a fixed effects model is in how we estimate the variance components. In a fixed effect model, the expectation for the treatment variance is based solely upon the treatment effect \\(E[\\sigma_A^2] = MSA\\). However, in a random effects model, we assume that since we are sampling on a random subsample of potential mothers, the treatment effect is a bit upwardly biased and we need to correct that variance. Here is an example of how those differ. Treating mother as a fixed effect, we can see the estimates of the varince for both Mom and Residuals as the values int he third column. anova( aov(Cold~Mom,data=seedlings)) ## Analysis of Variance Table ## ## Response: Cold ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Mom 18 19878 1104.35 9.4969 5.518e-16 *** ## Residuals 125 14536 116.28 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 However, those same variance estiamtes, assuming that Mom is a random effect, are different (moreso for the Mom effect than that for the Residual variance). print(VarCorr(h2.cold), comp=&quot;Variance&quot;) ## Groups Name Variance ## Mom (Intercept) 136.55 ## Residual 116.37 Then from the formula above, we can estimate the fraction of the total variance that is attributed to the family effect as: sigmas &lt;- as.data.frame( VarCorr( h2.cold) )$vcov H2 &lt;- sigmas[1] / sum( sigmas ) H2 ## [1] 0.539898 This result shows suggests that roughly 53.99% of the variation can be attributed to genetic effects. Narrow-sense heritabilty is a bit more precise in that it uses not the total genetic variance but only that associated with the additive component of variation. \\[ h^2 = \\frac{\\sigma_A^2}{\\sigma_P^2} \\] However, this precision is gained at the expense of a bit more complexity. There are a few different basic methods used for estimating \\(h^2\\) from sibling data. Twin studies. If you have data from monozygotic (MZ) and dizygotic (DZ) twins, the narrow sense heritability can be estimated using the differenes in phenotypic correlations, \\(h^2 = r_{MZ} - r_{DZ}\\) (the so-called Falconer’s Formula). Jaccard (1983) had suggested a modification of this to use \\(h^2 = (r_{MZ} - r_{DZ})/(1-r_{DZ})\\) instead though it is not that common of a standardization. Another method that can be used relies upon estimates of consanguanity (e.g., genetic relatedness) of the siblings. In most organisms, this would be a value that is relatively easy to calculate—they are half sibs because we collected them from the same maternal individual! However, in many plants it is not uncommon to have mixes of full and half siblings due to selfing as well as have correlated paternity (e.g., two seeds are actually full siblings since they share the same fater as well as mother). Without a little more genetic information on these individuals we cannot determine either of these parameters. The final method uses both parent and offspring phenotypes. Here, narrow sense heritabilty is the slope of a regression line fitting offspring phenotype to the average of the parental phenotype. If heritabilty is high, then the slope of this line will approach 1.0, whereas if offspring phenotypes have little relation to those measured on the parents (e.g., \\(\\sigma_E^2\\) and perhaps \\(\\sigma_{GxE}^2\\) have more influence) then the slope will tend towards zero. There is a caveat here in that when a trait has a low heritability (e.g., the slope is close to zero), it is often the case that the variation explained by that model (the \\(R^2\\)) is also low. Conversely, when heritability is high, it is often the case that \\(R^2\\) is also increased. This means that the power of this kind of anlaysis improves with heritability in that we get a better estimate when it is high. You can easily find examples of each of these methods in the literature. However, in landscape studies, we often lack the kind of information necessary to collect these data, which is why understanding quantitative genetics in natural populations is a challenging endeavour. Unfortunately, the data we are working with do not have the additional relatedness information or parental seedling characteristics included so we cannot estimate narrow sense heritibility. 5.4 Questions For many quatitative traits, environmental variance (\\(\\sigma_E^2\\)) may make large contributions to observed phenotypic variance. From the traits above, which show the largest environmental effects (e.g., SHADE vs SUN)? What is the broad-sense heritability for the “Height” and the “Leaf Area” traits in the SHADE treatment? (Make sure to drop any family that has too few seedlings) Are they similar or different? Explain the results. Estimate broad-sense heritability for the SUN families for “Height” and “Leaf Area”. How do these differ from the estimates of heritability in the SHADE seedlings? Assuming that the dominance and interaction variance (\\(\\sigma_D^2\\) and \\(\\sigma_I^2\\)) of the height and leaf area traits were constant (e.g., \\(H^2\\) would be proportional to \\(h^2\\)), rank the traits in terms of their ability to respond to selection. Which are able to respond faster? What are some of the challenges associated with this partiular data set? "],
["references-cited.html", "References Cited", " References Cited Dyer RJ. 2015 Is there such a thing as landscape genetics? Molecular Ecology, 24, 3518-3528. Eckert AJ Dyer RJ. 2012. Defining the landscape of adaptive genetic diversity. Molecular Ecology, 21, 2836-2838. Jacquard A. 1983. Heritability: one word, three concepts. Biometrics, 39, 465-477. Manel S, Schwartz MK, Luikart G, Taberlet P. 2003. Landscape genetics: combining landscape ecology and population genetics. Trends in Ecology and Evolution, 18, 189-197. "],
["rgdal-rgeos.html", "RGDAL &amp; RGEOS", " RGDAL &amp; RGEOS Every time I upgrade R in any significant way, two libraries seem to raise their ugly heads and scream like a spoiled child—–rgdal and rgeos. Why do these two have to be SOOOO much of a pain? Why can’t we have a auto build of a binary with all the options in it for OSX? Who knows? I always feel like I get the fuzzy end of the lollipop with these two. Here is my latest approach for getting them going. First you have to make sure you have the latest GDAL libraries. I used to get mine from Kyngchaos, just download the framework, install it, and then do some kind of long R CMD INSTALL dance, which seems to no longer work for me. I also tried installing from Ripley’s repository and found that (a) It was a version older than the one I already had on my machine, and (b) you can’t install from that repository, there is a malformed header and the install.packages() function just barfs. Time to try something new. I typically stay away from the various installer frameworks out there on OSX to keep everything in Frameworks. But this time, I used MacPorts. You can find the latest version here. Here is how I got it to help me out. Download XCode from Apple, it is both free and has a lot of tools that make your life as a programmer easier. It is a big package and you’ll have to install the command line developer tools as well. You will be prompted on how to do this. Downloaded the version of macports for your OS, I’m currently on 10.11 and installed it with little problems. It takes a bit of time at the end of the installation because it is downloading a lot of information. Be patient. In the terminal, I updated it sudo ports -v selfupdate and again be patient, it is going to grab a lot of stuff from the internet. I then used it to install gdal as a unix library (rather than as a framework so it won’t be located in /Library/Frameworks) by sudo ports install gdal. There were a lot of dependencies for this one so it took a while. I then had R install rgdal as install.packages( rgdal, type=&quot;source&quot;) Worked like a charm. "]
]
